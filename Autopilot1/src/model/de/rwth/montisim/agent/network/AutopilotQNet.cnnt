 configuration AutopilotQNet {
     context: cpu

     learning_method : reinforcement

     agent_name : "AutopilotAgent"

     rl_algorithm : td3-algorithm

     policy_noise: 0.2
     noise_clip: 0.5
     policy_delay: 2

     critic: de.rwth.montisim.agent.network.autopilotCritic

     environment : ros_interface {
         state_topic: "/sim/state"
         terminal_state_topic: "/sim/terminal"
         action_topic: "/sim/step"
         reset_topic: "/sim/reset"
         reward_topic: "/sim/reward"
     }

     num_episodes : 800 
     discount_factor : 0.99
     num_max_steps : 900000
     training_interval : 1
     start_training_at : 0

     evaluation_samples: 3 
     soft_target_update_rate: 0.005

     snapshot_interval : 500
     replay_memory : buffer{
         memory_size : 10000
         sample_size : 100
     }

     strategy : ornstein_uhlenbeck{
         epsilon : 1.0
         min_epsilon : 0.0001
         epsilon_decay_method: linear
         epsilon_decay : 0.000003
         epsilon_decay_start: 10
         epsilon_decay_per_step: true
         theta: (1.0, 1.0, 1.0)
         mu: (0.0, -0.7, 0.0)
         sigma: (0.3, 0.2, 0.3)
     }

     actor_optimizer: adam {
         learning_rate: 0.001
     }

     critic_optimizer: adam {
         learning_rate: 0.001
     }
 }
