configuration AutopilotQNet {
	context: gpu

	learning_method: reinforcement

	agent_name: "AutopilotAgent"

	rl_algorithm: td3

	self_play: no

	policy_delay : 5

	critic: de.rwth.montisim.agent.network.autopilotCritic

	environment: ros_interface {
		state: "/sim/state"
		terminal: "/sim/terminal"
		action: "/sim/step"
		reset: "/sim/reset"
		reward: "/sim/reward"
	}

	num_episodes: 400
	discount_factor: 0.995
	num_max_steps: 5000
	training_interval: 1
	start_training_at: 0

	evaluation_samples: 3
	soft_target_update_rate: 0.005

	snapshot_interval: 50

	replay_memory: buffer {
		memory_size: 50000
		sample_size: 500
	}

	strategy: ornstein_uhlenbeck {
		epsilon: 1.0
		min_epsilon: 0.0
		epsilon_decay_method: linear
		epsilon_decay: 0.0025
		epsilon_decay_start: 0
		epsilon_decay_per_step: false
		theta: (0.15, 0.15, 0.15)
		mu: (0.0, 0.0, 0.0)
		sigma: (0.2, 0.2, 0.0)
	}

	actor_optimizer: adam {
		learning_rate: 0.0001
	}

	critic_optimizer: adam {
		learning_rate: 0.001
	}
}
