configuration TorcsActor {
	context : cpu

	learning_method : reinforcement

	agent_name: "TorcsAgent"

	rl_algorithm: td3-algorithm

	policy_noise: 0.2
	noise_clip: 0.5
	policy_delay: 2

	critic: agent.network.torcsCritic

	environment : ros_interface {
		state_topic: "/torcs/affordance"
		terminal_state_topic: "/torcs/terminal"
		action_topic: "/torcs/step"
		reset_topic: "/torcs/reset"
		reward_topic: "/torcs/reward"
	}

	//reward_function: agent.network.reward

	num_episodes : 3500
	discount_factor : 0.99
	num_max_steps : 900000
	training_interval : 1
	start_training_at: 0

	evaluation_samples: 1
	soft_target_update_rate: 0.01

	snapshot_interval : 150

	replay_memory : buffer{
		memory_size : 120000
		sample_size : 100
	}

	strategy : ornstein_uhlenbeck{
		epsilon : 1.0
		min_epsilon : 0.0001
		epsilon_decay_method: linear
		epsilon_decay : 0.000008
		epsilon_decay_start: 10
		epsilon_decay_per_step: true
		theta: (0.6, 1.0, 1.0)
		mu: (0.0, 0.0, -1.2)
		sigma: (0.3, 0.2, 0.05)
	}

	actor_optimizer: adam {
		learning_rate: 0.001
	}

	critic_optimizer: adam {
		learning_rate: 0.001
	}
}
