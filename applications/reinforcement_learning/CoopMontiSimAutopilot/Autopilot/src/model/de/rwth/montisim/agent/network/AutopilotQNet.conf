configuration AutopilotQNet {
	context: gpu

	learning_method: reinforcement

	agent_name: "AutopilotAgent"

	rl_algorithm: td3

	policy_noise: 0.2
	noise_clip: 0.5
	policy_delay: 2

	self_play: yes

	critic: de.rwth.montisim.agent.network.autopilotCritic

	environment: ros_interface {
		state: "/sim/state"
		terminal: "/sim/terminal"
		action: "/sim/step"
		reset: "/sim/reset"
		reward: "/sim/reward"
	}

	num_episodes: 200
	discount_factor: 0.99
	num_max_steps: 3000
	training_interval: 1
	start_training_at: 0

	evaluation_samples: 3
	soft_target_update_rate: 0.005

	snapshot_interval: 25

	replay_memory: buffer {
		memory_size: 1000000
		sample_size: 100
	}

	strategy: gaussian {
		/* from epsgreedy */
		epsilon: 1.0
		min_epsilon: 0.2
		epsilon_decay_method: linear
		epsilon_decay: 0.004
		epsilon_decay_start: 0
		epsilon_decay_per_step: false

		/* from gaussian */
		noise_variance: 0.075
	}

	actor_optimizer: adam {
		learning_rate: 0.0001
	}

	critic_optimizer: adam {
		learning_rate: 0.001
	}
}
