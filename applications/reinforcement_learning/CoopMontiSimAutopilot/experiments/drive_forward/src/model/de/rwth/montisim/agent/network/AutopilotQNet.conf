configuration AutopilotQNet {
	context: gpu

	learning_method: reinforcement

	agent_name: "AutopilotAgent"

	rl_algorithm: td3

	self_play: no

	policy_noise: 0.15
	noise_clip: 0.4
	policy_delay : 5

	critic: de.rwth.montisim.agent.network.autopilotCritic

	environment: ros_interface {
		state: "/sim/state"
		terminal: "/sim/terminal"
		action: "/sim/step"
		reset: "/sim/reset"
		reward: "/sim/reward"
	}

	num_episodes: 2000
	discount_factor: 0.99
	num_max_steps: 1000
	training_interval: 1
	start_training_at: 500

	evaluation_samples: 3
	snapshot_interval: 50

	soft_target_update_rate: 0.0025

	replay_memory: buffer {
		memory_size: 10000000
		sample_size: 10000
	}

	strategy: ornstein_uhlenbeck {
		epsilon: 1.0
		min_epsilon: 0.0
		epsilon_decay_method: linear
		epsilon_decay: 0.00066666666
		epsilon_decay_start: 500
		epsilon_decay_per_step: false
		theta: (0.15, 0.15, 0.15)
		mu: (0.0, 0.0, 0.0)
		sigma: (0.2, 0.2, 0.2)
	}

	actor_optimizer: adam {
		learning_rate: 0.0001
	}

	critic_optimizer: adam {
		learning_rate: 0.001
	}
}
