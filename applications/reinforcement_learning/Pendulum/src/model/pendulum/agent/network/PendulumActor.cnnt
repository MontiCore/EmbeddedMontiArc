/* (c) https://github.com/MontiCore/monticore */
configuration PendulumActor {
    agent_name: "PendulumActor"
    
    context: cpu

    learning_method: reinforcement
    rl_algorithm: td3-algorithm
    critic: pendulum.agent.network.pendulumCritic

    environment: ros_interface {
        state_topic: "/gym/state"
        terminal_state_topic: "/gym/terminal"
        reward_topic: "/gym/reward"
        reset_topic: "/gym/reset"
        action_topic: "/postprocessor/step"
    }

    policy_noise: 0.2
    noise_clip: 0.5
    policy_delay: 2

    num_episodes: 20
    start_training_at: 1
    
    num_max_steps: 10000
    training_interval: 1

    snapshot_interval: 250
    evaluation_samples: 10

    soft_target_update_rate: 0.005

    replay_memory: buffer{
        memory_size : 1000000
        sample_size : 100
    }

    strategy : gaussian {
        epsilon : 1.0
        min_epsilon : 0.05
        epsilon_decay_method: linear
        epsilon_decay_start: 500
        epsilon_decay : 0.005
        noise_variance : 0.1
    }

    actor_optimizer : adam {
        learning_rate : 0.001
    }

    critic_optimizer : adam {
        learning_rate : 0.001
    }
}
