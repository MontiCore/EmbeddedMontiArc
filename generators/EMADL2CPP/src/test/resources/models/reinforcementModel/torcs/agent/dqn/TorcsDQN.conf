/* (c) https://github.com/MontiCore/monticore */
configuration TorcsDQN {
    context: cpu

    learning_method: reinforcement
    rl_algorithm: dqn

    environment: ros_interface {
        state: "preprocessor_state"
        terminal: "prepocessor_is_terminal"
        action: "postprocessor_action"
        reset: "torcs_reset"
    }

    reward: torcs.agent.dqn.reward

    num_episodes: 20000
    discount_factor: 0.999
    num_max_steps: 999999999
    training_interval: 1

    use_fix_target_network: true
    target_network_update_interval: 500

    snapshot_interval: 1000

    use_double_dqn: true

    loss: huber

    replay_memory: buffer{
        memory_size: 1000000
        sample_size: 32
    }

    strategy: epsgreedy{
        epsilon: 1.0
        min_epsilon: 0.01
        epsilon_decay_method: linear
        epsilon_decay: 0.0001
    }

    optimizer: rmsprop{
        learning_rate: 0.001
    }
}