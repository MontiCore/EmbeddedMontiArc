# (c) https://github.com/MontiCore/monticore
import logging
import os
import errno
import shutil
import h5py
import sys
import numpy as np
import tensorflow as tf
from CNNDataLoader_ResNeXt50 import CNNDataLoader_ResNeXt50 as CNNDataLoader


def huber_loss(y_true, y_pred):
    return tf.losses.huber_loss(y_true, y_pred)

def epe(y_true, y_pred):
    return tf.keras.backend.mean(tf.keras.backend.sqrt(tf.keras.backend.sum(tf.keras.backend.square(y_pred - y_true), axis=[1])),axis=[1,2])


def rmse(y_true, y_pred):
    return tf.keras.backend.sqrt(keras.losses.mean_squared_error(y_true, y_pred))    

def f1(y_true, y_pred):
    frac1 = tf.metrics.precision(y_true, y_pred) * tf.metrics.recall(y_true, y_pred)
    frac2 = tf.metrics.precision(y_true, y_pred) + tf.metrics.recall(y_true, y_pred)

    return 2 * frac1 / frac2


class LRScheduler:
    def __init__(self, params):

        self._decay = params["lr_decay"]

        if "lr_policy" in params:
            self._policy = params["lr_policy"]
            logging.info("Using %s learning_rate_policy!\n\n", self._policy)
        else:
            self._policy = "step"
            logging.warning("No lerning_rate_policy specified. Using step scheduler!\n\n")

        if "lr_minimum" in params:
            self._minimum = params["lr_minimum"]
        else:
            self._minimum = 1e-08

        if "step_size" in params:    
            self._step_size = params["step_size"]
        else:
            self._step_size = None


        self.scheduler = self.get_lr_scheduler()


    def get_lr_scheduler(self):

        mapping = {
            "fixed":        self.fixed_scheduler,
            "step":         self.step_scheduler}

        mapping_not_supported = {
            "exp":          "exp",
            "inv":          "inv",
            "poly":         "poly",
            "sigmoid":      "sigmoid"}

        if self._policy in mapping:
            return mapping[self._policy]
        elif self._policy in mapping_not_supported:
            #This is due to some parameters neccessery for this policies, missing in the CNNTrainLang grammar, and as the MXNET generator also only implements 
            #the step policy at the time of implementing this, we chose to not add it for now. These policies can be added by using the respective commented out functions below
            logging.warning("The %s learning_rate_policy is currently not supported by the keras/tensorlfow generator. \n", self._policy)
        else:
            logging.warning("The following learning_rate_policy is not supported by the keras/tensorflow generator: %s \n", self._policy)


    #note that the keras callback for lr scheduling only gets called inbetween epochs, not single iterations
    def fixed_scheduler(self, epoch_ind, old_lr):
        return old_lr

    def step_scheduler(self, epoch_ind, old_lr):

        if (epoch_ind % self._step_size == 0) and epoch_ind > 0:
            new_lr = old_lr * self._decay

            if new_lr < self._minimum:
                new_lr = self._minimum
        
            return new_lr
        
        else:
            return old_lr

    #def exp_scheduler(self, epoch_ind, old_lr):
        #return old_lr

    #def inv_scheduler(self, epoch_ind, old_lr):
        #return old_lr

    #def poly_scheduler(self, epoch_ind, old_lr):
        #return old_lr

    #def sigmoid_scheduler(self, epoch_ind, old_lr):
        #return old_lr

    
#If clip weights for rmsProp optimizer is specified this class is needed, as the keras/ tensorflow variant of rmsProp does not support weight clipping
class WeightClip(tf.keras.constraints.Constraint):
    def __init__(self, clip_val=2):
        self.clip_val = clip_val

    def __call__(self, w):
        return K.clip(w, -self.clip_val, self.clip_val)

    def get_config(self):
        return {'name': self.__class__.__name__,
                'clip_val': self.clip_val}

    
    
class CNNCreator_ResNeXt50:
     
    def __init__(self):
        self.model = None
        self._data_dir_ = "data/ResNeXt50/"
        self._model_dir_ = "model/ResNeXt50/"
        self._model_prefix_ = "model"
        
        self._input_names_ = ['data']
        self._output_names_ = ['predictions_label']
        self._output_shapes_ = [(1000,)]
        
        self._weight_constraint_ = None
        self._regularizer_ = None
    
    def load(self):
        lastEpoch = 0
        model_file = None

        try:
            os.remove(self._model_dir_ + self._model_prefix_ + ".newest.hdf5")
        except OSError:
            pass

        if os.path.isdir(self._model_dir_):
            for file in os.listdir(self._model_dir_):
                if ".hdf5" in file and self._model_prefix_ in file:
                    epochStr = file.replace(".hdf5", "").replace(self._model_prefix_ + ".", "")
                    epoch = int(epochStr)
                    if epoch > lastEpoch:
                        lastEpoch = epoch
                        model_file = file
        if model_file is None:
            return 0
        else:
            logging.info("Loading checkpoint: " + model_file)

            self.model = tf.keras.models.load_model(self._model_dir_ + model_file)

            return lastEpoch

    def build_optimizer(self, optimizer_name, params):

        fixed_params, lr_scheduler_params = self.translate_optimizer_param_names(params)

        if optimizer_name == "adam":
            return tf.keras.optimizers.Adam(**fixed_params), lr_scheduler_params
        elif optimizer_name == "sgd":
            return tf.keras.optimizers.SGD(nesterov=False, **fixed_params), lr_scheduler_params
        elif optimizer_name == "nag":
            return tf.keras.optimizers.SGD(nesterov=True, **fixed_params), lr_scheduler_params
        elif optimizer_name == "rmsprop":
            return tf.keras.optimizers.RMSprop(**fixed_params), lr_scheduler_params
        elif optimizer_name == "adagrad":
            return tf.keras.optimizers.Adagrad(**fixed_params), lr_scheduler_params
        elif optimizer_name == "adadelta":
            return tf.keras.optimizers.Adadelta(**fixed_params), lr_scheduler_params
        else:
            logging.warning("Optimizer not supported by keras/tensorflow: %s \n", optimizer_name)

    def translate_optimizer_param_names(self, params):

        mapping = {
            "learning_rate":            "lr",
            "momentum":                 "momentum",
            "beta1":                    "beta_1",
            "beta2":                    "beta_2",
            "gamma1":                   "rho",
            "gamma2":                   "momentum",
            "centered":                 "centered",
            "epsilon":                  "epsilon",
            "rho":                      "rho",
            "clip_gradient":            "clipvalue"}

        mapping_lr_scheduler = {
            "learning_rate_decay":      "lr_decay",
            "learning_rate_policy":     "lr_policy",
            "learning_rate_minimum":    "lr_minimum",
            "step_size":                "step_size"}

        fixed_params = {}
        lr_scheduler_params = {}
        for k in params:
            if k == "clip_weights":
                self._weight_constraint_ = WeightClip(params[k])
            elif k == "weight_decay":
                self._regularizer_ = tf.keras.regularizers.l2(params[k])
            elif k in mapping_lr_scheduler:
                lr_scheduler_params[mapping_lr_scheduler[k]] = params[k]
            elif k in mapping.keys():
                fixed_params[mapping[k]] = params[k]
            else:
                logging.warning("The following parameter is not supported by the keras/tensorflow generator %s \n", k)          
        return fixed_params, lr_scheduler_params

    def translate_loss_name(self, loss, num_outputs):
        mapping = {
            "l2":                   "mean_squared_error",
            "l1":                   "mean_absolute_error",
            "cross_entropy":        "sparse_categorical_crossentropy" if num_outputs > 1 else "binary_crossentropy",
            "log_cosh":             "logcosh",
            "hinge":                "hinge",
            "squared_hinge":        "squared_hinge",
            "kullback_leibler":     "kullback_leibler_divergence",
            "huber_loss":           huber_loss,
            "epe":                  epe}

        if loss in mapping.keys():
        	fixed_loss = mapping[loss]
        else:
            logging.warning("The following loss is not supported by the keras/tensorflow generator:%s \n", k)
        return fixed_loss

    def translate_eval_metric_names(self, metrics, num_outputs):
        mapping = {
            "accuracy":             "acc",
            "mse":                  "mse",
            "mae":                  "mae",
            "rmse":                 rmse,
            "top_k_accuracy":       "top_k_categorical_accuracy",
            "cross_entropy":        "sparse_categorical_crossentropy" if num_outputs > 1 else "binary_crossentropy",
            "f1":                   f1}

        fixed_metric_names = []
        for k in metrics:
            if k in mapping.keys():
                fixed_metric_names.append(mapping[k])
            elif k != []:
                logging.warning("The following metric is not supported by the keras/tensorflow generator: %s \n", k)
        return fixed_metric_names

    def train(self, batch_size=64,
              num_epoch=10,
              eval_metric=[],
			  loss="cross_entropy",
              loss_weights=None,
              optimizer='adam',
              optimizer_params=(('learning_rate', 0.001),),
              load_checkpoint=True,
              context='gpu',
              checkpoint_period=5,
              normalize=True,
              onnx_export=False):
                                
        if context=="cpu":
            os.environ["CUDA_VISIBLE_DEVICES2"] = '-1'

        dataLoader = CNNDataLoader(self._data_dir_, self._input_names_, self._output_names_, self._output_shapes_)
        train_gen, test_gen, data_mean, data_std, steps_per_epoch, validation_steps = dataLoader.load_data_generators(batch_size, normalize)

        if self.model== None:
            if normalize:
                self.construct(data_mean, data_std)
            else:
                self.construct()

        optimizer_instance, lr_scheduler_params = self.build_optimizer(optimizer, optimizer_params)

        num_outputs = self.model.layers[-1].output_shape[1]
        metrics = self.translate_eval_metric_names([eval_metric], num_outputs)
        tf_loss = self.translate_loss_name(loss, num_outputs)

        begin_epoch = 0
        if load_checkpoint:
            begin_epoch = self.load()
            if begin_epoch == 0:
                if os.path.isdir(self._model_dir_):
                    shutil.rmtree(self._model_dir_)

                self.model.compile(
                    optimizer=optimizer_instance,
                    loss=tf_loss,
                    loss_weights=loss_weights,
                    metrics=metrics)     
        else:
            if os.path.isdir(self._model_dir_):
                shutil.rmtree(self._model_dir_)

            self.model.compile(
                optimizer=optimizer_instance,
                loss=tf_loss,
                loss_weights=loss_weights,
                metrics=metrics)

        try:
            os.makedirs(self._model_dir_)
        except OSError:
            if not os.path.isdir(self._model_dir_):
                raise

        #callbacks
        model_checkpoints_cb = tf.keras.callbacks.ModelCheckpoint(filepath=self._model_dir_ + self._model_prefix_ + "." + "{epoch:d}.hdf5", verbose=0, save_weights_only=False, period=checkpoint_period)

        if "lr_decay" in lr_scheduler_params:
            lr_scheduler = LRScheduler(lr_scheduler_params)
            lr_scheduler_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler.scheduler, 1)

            callbacks = [model_checkpoints_cb, lr_scheduler_cb]
        else:
            callbacks = [model_checkpoints_cb]

        self.model.fit_generator(
            generator=train_gen,
            validation_data=test_gen,
            epochs=num_epoch,
	        validation_steps = validation_steps,
            steps_per_epoch = steps_per_epoch,
			callbacks=callbacks) 


        #saving curent state of model as .h5py for resuming training in python
        tf.keras.models.save_model(self.model, self._model_dir_ + self._model_prefix_ + "." + str(num_epoch + begin_epoch) + ".hdf5")
        tf.keras.models.save_model(self.model, self._model_dir_ + self._model_prefix_ + ".newest.hdf5")

        #Saving model in .pb format for prediction in c++
        saver = tf.train.Saver()
        sess = tf.keras.backend.get_session()
        save_path = saver.save(sess, self._model_dir_ + self._model_prefix_ + "_cpp_pred")

        if onnx_export:
            import onnx
            import onnxmltools
            onnx_model = onnxmltools.convert_keras(self.model)
            onnx.save(onnx_model, self._model_dir_ + self._model_prefix_ + ".onnx")

    def construct(self, data_mean=None, data_std=None):
	
        input_tensors = []
        output_names = []

                                
#************* Start Stream 0*****************************
        
               
        data_ = tf.keras.layers.Input(shape=(3,224,224), name="data_")
        input_tensors.append(data_)      
        
        # We Want channels last for tensorflow
        # "tf_hwc_permute" name is used to check loaded networks for already existed permutation level, see LoadNetwork.ftl
        data_ = tf.keras.layers.Permute((2,3,1), name="tf_hwc_permute")(data_)

        # data_, output shape: {[224,224,3]}

            

        if not data_mean is None:
            assert(not data_std is None)

            data_  = tf.keras.layers.Lambda(lambda x : (x - data_mean["data"])/data_std["data"])(data_)
            data_ = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(data_)
	
        conv1_ = tf.keras.layers.Conv2D(64, 
                                                 kernel_size=(7,7), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv1_")(data_)
        # conv1_, output shape: {[112,112,64]}

        batchnorm1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm1_",)(conv1_) #TODO: fix_gamma=True

        relu1_ = tf.keras.layers.Activation(activation = "relu", name="relu1_")(batchnorm1_)

        pool1_ = tf.keras.layers.MaxPool2D(pool_size = (3,3), #or element.poolsize?
            strides = (2,2), #or element.strides?  (plural)
            padding="same",            
            data_format = "channels_last",
            name="pool1_")(relu1_)
        # pool1_, output shape: {[56,56,64]}

        conv3_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_1_")(pool1_)
        # conv3_1_1_, output shape: {[56,56,4]}

        batchnorm3_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_1_",)(conv3_1_1_) #TODO: fix_gamma=True

        relu3_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_1_")(batchnorm3_1_1_)

        conv4_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_1_")(relu3_1_1_)
        # conv4_1_1_, output shape: {[56,56,4]}

        batchnorm4_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_1_",)(conv4_1_1_) #TODO: fix_gamma=True

        relu4_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_1_")(batchnorm4_1_1_)

        conv5_1_1_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_1_")(relu4_1_1_)
        # conv5_1_1_, output shape: {[56,56,256]}

        batchnorm5_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_1_",)(conv5_1_1_) #TODO: fix_gamma=True

        conv3_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_2_")(pool1_)
        # conv3_1_2_, output shape: {[56,56,4]}

        batchnorm3_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_2_",)(conv3_1_2_) #TODO: fix_gamma=True

        relu3_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_2_")(batchnorm3_1_2_)

        conv4_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_2_")(relu3_1_2_)
        # conv4_1_2_, output shape: {[56,56,4]}

        batchnorm4_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_2_",)(conv4_1_2_) #TODO: fix_gamma=True

        relu4_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_2_")(batchnorm4_1_2_)

        conv5_1_2_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_2_")(relu4_1_2_)
        # conv5_1_2_, output shape: {[56,56,256]}

        batchnorm5_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_2_",)(conv5_1_2_) #TODO: fix_gamma=True

        conv3_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_3_")(pool1_)
        # conv3_1_3_, output shape: {[56,56,4]}

        batchnorm3_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_3_",)(conv3_1_3_) #TODO: fix_gamma=True

        relu3_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_3_")(batchnorm3_1_3_)

        conv4_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_3_")(relu3_1_3_)
        # conv4_1_3_, output shape: {[56,56,4]}

        batchnorm4_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_3_",)(conv4_1_3_) #TODO: fix_gamma=True

        relu4_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_3_")(batchnorm4_1_3_)

        conv5_1_3_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_3_")(relu4_1_3_)
        # conv5_1_3_, output shape: {[56,56,256]}

        batchnorm5_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_3_",)(conv5_1_3_) #TODO: fix_gamma=True

        conv3_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_4_")(pool1_)
        # conv3_1_4_, output shape: {[56,56,4]}

        batchnorm3_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_4_",)(conv3_1_4_) #TODO: fix_gamma=True

        relu3_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_4_")(batchnorm3_1_4_)

        conv4_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_4_")(relu3_1_4_)
        # conv4_1_4_, output shape: {[56,56,4]}

        batchnorm4_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_4_",)(conv4_1_4_) #TODO: fix_gamma=True

        relu4_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_4_")(batchnorm4_1_4_)

        conv5_1_4_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_4_")(relu4_1_4_)
        # conv5_1_4_, output shape: {[56,56,256]}

        batchnorm5_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_4_",)(conv5_1_4_) #TODO: fix_gamma=True

        conv3_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_5_")(pool1_)
        # conv3_1_5_, output shape: {[56,56,4]}

        batchnorm3_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_5_",)(conv3_1_5_) #TODO: fix_gamma=True

        relu3_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_5_")(batchnorm3_1_5_)

        conv4_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_5_")(relu3_1_5_)
        # conv4_1_5_, output shape: {[56,56,4]}

        batchnorm4_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_5_",)(conv4_1_5_) #TODO: fix_gamma=True

        relu4_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_5_")(batchnorm4_1_5_)

        conv5_1_5_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_5_")(relu4_1_5_)
        # conv5_1_5_, output shape: {[56,56,256]}

        batchnorm5_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_5_",)(conv5_1_5_) #TODO: fix_gamma=True

        conv3_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_6_")(pool1_)
        # conv3_1_6_, output shape: {[56,56,4]}

        batchnorm3_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_6_",)(conv3_1_6_) #TODO: fix_gamma=True

        relu3_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_6_")(batchnorm3_1_6_)

        conv4_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_6_")(relu3_1_6_)
        # conv4_1_6_, output shape: {[56,56,4]}

        batchnorm4_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_6_",)(conv4_1_6_) #TODO: fix_gamma=True

        relu4_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_6_")(batchnorm4_1_6_)

        conv5_1_6_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_6_")(relu4_1_6_)
        # conv5_1_6_, output shape: {[56,56,256]}

        batchnorm5_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_6_",)(conv5_1_6_) #TODO: fix_gamma=True

        conv3_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_7_")(pool1_)
        # conv3_1_7_, output shape: {[56,56,4]}

        batchnorm3_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_7_",)(conv3_1_7_) #TODO: fix_gamma=True

        relu3_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_7_")(batchnorm3_1_7_)

        conv4_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_7_")(relu3_1_7_)
        # conv4_1_7_, output shape: {[56,56,4]}

        batchnorm4_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_7_",)(conv4_1_7_) #TODO: fix_gamma=True

        relu4_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_7_")(batchnorm4_1_7_)

        conv5_1_7_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_7_")(relu4_1_7_)
        # conv5_1_7_, output shape: {[56,56,256]}

        batchnorm5_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_7_",)(conv5_1_7_) #TODO: fix_gamma=True

        conv3_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_8_")(pool1_)
        # conv3_1_8_, output shape: {[56,56,4]}

        batchnorm3_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_8_",)(conv3_1_8_) #TODO: fix_gamma=True

        relu3_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_8_")(batchnorm3_1_8_)

        conv4_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_8_")(relu3_1_8_)
        # conv4_1_8_, output shape: {[56,56,4]}

        batchnorm4_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_8_",)(conv4_1_8_) #TODO: fix_gamma=True

        relu4_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_8_")(batchnorm4_1_8_)

        conv5_1_8_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_8_")(relu4_1_8_)
        # conv5_1_8_, output shape: {[56,56,256]}

        batchnorm5_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_8_",)(conv5_1_8_) #TODO: fix_gamma=True

        conv3_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_9_")(pool1_)
        # conv3_1_9_, output shape: {[56,56,4]}

        batchnorm3_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_9_",)(conv3_1_9_) #TODO: fix_gamma=True

        relu3_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_9_")(batchnorm3_1_9_)

        conv4_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_9_")(relu3_1_9_)
        # conv4_1_9_, output shape: {[56,56,4]}

        batchnorm4_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_9_",)(conv4_1_9_) #TODO: fix_gamma=True

        relu4_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_9_")(batchnorm4_1_9_)

        conv5_1_9_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_9_")(relu4_1_9_)
        # conv5_1_9_, output shape: {[56,56,256]}

        batchnorm5_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_9_",)(conv5_1_9_) #TODO: fix_gamma=True

        conv3_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_10_")(pool1_)
        # conv3_1_10_, output shape: {[56,56,4]}

        batchnorm3_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_10_",)(conv3_1_10_) #TODO: fix_gamma=True

        relu3_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_10_")(batchnorm3_1_10_)

        conv4_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_10_")(relu3_1_10_)
        # conv4_1_10_, output shape: {[56,56,4]}

        batchnorm4_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_10_",)(conv4_1_10_) #TODO: fix_gamma=True

        relu4_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_10_")(batchnorm4_1_10_)

        conv5_1_10_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_10_")(relu4_1_10_)
        # conv5_1_10_, output shape: {[56,56,256]}

        batchnorm5_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_10_",)(conv5_1_10_) #TODO: fix_gamma=True

        conv3_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_11_")(pool1_)
        # conv3_1_11_, output shape: {[56,56,4]}

        batchnorm3_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_11_",)(conv3_1_11_) #TODO: fix_gamma=True

        relu3_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_11_")(batchnorm3_1_11_)

        conv4_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_11_")(relu3_1_11_)
        # conv4_1_11_, output shape: {[56,56,4]}

        batchnorm4_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_11_",)(conv4_1_11_) #TODO: fix_gamma=True

        relu4_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_11_")(batchnorm4_1_11_)

        conv5_1_11_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_11_")(relu4_1_11_)
        # conv5_1_11_, output shape: {[56,56,256]}

        batchnorm5_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_11_",)(conv5_1_11_) #TODO: fix_gamma=True

        conv3_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_12_")(pool1_)
        # conv3_1_12_, output shape: {[56,56,4]}

        batchnorm3_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_12_",)(conv3_1_12_) #TODO: fix_gamma=True

        relu3_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_12_")(batchnorm3_1_12_)

        conv4_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_12_")(relu3_1_12_)
        # conv4_1_12_, output shape: {[56,56,4]}

        batchnorm4_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_12_",)(conv4_1_12_) #TODO: fix_gamma=True

        relu4_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_12_")(batchnorm4_1_12_)

        conv5_1_12_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_12_")(relu4_1_12_)
        # conv5_1_12_, output shape: {[56,56,256]}

        batchnorm5_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_12_",)(conv5_1_12_) #TODO: fix_gamma=True

        conv3_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_13_")(pool1_)
        # conv3_1_13_, output shape: {[56,56,4]}

        batchnorm3_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_13_",)(conv3_1_13_) #TODO: fix_gamma=True

        relu3_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_13_")(batchnorm3_1_13_)

        conv4_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_13_")(relu3_1_13_)
        # conv4_1_13_, output shape: {[56,56,4]}

        batchnorm4_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_13_",)(conv4_1_13_) #TODO: fix_gamma=True

        relu4_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_13_")(batchnorm4_1_13_)

        conv5_1_13_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_13_")(relu4_1_13_)
        # conv5_1_13_, output shape: {[56,56,256]}

        batchnorm5_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_13_",)(conv5_1_13_) #TODO: fix_gamma=True

        conv3_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_14_")(pool1_)
        # conv3_1_14_, output shape: {[56,56,4]}

        batchnorm3_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_14_",)(conv3_1_14_) #TODO: fix_gamma=True

        relu3_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_14_")(batchnorm3_1_14_)

        conv4_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_14_")(relu3_1_14_)
        # conv4_1_14_, output shape: {[56,56,4]}

        batchnorm4_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_14_",)(conv4_1_14_) #TODO: fix_gamma=True

        relu4_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_14_")(batchnorm4_1_14_)

        conv5_1_14_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_14_")(relu4_1_14_)
        # conv5_1_14_, output shape: {[56,56,256]}

        batchnorm5_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_14_",)(conv5_1_14_) #TODO: fix_gamma=True

        conv3_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_15_")(pool1_)
        # conv3_1_15_, output shape: {[56,56,4]}

        batchnorm3_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_15_",)(conv3_1_15_) #TODO: fix_gamma=True

        relu3_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_15_")(batchnorm3_1_15_)

        conv4_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_15_")(relu3_1_15_)
        # conv4_1_15_, output shape: {[56,56,4]}

        batchnorm4_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_15_",)(conv4_1_15_) #TODO: fix_gamma=True

        relu4_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_15_")(batchnorm4_1_15_)

        conv5_1_15_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_15_")(relu4_1_15_)
        # conv5_1_15_, output shape: {[56,56,256]}

        batchnorm5_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_15_",)(conv5_1_15_) #TODO: fix_gamma=True

        conv3_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_16_")(pool1_)
        # conv3_1_16_, output shape: {[56,56,4]}

        batchnorm3_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_16_",)(conv3_1_16_) #TODO: fix_gamma=True

        relu3_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_16_")(batchnorm3_1_16_)

        conv4_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_16_")(relu3_1_16_)
        # conv4_1_16_, output shape: {[56,56,4]}

        batchnorm4_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_16_",)(conv4_1_16_) #TODO: fix_gamma=True

        relu4_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_16_")(batchnorm4_1_16_)

        conv5_1_16_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_16_")(relu4_1_16_)
        # conv5_1_16_, output shape: {[56,56,256]}

        batchnorm5_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_16_",)(conv5_1_16_) #TODO: fix_gamma=True

        conv3_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_17_")(pool1_)
        # conv3_1_17_, output shape: {[56,56,4]}

        batchnorm3_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_17_",)(conv3_1_17_) #TODO: fix_gamma=True

        relu3_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_17_")(batchnorm3_1_17_)

        conv4_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_17_")(relu3_1_17_)
        # conv4_1_17_, output shape: {[56,56,4]}

        batchnorm4_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_17_",)(conv4_1_17_) #TODO: fix_gamma=True

        relu4_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_17_")(batchnorm4_1_17_)

        conv5_1_17_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_17_")(relu4_1_17_)
        # conv5_1_17_, output shape: {[56,56,256]}

        batchnorm5_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_17_",)(conv5_1_17_) #TODO: fix_gamma=True

        conv3_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_18_")(pool1_)
        # conv3_1_18_, output shape: {[56,56,4]}

        batchnorm3_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_18_",)(conv3_1_18_) #TODO: fix_gamma=True

        relu3_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_18_")(batchnorm3_1_18_)

        conv4_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_18_")(relu3_1_18_)
        # conv4_1_18_, output shape: {[56,56,4]}

        batchnorm4_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_18_",)(conv4_1_18_) #TODO: fix_gamma=True

        relu4_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_18_")(batchnorm4_1_18_)

        conv5_1_18_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_18_")(relu4_1_18_)
        # conv5_1_18_, output shape: {[56,56,256]}

        batchnorm5_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_18_",)(conv5_1_18_) #TODO: fix_gamma=True

        conv3_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_19_")(pool1_)
        # conv3_1_19_, output shape: {[56,56,4]}

        batchnorm3_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_19_",)(conv3_1_19_) #TODO: fix_gamma=True

        relu3_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_19_")(batchnorm3_1_19_)

        conv4_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_19_")(relu3_1_19_)
        # conv4_1_19_, output shape: {[56,56,4]}

        batchnorm4_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_19_",)(conv4_1_19_) #TODO: fix_gamma=True

        relu4_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_19_")(batchnorm4_1_19_)

        conv5_1_19_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_19_")(relu4_1_19_)
        # conv5_1_19_, output shape: {[56,56,256]}

        batchnorm5_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_19_",)(conv5_1_19_) #TODO: fix_gamma=True

        conv3_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_20_")(pool1_)
        # conv3_1_20_, output shape: {[56,56,4]}

        batchnorm3_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_20_",)(conv3_1_20_) #TODO: fix_gamma=True

        relu3_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_20_")(batchnorm3_1_20_)

        conv4_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_20_")(relu3_1_20_)
        # conv4_1_20_, output shape: {[56,56,4]}

        batchnorm4_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_20_",)(conv4_1_20_) #TODO: fix_gamma=True

        relu4_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_20_")(batchnorm4_1_20_)

        conv5_1_20_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_20_")(relu4_1_20_)
        # conv5_1_20_, output shape: {[56,56,256]}

        batchnorm5_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_20_",)(conv5_1_20_) #TODO: fix_gamma=True

        conv3_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_21_")(pool1_)
        # conv3_1_21_, output shape: {[56,56,4]}

        batchnorm3_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_21_",)(conv3_1_21_) #TODO: fix_gamma=True

        relu3_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_21_")(batchnorm3_1_21_)

        conv4_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_21_")(relu3_1_21_)
        # conv4_1_21_, output shape: {[56,56,4]}

        batchnorm4_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_21_",)(conv4_1_21_) #TODO: fix_gamma=True

        relu4_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_21_")(batchnorm4_1_21_)

        conv5_1_21_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_21_")(relu4_1_21_)
        # conv5_1_21_, output shape: {[56,56,256]}

        batchnorm5_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_21_",)(conv5_1_21_) #TODO: fix_gamma=True

        conv3_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_22_")(pool1_)
        # conv3_1_22_, output shape: {[56,56,4]}

        batchnorm3_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_22_",)(conv3_1_22_) #TODO: fix_gamma=True

        relu3_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_22_")(batchnorm3_1_22_)

        conv4_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_22_")(relu3_1_22_)
        # conv4_1_22_, output shape: {[56,56,4]}

        batchnorm4_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_22_",)(conv4_1_22_) #TODO: fix_gamma=True

        relu4_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_22_")(batchnorm4_1_22_)

        conv5_1_22_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_22_")(relu4_1_22_)
        # conv5_1_22_, output shape: {[56,56,256]}

        batchnorm5_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_22_",)(conv5_1_22_) #TODO: fix_gamma=True

        conv3_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_23_")(pool1_)
        # conv3_1_23_, output shape: {[56,56,4]}

        batchnorm3_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_23_",)(conv3_1_23_) #TODO: fix_gamma=True

        relu3_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_23_")(batchnorm3_1_23_)

        conv4_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_23_")(relu3_1_23_)
        # conv4_1_23_, output shape: {[56,56,4]}

        batchnorm4_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_23_",)(conv4_1_23_) #TODO: fix_gamma=True

        relu4_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_23_")(batchnorm4_1_23_)

        conv5_1_23_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_23_")(relu4_1_23_)
        # conv5_1_23_, output shape: {[56,56,256]}

        batchnorm5_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_23_",)(conv5_1_23_) #TODO: fix_gamma=True

        conv3_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_24_")(pool1_)
        # conv3_1_24_, output shape: {[56,56,4]}

        batchnorm3_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_24_",)(conv3_1_24_) #TODO: fix_gamma=True

        relu3_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_24_")(batchnorm3_1_24_)

        conv4_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_24_")(relu3_1_24_)
        # conv4_1_24_, output shape: {[56,56,4]}

        batchnorm4_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_24_",)(conv4_1_24_) #TODO: fix_gamma=True

        relu4_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_24_")(batchnorm4_1_24_)

        conv5_1_24_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_24_")(relu4_1_24_)
        # conv5_1_24_, output shape: {[56,56,256]}

        batchnorm5_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_24_",)(conv5_1_24_) #TODO: fix_gamma=True

        conv3_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_25_")(pool1_)
        # conv3_1_25_, output shape: {[56,56,4]}

        batchnorm3_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_25_",)(conv3_1_25_) #TODO: fix_gamma=True

        relu3_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_25_")(batchnorm3_1_25_)

        conv4_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_25_")(relu3_1_25_)
        # conv4_1_25_, output shape: {[56,56,4]}

        batchnorm4_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_25_",)(conv4_1_25_) #TODO: fix_gamma=True

        relu4_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_25_")(batchnorm4_1_25_)

        conv5_1_25_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_25_")(relu4_1_25_)
        # conv5_1_25_, output shape: {[56,56,256]}

        batchnorm5_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_25_",)(conv5_1_25_) #TODO: fix_gamma=True

        conv3_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_26_")(pool1_)
        # conv3_1_26_, output shape: {[56,56,4]}

        batchnorm3_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_26_",)(conv3_1_26_) #TODO: fix_gamma=True

        relu3_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_26_")(batchnorm3_1_26_)

        conv4_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_26_")(relu3_1_26_)
        # conv4_1_26_, output shape: {[56,56,4]}

        batchnorm4_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_26_",)(conv4_1_26_) #TODO: fix_gamma=True

        relu4_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_26_")(batchnorm4_1_26_)

        conv5_1_26_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_26_")(relu4_1_26_)
        # conv5_1_26_, output shape: {[56,56,256]}

        batchnorm5_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_26_",)(conv5_1_26_) #TODO: fix_gamma=True

        conv3_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_27_")(pool1_)
        # conv3_1_27_, output shape: {[56,56,4]}

        batchnorm3_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_27_",)(conv3_1_27_) #TODO: fix_gamma=True

        relu3_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_27_")(batchnorm3_1_27_)

        conv4_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_27_")(relu3_1_27_)
        # conv4_1_27_, output shape: {[56,56,4]}

        batchnorm4_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_27_",)(conv4_1_27_) #TODO: fix_gamma=True

        relu4_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_27_")(batchnorm4_1_27_)

        conv5_1_27_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_27_")(relu4_1_27_)
        # conv5_1_27_, output shape: {[56,56,256]}

        batchnorm5_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_27_",)(conv5_1_27_) #TODO: fix_gamma=True

        conv3_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_28_")(pool1_)
        # conv3_1_28_, output shape: {[56,56,4]}

        batchnorm3_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_28_",)(conv3_1_28_) #TODO: fix_gamma=True

        relu3_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_28_")(batchnorm3_1_28_)

        conv4_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_28_")(relu3_1_28_)
        # conv4_1_28_, output shape: {[56,56,4]}

        batchnorm4_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_28_",)(conv4_1_28_) #TODO: fix_gamma=True

        relu4_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_28_")(batchnorm4_1_28_)

        conv5_1_28_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_28_")(relu4_1_28_)
        # conv5_1_28_, output shape: {[56,56,256]}

        batchnorm5_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_28_",)(conv5_1_28_) #TODO: fix_gamma=True

        conv3_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_29_")(pool1_)
        # conv3_1_29_, output shape: {[56,56,4]}

        batchnorm3_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_29_",)(conv3_1_29_) #TODO: fix_gamma=True

        relu3_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_29_")(batchnorm3_1_29_)

        conv4_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_29_")(relu3_1_29_)
        # conv4_1_29_, output shape: {[56,56,4]}

        batchnorm4_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_29_",)(conv4_1_29_) #TODO: fix_gamma=True

        relu4_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_29_")(batchnorm4_1_29_)

        conv5_1_29_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_29_")(relu4_1_29_)
        # conv5_1_29_, output shape: {[56,56,256]}

        batchnorm5_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_29_",)(conv5_1_29_) #TODO: fix_gamma=True

        conv3_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_30_")(pool1_)
        # conv3_1_30_, output shape: {[56,56,4]}

        batchnorm3_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_30_",)(conv3_1_30_) #TODO: fix_gamma=True

        relu3_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_30_")(batchnorm3_1_30_)

        conv4_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_30_")(relu3_1_30_)
        # conv4_1_30_, output shape: {[56,56,4]}

        batchnorm4_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_30_",)(conv4_1_30_) #TODO: fix_gamma=True

        relu4_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_30_")(batchnorm4_1_30_)

        conv5_1_30_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_30_")(relu4_1_30_)
        # conv5_1_30_, output shape: {[56,56,256]}

        batchnorm5_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_30_",)(conv5_1_30_) #TODO: fix_gamma=True

        conv3_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_31_")(pool1_)
        # conv3_1_31_, output shape: {[56,56,4]}

        batchnorm3_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_31_",)(conv3_1_31_) #TODO: fix_gamma=True

        relu3_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_31_")(batchnorm3_1_31_)

        conv4_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_31_")(relu3_1_31_)
        # conv4_1_31_, output shape: {[56,56,4]}

        batchnorm4_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_31_",)(conv4_1_31_) #TODO: fix_gamma=True

        relu4_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_31_")(batchnorm4_1_31_)

        conv5_1_31_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_31_")(relu4_1_31_)
        # conv5_1_31_, output shape: {[56,56,256]}

        batchnorm5_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_31_",)(conv5_1_31_) #TODO: fix_gamma=True

        conv3_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv3_1_32_")(pool1_)
        # conv3_1_32_, output shape: {[56,56,4]}

        batchnorm3_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm3_1_32_",)(conv3_1_32_) #TODO: fix_gamma=True

        relu3_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu3_1_32_")(batchnorm3_1_32_)

        conv4_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv4_1_32_")(relu3_1_32_)
        # conv4_1_32_, output shape: {[56,56,4]}

        batchnorm4_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm4_1_32_",)(conv4_1_32_) #TODO: fix_gamma=True

        relu4_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu4_1_32_")(batchnorm4_1_32_)

        conv5_1_32_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv5_1_32_")(relu4_1_32_)
        # conv5_1_32_, output shape: {[56,56,256]}

        batchnorm5_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm5_1_32_",)(conv5_1_32_) #TODO: fix_gamma=True

        add6_1_ = tf.keras.layers.Add()([batchnorm5_1_1_,  batchnorm5_1_2_,  batchnorm5_1_3_,  batchnorm5_1_4_,  batchnorm5_1_5_,  batchnorm5_1_6_,  batchnorm5_1_7_,  batchnorm5_1_8_,  batchnorm5_1_9_,  batchnorm5_1_10_,  batchnorm5_1_11_,  batchnorm5_1_12_,  batchnorm5_1_13_,  batchnorm5_1_14_,  batchnorm5_1_15_,  batchnorm5_1_16_,  batchnorm5_1_17_,  batchnorm5_1_18_,  batchnorm5_1_19_,  batchnorm5_1_20_,  batchnorm5_1_21_,  batchnorm5_1_22_,  batchnorm5_1_23_,  batchnorm5_1_24_,  batchnorm5_1_25_,  batchnorm5_1_26_,  batchnorm5_1_27_,  batchnorm5_1_28_,  batchnorm5_1_29_,  batchnorm5_1_30_,  batchnorm5_1_31_,  batchnorm5_1_32_])
        # add6_1_, output shape: {[56,56,256]}

        conv2_2_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv2_2_")(pool1_)
        # conv2_2_, output shape: {[56,56,256]}

        batchnorm2_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm2_2_",)(conv2_2_) #TODO: fix_gamma=True

        add7_ = tf.keras.layers.Add()([add6_1_,  batchnorm2_2_])
        # add7_, output shape: {[56,56,256]}

        relu7_ = tf.keras.layers.Activation(activation = "relu", name="relu7_")(add7_)

        conv9_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_1_")(relu7_)
        # conv9_1_1_, output shape: {[56,56,4]}

        batchnorm9_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_1_",)(conv9_1_1_) #TODO: fix_gamma=True

        relu9_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_1_")(batchnorm9_1_1_)

        conv10_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_1_")(relu9_1_1_)
        # conv10_1_1_, output shape: {[56,56,4]}

        batchnorm10_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_1_",)(conv10_1_1_) #TODO: fix_gamma=True

        relu10_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_1_")(batchnorm10_1_1_)

        conv11_1_1_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_1_")(relu10_1_1_)
        # conv11_1_1_, output shape: {[56,56,256]}

        batchnorm11_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_1_",)(conv11_1_1_) #TODO: fix_gamma=True

        conv9_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_2_")(relu7_)
        # conv9_1_2_, output shape: {[56,56,4]}

        batchnorm9_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_2_",)(conv9_1_2_) #TODO: fix_gamma=True

        relu9_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_2_")(batchnorm9_1_2_)

        conv10_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_2_")(relu9_1_2_)
        # conv10_1_2_, output shape: {[56,56,4]}

        batchnorm10_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_2_",)(conv10_1_2_) #TODO: fix_gamma=True

        relu10_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_2_")(batchnorm10_1_2_)

        conv11_1_2_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_2_")(relu10_1_2_)
        # conv11_1_2_, output shape: {[56,56,256]}

        batchnorm11_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_2_",)(conv11_1_2_) #TODO: fix_gamma=True

        conv9_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_3_")(relu7_)
        # conv9_1_3_, output shape: {[56,56,4]}

        batchnorm9_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_3_",)(conv9_1_3_) #TODO: fix_gamma=True

        relu9_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_3_")(batchnorm9_1_3_)

        conv10_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_3_")(relu9_1_3_)
        # conv10_1_3_, output shape: {[56,56,4]}

        batchnorm10_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_3_",)(conv10_1_3_) #TODO: fix_gamma=True

        relu10_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_3_")(batchnorm10_1_3_)

        conv11_1_3_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_3_")(relu10_1_3_)
        # conv11_1_3_, output shape: {[56,56,256]}

        batchnorm11_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_3_",)(conv11_1_3_) #TODO: fix_gamma=True

        conv9_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_4_")(relu7_)
        # conv9_1_4_, output shape: {[56,56,4]}

        batchnorm9_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_4_",)(conv9_1_4_) #TODO: fix_gamma=True

        relu9_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_4_")(batchnorm9_1_4_)

        conv10_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_4_")(relu9_1_4_)
        # conv10_1_4_, output shape: {[56,56,4]}

        batchnorm10_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_4_",)(conv10_1_4_) #TODO: fix_gamma=True

        relu10_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_4_")(batchnorm10_1_4_)

        conv11_1_4_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_4_")(relu10_1_4_)
        # conv11_1_4_, output shape: {[56,56,256]}

        batchnorm11_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_4_",)(conv11_1_4_) #TODO: fix_gamma=True

        conv9_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_5_")(relu7_)
        # conv9_1_5_, output shape: {[56,56,4]}

        batchnorm9_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_5_",)(conv9_1_5_) #TODO: fix_gamma=True

        relu9_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_5_")(batchnorm9_1_5_)

        conv10_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_5_")(relu9_1_5_)
        # conv10_1_5_, output shape: {[56,56,4]}

        batchnorm10_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_5_",)(conv10_1_5_) #TODO: fix_gamma=True

        relu10_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_5_")(batchnorm10_1_5_)

        conv11_1_5_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_5_")(relu10_1_5_)
        # conv11_1_5_, output shape: {[56,56,256]}

        batchnorm11_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_5_",)(conv11_1_5_) #TODO: fix_gamma=True

        conv9_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_6_")(relu7_)
        # conv9_1_6_, output shape: {[56,56,4]}

        batchnorm9_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_6_",)(conv9_1_6_) #TODO: fix_gamma=True

        relu9_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_6_")(batchnorm9_1_6_)

        conv10_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_6_")(relu9_1_6_)
        # conv10_1_6_, output shape: {[56,56,4]}

        batchnorm10_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_6_",)(conv10_1_6_) #TODO: fix_gamma=True

        relu10_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_6_")(batchnorm10_1_6_)

        conv11_1_6_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_6_")(relu10_1_6_)
        # conv11_1_6_, output shape: {[56,56,256]}

        batchnorm11_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_6_",)(conv11_1_6_) #TODO: fix_gamma=True

        conv9_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_7_")(relu7_)
        # conv9_1_7_, output shape: {[56,56,4]}

        batchnorm9_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_7_",)(conv9_1_7_) #TODO: fix_gamma=True

        relu9_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_7_")(batchnorm9_1_7_)

        conv10_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_7_")(relu9_1_7_)
        # conv10_1_7_, output shape: {[56,56,4]}

        batchnorm10_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_7_",)(conv10_1_7_) #TODO: fix_gamma=True

        relu10_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_7_")(batchnorm10_1_7_)

        conv11_1_7_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_7_")(relu10_1_7_)
        # conv11_1_7_, output shape: {[56,56,256]}

        batchnorm11_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_7_",)(conv11_1_7_) #TODO: fix_gamma=True

        conv9_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_8_")(relu7_)
        # conv9_1_8_, output shape: {[56,56,4]}

        batchnorm9_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_8_",)(conv9_1_8_) #TODO: fix_gamma=True

        relu9_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_8_")(batchnorm9_1_8_)

        conv10_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_8_")(relu9_1_8_)
        # conv10_1_8_, output shape: {[56,56,4]}

        batchnorm10_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_8_",)(conv10_1_8_) #TODO: fix_gamma=True

        relu10_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_8_")(batchnorm10_1_8_)

        conv11_1_8_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_8_")(relu10_1_8_)
        # conv11_1_8_, output shape: {[56,56,256]}

        batchnorm11_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_8_",)(conv11_1_8_) #TODO: fix_gamma=True

        conv9_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_9_")(relu7_)
        # conv9_1_9_, output shape: {[56,56,4]}

        batchnorm9_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_9_",)(conv9_1_9_) #TODO: fix_gamma=True

        relu9_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_9_")(batchnorm9_1_9_)

        conv10_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_9_")(relu9_1_9_)
        # conv10_1_9_, output shape: {[56,56,4]}

        batchnorm10_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_9_",)(conv10_1_9_) #TODO: fix_gamma=True

        relu10_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_9_")(batchnorm10_1_9_)

        conv11_1_9_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_9_")(relu10_1_9_)
        # conv11_1_9_, output shape: {[56,56,256]}

        batchnorm11_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_9_",)(conv11_1_9_) #TODO: fix_gamma=True

        conv9_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_10_")(relu7_)
        # conv9_1_10_, output shape: {[56,56,4]}

        batchnorm9_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_10_",)(conv9_1_10_) #TODO: fix_gamma=True

        relu9_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_10_")(batchnorm9_1_10_)

        conv10_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_10_")(relu9_1_10_)
        # conv10_1_10_, output shape: {[56,56,4]}

        batchnorm10_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_10_",)(conv10_1_10_) #TODO: fix_gamma=True

        relu10_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_10_")(batchnorm10_1_10_)

        conv11_1_10_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_10_")(relu10_1_10_)
        # conv11_1_10_, output shape: {[56,56,256]}

        batchnorm11_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_10_",)(conv11_1_10_) #TODO: fix_gamma=True

        conv9_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_11_")(relu7_)
        # conv9_1_11_, output shape: {[56,56,4]}

        batchnorm9_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_11_",)(conv9_1_11_) #TODO: fix_gamma=True

        relu9_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_11_")(batchnorm9_1_11_)

        conv10_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_11_")(relu9_1_11_)
        # conv10_1_11_, output shape: {[56,56,4]}

        batchnorm10_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_11_",)(conv10_1_11_) #TODO: fix_gamma=True

        relu10_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_11_")(batchnorm10_1_11_)

        conv11_1_11_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_11_")(relu10_1_11_)
        # conv11_1_11_, output shape: {[56,56,256]}

        batchnorm11_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_11_",)(conv11_1_11_) #TODO: fix_gamma=True

        conv9_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_12_")(relu7_)
        # conv9_1_12_, output shape: {[56,56,4]}

        batchnorm9_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_12_",)(conv9_1_12_) #TODO: fix_gamma=True

        relu9_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_12_")(batchnorm9_1_12_)

        conv10_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_12_")(relu9_1_12_)
        # conv10_1_12_, output shape: {[56,56,4]}

        batchnorm10_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_12_",)(conv10_1_12_) #TODO: fix_gamma=True

        relu10_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_12_")(batchnorm10_1_12_)

        conv11_1_12_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_12_")(relu10_1_12_)
        # conv11_1_12_, output shape: {[56,56,256]}

        batchnorm11_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_12_",)(conv11_1_12_) #TODO: fix_gamma=True

        conv9_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_13_")(relu7_)
        # conv9_1_13_, output shape: {[56,56,4]}

        batchnorm9_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_13_",)(conv9_1_13_) #TODO: fix_gamma=True

        relu9_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_13_")(batchnorm9_1_13_)

        conv10_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_13_")(relu9_1_13_)
        # conv10_1_13_, output shape: {[56,56,4]}

        batchnorm10_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_13_",)(conv10_1_13_) #TODO: fix_gamma=True

        relu10_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_13_")(batchnorm10_1_13_)

        conv11_1_13_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_13_")(relu10_1_13_)
        # conv11_1_13_, output shape: {[56,56,256]}

        batchnorm11_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_13_",)(conv11_1_13_) #TODO: fix_gamma=True

        conv9_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_14_")(relu7_)
        # conv9_1_14_, output shape: {[56,56,4]}

        batchnorm9_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_14_",)(conv9_1_14_) #TODO: fix_gamma=True

        relu9_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_14_")(batchnorm9_1_14_)

        conv10_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_14_")(relu9_1_14_)
        # conv10_1_14_, output shape: {[56,56,4]}

        batchnorm10_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_14_",)(conv10_1_14_) #TODO: fix_gamma=True

        relu10_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_14_")(batchnorm10_1_14_)

        conv11_1_14_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_14_")(relu10_1_14_)
        # conv11_1_14_, output shape: {[56,56,256]}

        batchnorm11_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_14_",)(conv11_1_14_) #TODO: fix_gamma=True

        conv9_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_15_")(relu7_)
        # conv9_1_15_, output shape: {[56,56,4]}

        batchnorm9_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_15_",)(conv9_1_15_) #TODO: fix_gamma=True

        relu9_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_15_")(batchnorm9_1_15_)

        conv10_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_15_")(relu9_1_15_)
        # conv10_1_15_, output shape: {[56,56,4]}

        batchnorm10_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_15_",)(conv10_1_15_) #TODO: fix_gamma=True

        relu10_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_15_")(batchnorm10_1_15_)

        conv11_1_15_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_15_")(relu10_1_15_)
        # conv11_1_15_, output shape: {[56,56,256]}

        batchnorm11_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_15_",)(conv11_1_15_) #TODO: fix_gamma=True

        conv9_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_16_")(relu7_)
        # conv9_1_16_, output shape: {[56,56,4]}

        batchnorm9_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_16_",)(conv9_1_16_) #TODO: fix_gamma=True

        relu9_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_16_")(batchnorm9_1_16_)

        conv10_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_16_")(relu9_1_16_)
        # conv10_1_16_, output shape: {[56,56,4]}

        batchnorm10_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_16_",)(conv10_1_16_) #TODO: fix_gamma=True

        relu10_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_16_")(batchnorm10_1_16_)

        conv11_1_16_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_16_")(relu10_1_16_)
        # conv11_1_16_, output shape: {[56,56,256]}

        batchnorm11_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_16_",)(conv11_1_16_) #TODO: fix_gamma=True

        conv9_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_17_")(relu7_)
        # conv9_1_17_, output shape: {[56,56,4]}

        batchnorm9_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_17_",)(conv9_1_17_) #TODO: fix_gamma=True

        relu9_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_17_")(batchnorm9_1_17_)

        conv10_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_17_")(relu9_1_17_)
        # conv10_1_17_, output shape: {[56,56,4]}

        batchnorm10_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_17_",)(conv10_1_17_) #TODO: fix_gamma=True

        relu10_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_17_")(batchnorm10_1_17_)

        conv11_1_17_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_17_")(relu10_1_17_)
        # conv11_1_17_, output shape: {[56,56,256]}

        batchnorm11_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_17_",)(conv11_1_17_) #TODO: fix_gamma=True

        conv9_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_18_")(relu7_)
        # conv9_1_18_, output shape: {[56,56,4]}

        batchnorm9_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_18_",)(conv9_1_18_) #TODO: fix_gamma=True

        relu9_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_18_")(batchnorm9_1_18_)

        conv10_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_18_")(relu9_1_18_)
        # conv10_1_18_, output shape: {[56,56,4]}

        batchnorm10_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_18_",)(conv10_1_18_) #TODO: fix_gamma=True

        relu10_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_18_")(batchnorm10_1_18_)

        conv11_1_18_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_18_")(relu10_1_18_)
        # conv11_1_18_, output shape: {[56,56,256]}

        batchnorm11_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_18_",)(conv11_1_18_) #TODO: fix_gamma=True

        conv9_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_19_")(relu7_)
        # conv9_1_19_, output shape: {[56,56,4]}

        batchnorm9_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_19_",)(conv9_1_19_) #TODO: fix_gamma=True

        relu9_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_19_")(batchnorm9_1_19_)

        conv10_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_19_")(relu9_1_19_)
        # conv10_1_19_, output shape: {[56,56,4]}

        batchnorm10_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_19_",)(conv10_1_19_) #TODO: fix_gamma=True

        relu10_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_19_")(batchnorm10_1_19_)

        conv11_1_19_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_19_")(relu10_1_19_)
        # conv11_1_19_, output shape: {[56,56,256]}

        batchnorm11_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_19_",)(conv11_1_19_) #TODO: fix_gamma=True

        conv9_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_20_")(relu7_)
        # conv9_1_20_, output shape: {[56,56,4]}

        batchnorm9_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_20_",)(conv9_1_20_) #TODO: fix_gamma=True

        relu9_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_20_")(batchnorm9_1_20_)

        conv10_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_20_")(relu9_1_20_)
        # conv10_1_20_, output shape: {[56,56,4]}

        batchnorm10_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_20_",)(conv10_1_20_) #TODO: fix_gamma=True

        relu10_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_20_")(batchnorm10_1_20_)

        conv11_1_20_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_20_")(relu10_1_20_)
        # conv11_1_20_, output shape: {[56,56,256]}

        batchnorm11_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_20_",)(conv11_1_20_) #TODO: fix_gamma=True

        conv9_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_21_")(relu7_)
        # conv9_1_21_, output shape: {[56,56,4]}

        batchnorm9_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_21_",)(conv9_1_21_) #TODO: fix_gamma=True

        relu9_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_21_")(batchnorm9_1_21_)

        conv10_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_21_")(relu9_1_21_)
        # conv10_1_21_, output shape: {[56,56,4]}

        batchnorm10_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_21_",)(conv10_1_21_) #TODO: fix_gamma=True

        relu10_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_21_")(batchnorm10_1_21_)

        conv11_1_21_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_21_")(relu10_1_21_)
        # conv11_1_21_, output shape: {[56,56,256]}

        batchnorm11_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_21_",)(conv11_1_21_) #TODO: fix_gamma=True

        conv9_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_22_")(relu7_)
        # conv9_1_22_, output shape: {[56,56,4]}

        batchnorm9_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_22_",)(conv9_1_22_) #TODO: fix_gamma=True

        relu9_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_22_")(batchnorm9_1_22_)

        conv10_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_22_")(relu9_1_22_)
        # conv10_1_22_, output shape: {[56,56,4]}

        batchnorm10_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_22_",)(conv10_1_22_) #TODO: fix_gamma=True

        relu10_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_22_")(batchnorm10_1_22_)

        conv11_1_22_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_22_")(relu10_1_22_)
        # conv11_1_22_, output shape: {[56,56,256]}

        batchnorm11_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_22_",)(conv11_1_22_) #TODO: fix_gamma=True

        conv9_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_23_")(relu7_)
        # conv9_1_23_, output shape: {[56,56,4]}

        batchnorm9_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_23_",)(conv9_1_23_) #TODO: fix_gamma=True

        relu9_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_23_")(batchnorm9_1_23_)

        conv10_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_23_")(relu9_1_23_)
        # conv10_1_23_, output shape: {[56,56,4]}

        batchnorm10_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_23_",)(conv10_1_23_) #TODO: fix_gamma=True

        relu10_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_23_")(batchnorm10_1_23_)

        conv11_1_23_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_23_")(relu10_1_23_)
        # conv11_1_23_, output shape: {[56,56,256]}

        batchnorm11_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_23_",)(conv11_1_23_) #TODO: fix_gamma=True

        conv9_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_24_")(relu7_)
        # conv9_1_24_, output shape: {[56,56,4]}

        batchnorm9_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_24_",)(conv9_1_24_) #TODO: fix_gamma=True

        relu9_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_24_")(batchnorm9_1_24_)

        conv10_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_24_")(relu9_1_24_)
        # conv10_1_24_, output shape: {[56,56,4]}

        batchnorm10_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_24_",)(conv10_1_24_) #TODO: fix_gamma=True

        relu10_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_24_")(batchnorm10_1_24_)

        conv11_1_24_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_24_")(relu10_1_24_)
        # conv11_1_24_, output shape: {[56,56,256]}

        batchnorm11_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_24_",)(conv11_1_24_) #TODO: fix_gamma=True

        conv9_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_25_")(relu7_)
        # conv9_1_25_, output shape: {[56,56,4]}

        batchnorm9_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_25_",)(conv9_1_25_) #TODO: fix_gamma=True

        relu9_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_25_")(batchnorm9_1_25_)

        conv10_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_25_")(relu9_1_25_)
        # conv10_1_25_, output shape: {[56,56,4]}

        batchnorm10_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_25_",)(conv10_1_25_) #TODO: fix_gamma=True

        relu10_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_25_")(batchnorm10_1_25_)

        conv11_1_25_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_25_")(relu10_1_25_)
        # conv11_1_25_, output shape: {[56,56,256]}

        batchnorm11_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_25_",)(conv11_1_25_) #TODO: fix_gamma=True

        conv9_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_26_")(relu7_)
        # conv9_1_26_, output shape: {[56,56,4]}

        batchnorm9_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_26_",)(conv9_1_26_) #TODO: fix_gamma=True

        relu9_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_26_")(batchnorm9_1_26_)

        conv10_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_26_")(relu9_1_26_)
        # conv10_1_26_, output shape: {[56,56,4]}

        batchnorm10_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_26_",)(conv10_1_26_) #TODO: fix_gamma=True

        relu10_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_26_")(batchnorm10_1_26_)

        conv11_1_26_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_26_")(relu10_1_26_)
        # conv11_1_26_, output shape: {[56,56,256]}

        batchnorm11_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_26_",)(conv11_1_26_) #TODO: fix_gamma=True

        conv9_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_27_")(relu7_)
        # conv9_1_27_, output shape: {[56,56,4]}

        batchnorm9_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_27_",)(conv9_1_27_) #TODO: fix_gamma=True

        relu9_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_27_")(batchnorm9_1_27_)

        conv10_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_27_")(relu9_1_27_)
        # conv10_1_27_, output shape: {[56,56,4]}

        batchnorm10_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_27_",)(conv10_1_27_) #TODO: fix_gamma=True

        relu10_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_27_")(batchnorm10_1_27_)

        conv11_1_27_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_27_")(relu10_1_27_)
        # conv11_1_27_, output shape: {[56,56,256]}

        batchnorm11_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_27_",)(conv11_1_27_) #TODO: fix_gamma=True

        conv9_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_28_")(relu7_)
        # conv9_1_28_, output shape: {[56,56,4]}

        batchnorm9_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_28_",)(conv9_1_28_) #TODO: fix_gamma=True

        relu9_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_28_")(batchnorm9_1_28_)

        conv10_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_28_")(relu9_1_28_)
        # conv10_1_28_, output shape: {[56,56,4]}

        batchnorm10_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_28_",)(conv10_1_28_) #TODO: fix_gamma=True

        relu10_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_28_")(batchnorm10_1_28_)

        conv11_1_28_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_28_")(relu10_1_28_)
        # conv11_1_28_, output shape: {[56,56,256]}

        batchnorm11_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_28_",)(conv11_1_28_) #TODO: fix_gamma=True

        conv9_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_29_")(relu7_)
        # conv9_1_29_, output shape: {[56,56,4]}

        batchnorm9_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_29_",)(conv9_1_29_) #TODO: fix_gamma=True

        relu9_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_29_")(batchnorm9_1_29_)

        conv10_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_29_")(relu9_1_29_)
        # conv10_1_29_, output shape: {[56,56,4]}

        batchnorm10_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_29_",)(conv10_1_29_) #TODO: fix_gamma=True

        relu10_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_29_")(batchnorm10_1_29_)

        conv11_1_29_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_29_")(relu10_1_29_)
        # conv11_1_29_, output shape: {[56,56,256]}

        batchnorm11_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_29_",)(conv11_1_29_) #TODO: fix_gamma=True

        conv9_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_30_")(relu7_)
        # conv9_1_30_, output shape: {[56,56,4]}

        batchnorm9_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_30_",)(conv9_1_30_) #TODO: fix_gamma=True

        relu9_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_30_")(batchnorm9_1_30_)

        conv10_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_30_")(relu9_1_30_)
        # conv10_1_30_, output shape: {[56,56,4]}

        batchnorm10_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_30_",)(conv10_1_30_) #TODO: fix_gamma=True

        relu10_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_30_")(batchnorm10_1_30_)

        conv11_1_30_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_30_")(relu10_1_30_)
        # conv11_1_30_, output shape: {[56,56,256]}

        batchnorm11_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_30_",)(conv11_1_30_) #TODO: fix_gamma=True

        conv9_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_31_")(relu7_)
        # conv9_1_31_, output shape: {[56,56,4]}

        batchnorm9_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_31_",)(conv9_1_31_) #TODO: fix_gamma=True

        relu9_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_31_")(batchnorm9_1_31_)

        conv10_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_31_")(relu9_1_31_)
        # conv10_1_31_, output shape: {[56,56,4]}

        batchnorm10_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_31_",)(conv10_1_31_) #TODO: fix_gamma=True

        relu10_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_31_")(batchnorm10_1_31_)

        conv11_1_31_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_31_")(relu10_1_31_)
        # conv11_1_31_, output shape: {[56,56,256]}

        batchnorm11_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_31_",)(conv11_1_31_) #TODO: fix_gamma=True

        conv9_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv9_1_32_")(relu7_)
        # conv9_1_32_, output shape: {[56,56,4]}

        batchnorm9_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm9_1_32_",)(conv9_1_32_) #TODO: fix_gamma=True

        relu9_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu9_1_32_")(batchnorm9_1_32_)

        conv10_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv10_1_32_")(relu9_1_32_)
        # conv10_1_32_, output shape: {[56,56,4]}

        batchnorm10_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm10_1_32_",)(conv10_1_32_) #TODO: fix_gamma=True

        relu10_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu10_1_32_")(batchnorm10_1_32_)

        conv11_1_32_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv11_1_32_")(relu10_1_32_)
        # conv11_1_32_, output shape: {[56,56,256]}

        batchnorm11_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm11_1_32_",)(conv11_1_32_) #TODO: fix_gamma=True

        add12_1_ = tf.keras.layers.Add()([batchnorm11_1_1_,  batchnorm11_1_2_,  batchnorm11_1_3_,  batchnorm11_1_4_,  batchnorm11_1_5_,  batchnorm11_1_6_,  batchnorm11_1_7_,  batchnorm11_1_8_,  batchnorm11_1_9_,  batchnorm11_1_10_,  batchnorm11_1_11_,  batchnorm11_1_12_,  batchnorm11_1_13_,  batchnorm11_1_14_,  batchnorm11_1_15_,  batchnorm11_1_16_,  batchnorm11_1_17_,  batchnorm11_1_18_,  batchnorm11_1_19_,  batchnorm11_1_20_,  batchnorm11_1_21_,  batchnorm11_1_22_,  batchnorm11_1_23_,  batchnorm11_1_24_,  batchnorm11_1_25_,  batchnorm11_1_26_,  batchnorm11_1_27_,  batchnorm11_1_28_,  batchnorm11_1_29_,  batchnorm11_1_30_,  batchnorm11_1_31_,  batchnorm11_1_32_])
        # add12_1_, output shape: {[56,56,256]}

        add13_ = tf.keras.layers.Add()([add12_1_,  relu7_])
        # add13_, output shape: {[56,56,256]}

        relu13_ = tf.keras.layers.Activation(activation = "relu", name="relu13_")(add13_)

        conv15_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_1_")(relu13_)
        # conv15_1_1_, output shape: {[56,56,4]}

        batchnorm15_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_1_",)(conv15_1_1_) #TODO: fix_gamma=True

        relu15_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_1_")(batchnorm15_1_1_)

        conv16_1_1_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_1_")(relu15_1_1_)
        # conv16_1_1_, output shape: {[56,56,4]}

        batchnorm16_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_1_",)(conv16_1_1_) #TODO: fix_gamma=True

        relu16_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_1_")(batchnorm16_1_1_)

        conv17_1_1_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_1_")(relu16_1_1_)
        # conv17_1_1_, output shape: {[56,56,256]}

        batchnorm17_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_1_",)(conv17_1_1_) #TODO: fix_gamma=True

        conv15_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_2_")(relu13_)
        # conv15_1_2_, output shape: {[56,56,4]}

        batchnorm15_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_2_",)(conv15_1_2_) #TODO: fix_gamma=True

        relu15_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_2_")(batchnorm15_1_2_)

        conv16_1_2_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_2_")(relu15_1_2_)
        # conv16_1_2_, output shape: {[56,56,4]}

        batchnorm16_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_2_",)(conv16_1_2_) #TODO: fix_gamma=True

        relu16_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_2_")(batchnorm16_1_2_)

        conv17_1_2_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_2_")(relu16_1_2_)
        # conv17_1_2_, output shape: {[56,56,256]}

        batchnorm17_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_2_",)(conv17_1_2_) #TODO: fix_gamma=True

        conv15_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_3_")(relu13_)
        # conv15_1_3_, output shape: {[56,56,4]}

        batchnorm15_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_3_",)(conv15_1_3_) #TODO: fix_gamma=True

        relu15_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_3_")(batchnorm15_1_3_)

        conv16_1_3_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_3_")(relu15_1_3_)
        # conv16_1_3_, output shape: {[56,56,4]}

        batchnorm16_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_3_",)(conv16_1_3_) #TODO: fix_gamma=True

        relu16_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_3_")(batchnorm16_1_3_)

        conv17_1_3_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_3_")(relu16_1_3_)
        # conv17_1_3_, output shape: {[56,56,256]}

        batchnorm17_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_3_",)(conv17_1_3_) #TODO: fix_gamma=True

        conv15_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_4_")(relu13_)
        # conv15_1_4_, output shape: {[56,56,4]}

        batchnorm15_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_4_",)(conv15_1_4_) #TODO: fix_gamma=True

        relu15_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_4_")(batchnorm15_1_4_)

        conv16_1_4_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_4_")(relu15_1_4_)
        # conv16_1_4_, output shape: {[56,56,4]}

        batchnorm16_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_4_",)(conv16_1_4_) #TODO: fix_gamma=True

        relu16_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_4_")(batchnorm16_1_4_)

        conv17_1_4_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_4_")(relu16_1_4_)
        # conv17_1_4_, output shape: {[56,56,256]}

        batchnorm17_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_4_",)(conv17_1_4_) #TODO: fix_gamma=True

        conv15_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_5_")(relu13_)
        # conv15_1_5_, output shape: {[56,56,4]}

        batchnorm15_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_5_",)(conv15_1_5_) #TODO: fix_gamma=True

        relu15_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_5_")(batchnorm15_1_5_)

        conv16_1_5_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_5_")(relu15_1_5_)
        # conv16_1_5_, output shape: {[56,56,4]}

        batchnorm16_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_5_",)(conv16_1_5_) #TODO: fix_gamma=True

        relu16_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_5_")(batchnorm16_1_5_)

        conv17_1_5_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_5_")(relu16_1_5_)
        # conv17_1_5_, output shape: {[56,56,256]}

        batchnorm17_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_5_",)(conv17_1_5_) #TODO: fix_gamma=True

        conv15_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_6_")(relu13_)
        # conv15_1_6_, output shape: {[56,56,4]}

        batchnorm15_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_6_",)(conv15_1_6_) #TODO: fix_gamma=True

        relu15_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_6_")(batchnorm15_1_6_)

        conv16_1_6_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_6_")(relu15_1_6_)
        # conv16_1_6_, output shape: {[56,56,4]}

        batchnorm16_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_6_",)(conv16_1_6_) #TODO: fix_gamma=True

        relu16_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_6_")(batchnorm16_1_6_)

        conv17_1_6_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_6_")(relu16_1_6_)
        # conv17_1_6_, output shape: {[56,56,256]}

        batchnorm17_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_6_",)(conv17_1_6_) #TODO: fix_gamma=True

        conv15_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_7_")(relu13_)
        # conv15_1_7_, output shape: {[56,56,4]}

        batchnorm15_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_7_",)(conv15_1_7_) #TODO: fix_gamma=True

        relu15_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_7_")(batchnorm15_1_7_)

        conv16_1_7_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_7_")(relu15_1_7_)
        # conv16_1_7_, output shape: {[56,56,4]}

        batchnorm16_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_7_",)(conv16_1_7_) #TODO: fix_gamma=True

        relu16_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_7_")(batchnorm16_1_7_)

        conv17_1_7_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_7_")(relu16_1_7_)
        # conv17_1_7_, output shape: {[56,56,256]}

        batchnorm17_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_7_",)(conv17_1_7_) #TODO: fix_gamma=True

        conv15_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_8_")(relu13_)
        # conv15_1_8_, output shape: {[56,56,4]}

        batchnorm15_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_8_",)(conv15_1_8_) #TODO: fix_gamma=True

        relu15_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_8_")(batchnorm15_1_8_)

        conv16_1_8_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_8_")(relu15_1_8_)
        # conv16_1_8_, output shape: {[56,56,4]}

        batchnorm16_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_8_",)(conv16_1_8_) #TODO: fix_gamma=True

        relu16_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_8_")(batchnorm16_1_8_)

        conv17_1_8_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_8_")(relu16_1_8_)
        # conv17_1_8_, output shape: {[56,56,256]}

        batchnorm17_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_8_",)(conv17_1_8_) #TODO: fix_gamma=True

        conv15_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_9_")(relu13_)
        # conv15_1_9_, output shape: {[56,56,4]}

        batchnorm15_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_9_",)(conv15_1_9_) #TODO: fix_gamma=True

        relu15_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_9_")(batchnorm15_1_9_)

        conv16_1_9_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_9_")(relu15_1_9_)
        # conv16_1_9_, output shape: {[56,56,4]}

        batchnorm16_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_9_",)(conv16_1_9_) #TODO: fix_gamma=True

        relu16_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_9_")(batchnorm16_1_9_)

        conv17_1_9_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_9_")(relu16_1_9_)
        # conv17_1_9_, output shape: {[56,56,256]}

        batchnorm17_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_9_",)(conv17_1_9_) #TODO: fix_gamma=True

        conv15_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_10_")(relu13_)
        # conv15_1_10_, output shape: {[56,56,4]}

        batchnorm15_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_10_",)(conv15_1_10_) #TODO: fix_gamma=True

        relu15_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_10_")(batchnorm15_1_10_)

        conv16_1_10_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_10_")(relu15_1_10_)
        # conv16_1_10_, output shape: {[56,56,4]}

        batchnorm16_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_10_",)(conv16_1_10_) #TODO: fix_gamma=True

        relu16_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_10_")(batchnorm16_1_10_)

        conv17_1_10_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_10_")(relu16_1_10_)
        # conv17_1_10_, output shape: {[56,56,256]}

        batchnorm17_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_10_",)(conv17_1_10_) #TODO: fix_gamma=True

        conv15_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_11_")(relu13_)
        # conv15_1_11_, output shape: {[56,56,4]}

        batchnorm15_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_11_",)(conv15_1_11_) #TODO: fix_gamma=True

        relu15_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_11_")(batchnorm15_1_11_)

        conv16_1_11_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_11_")(relu15_1_11_)
        # conv16_1_11_, output shape: {[56,56,4]}

        batchnorm16_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_11_",)(conv16_1_11_) #TODO: fix_gamma=True

        relu16_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_11_")(batchnorm16_1_11_)

        conv17_1_11_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_11_")(relu16_1_11_)
        # conv17_1_11_, output shape: {[56,56,256]}

        batchnorm17_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_11_",)(conv17_1_11_) #TODO: fix_gamma=True

        conv15_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_12_")(relu13_)
        # conv15_1_12_, output shape: {[56,56,4]}

        batchnorm15_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_12_",)(conv15_1_12_) #TODO: fix_gamma=True

        relu15_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_12_")(batchnorm15_1_12_)

        conv16_1_12_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_12_")(relu15_1_12_)
        # conv16_1_12_, output shape: {[56,56,4]}

        batchnorm16_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_12_",)(conv16_1_12_) #TODO: fix_gamma=True

        relu16_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_12_")(batchnorm16_1_12_)

        conv17_1_12_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_12_")(relu16_1_12_)
        # conv17_1_12_, output shape: {[56,56,256]}

        batchnorm17_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_12_",)(conv17_1_12_) #TODO: fix_gamma=True

        conv15_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_13_")(relu13_)
        # conv15_1_13_, output shape: {[56,56,4]}

        batchnorm15_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_13_",)(conv15_1_13_) #TODO: fix_gamma=True

        relu15_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_13_")(batchnorm15_1_13_)

        conv16_1_13_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_13_")(relu15_1_13_)
        # conv16_1_13_, output shape: {[56,56,4]}

        batchnorm16_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_13_",)(conv16_1_13_) #TODO: fix_gamma=True

        relu16_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_13_")(batchnorm16_1_13_)

        conv17_1_13_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_13_")(relu16_1_13_)
        # conv17_1_13_, output shape: {[56,56,256]}

        batchnorm17_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_13_",)(conv17_1_13_) #TODO: fix_gamma=True

        conv15_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_14_")(relu13_)
        # conv15_1_14_, output shape: {[56,56,4]}

        batchnorm15_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_14_",)(conv15_1_14_) #TODO: fix_gamma=True

        relu15_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_14_")(batchnorm15_1_14_)

        conv16_1_14_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_14_")(relu15_1_14_)
        # conv16_1_14_, output shape: {[56,56,4]}

        batchnorm16_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_14_",)(conv16_1_14_) #TODO: fix_gamma=True

        relu16_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_14_")(batchnorm16_1_14_)

        conv17_1_14_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_14_")(relu16_1_14_)
        # conv17_1_14_, output shape: {[56,56,256]}

        batchnorm17_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_14_",)(conv17_1_14_) #TODO: fix_gamma=True

        conv15_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_15_")(relu13_)
        # conv15_1_15_, output shape: {[56,56,4]}

        batchnorm15_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_15_",)(conv15_1_15_) #TODO: fix_gamma=True

        relu15_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_15_")(batchnorm15_1_15_)

        conv16_1_15_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_15_")(relu15_1_15_)
        # conv16_1_15_, output shape: {[56,56,4]}

        batchnorm16_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_15_",)(conv16_1_15_) #TODO: fix_gamma=True

        relu16_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_15_")(batchnorm16_1_15_)

        conv17_1_15_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_15_")(relu16_1_15_)
        # conv17_1_15_, output shape: {[56,56,256]}

        batchnorm17_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_15_",)(conv17_1_15_) #TODO: fix_gamma=True

        conv15_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_16_")(relu13_)
        # conv15_1_16_, output shape: {[56,56,4]}

        batchnorm15_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_16_",)(conv15_1_16_) #TODO: fix_gamma=True

        relu15_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_16_")(batchnorm15_1_16_)

        conv16_1_16_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_16_")(relu15_1_16_)
        # conv16_1_16_, output shape: {[56,56,4]}

        batchnorm16_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_16_",)(conv16_1_16_) #TODO: fix_gamma=True

        relu16_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_16_")(batchnorm16_1_16_)

        conv17_1_16_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_16_")(relu16_1_16_)
        # conv17_1_16_, output shape: {[56,56,256]}

        batchnorm17_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_16_",)(conv17_1_16_) #TODO: fix_gamma=True

        conv15_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_17_")(relu13_)
        # conv15_1_17_, output shape: {[56,56,4]}

        batchnorm15_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_17_",)(conv15_1_17_) #TODO: fix_gamma=True

        relu15_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_17_")(batchnorm15_1_17_)

        conv16_1_17_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_17_")(relu15_1_17_)
        # conv16_1_17_, output shape: {[56,56,4]}

        batchnorm16_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_17_",)(conv16_1_17_) #TODO: fix_gamma=True

        relu16_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_17_")(batchnorm16_1_17_)

        conv17_1_17_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_17_")(relu16_1_17_)
        # conv17_1_17_, output shape: {[56,56,256]}

        batchnorm17_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_17_",)(conv17_1_17_) #TODO: fix_gamma=True

        conv15_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_18_")(relu13_)
        # conv15_1_18_, output shape: {[56,56,4]}

        batchnorm15_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_18_",)(conv15_1_18_) #TODO: fix_gamma=True

        relu15_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_18_")(batchnorm15_1_18_)

        conv16_1_18_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_18_")(relu15_1_18_)
        # conv16_1_18_, output shape: {[56,56,4]}

        batchnorm16_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_18_",)(conv16_1_18_) #TODO: fix_gamma=True

        relu16_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_18_")(batchnorm16_1_18_)

        conv17_1_18_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_18_")(relu16_1_18_)
        # conv17_1_18_, output shape: {[56,56,256]}

        batchnorm17_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_18_",)(conv17_1_18_) #TODO: fix_gamma=True

        conv15_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_19_")(relu13_)
        # conv15_1_19_, output shape: {[56,56,4]}

        batchnorm15_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_19_",)(conv15_1_19_) #TODO: fix_gamma=True

        relu15_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_19_")(batchnorm15_1_19_)

        conv16_1_19_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_19_")(relu15_1_19_)
        # conv16_1_19_, output shape: {[56,56,4]}

        batchnorm16_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_19_",)(conv16_1_19_) #TODO: fix_gamma=True

        relu16_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_19_")(batchnorm16_1_19_)

        conv17_1_19_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_19_")(relu16_1_19_)
        # conv17_1_19_, output shape: {[56,56,256]}

        batchnorm17_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_19_",)(conv17_1_19_) #TODO: fix_gamma=True

        conv15_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_20_")(relu13_)
        # conv15_1_20_, output shape: {[56,56,4]}

        batchnorm15_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_20_",)(conv15_1_20_) #TODO: fix_gamma=True

        relu15_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_20_")(batchnorm15_1_20_)

        conv16_1_20_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_20_")(relu15_1_20_)
        # conv16_1_20_, output shape: {[56,56,4]}

        batchnorm16_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_20_",)(conv16_1_20_) #TODO: fix_gamma=True

        relu16_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_20_")(batchnorm16_1_20_)

        conv17_1_20_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_20_")(relu16_1_20_)
        # conv17_1_20_, output shape: {[56,56,256]}

        batchnorm17_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_20_",)(conv17_1_20_) #TODO: fix_gamma=True

        conv15_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_21_")(relu13_)
        # conv15_1_21_, output shape: {[56,56,4]}

        batchnorm15_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_21_",)(conv15_1_21_) #TODO: fix_gamma=True

        relu15_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_21_")(batchnorm15_1_21_)

        conv16_1_21_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_21_")(relu15_1_21_)
        # conv16_1_21_, output shape: {[56,56,4]}

        batchnorm16_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_21_",)(conv16_1_21_) #TODO: fix_gamma=True

        relu16_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_21_")(batchnorm16_1_21_)

        conv17_1_21_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_21_")(relu16_1_21_)
        # conv17_1_21_, output shape: {[56,56,256]}

        batchnorm17_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_21_",)(conv17_1_21_) #TODO: fix_gamma=True

        conv15_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_22_")(relu13_)
        # conv15_1_22_, output shape: {[56,56,4]}

        batchnorm15_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_22_",)(conv15_1_22_) #TODO: fix_gamma=True

        relu15_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_22_")(batchnorm15_1_22_)

        conv16_1_22_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_22_")(relu15_1_22_)
        # conv16_1_22_, output shape: {[56,56,4]}

        batchnorm16_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_22_",)(conv16_1_22_) #TODO: fix_gamma=True

        relu16_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_22_")(batchnorm16_1_22_)

        conv17_1_22_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_22_")(relu16_1_22_)
        # conv17_1_22_, output shape: {[56,56,256]}

        batchnorm17_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_22_",)(conv17_1_22_) #TODO: fix_gamma=True

        conv15_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_23_")(relu13_)
        # conv15_1_23_, output shape: {[56,56,4]}

        batchnorm15_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_23_",)(conv15_1_23_) #TODO: fix_gamma=True

        relu15_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_23_")(batchnorm15_1_23_)

        conv16_1_23_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_23_")(relu15_1_23_)
        # conv16_1_23_, output shape: {[56,56,4]}

        batchnorm16_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_23_",)(conv16_1_23_) #TODO: fix_gamma=True

        relu16_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_23_")(batchnorm16_1_23_)

        conv17_1_23_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_23_")(relu16_1_23_)
        # conv17_1_23_, output shape: {[56,56,256]}

        batchnorm17_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_23_",)(conv17_1_23_) #TODO: fix_gamma=True

        conv15_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_24_")(relu13_)
        # conv15_1_24_, output shape: {[56,56,4]}

        batchnorm15_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_24_",)(conv15_1_24_) #TODO: fix_gamma=True

        relu15_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_24_")(batchnorm15_1_24_)

        conv16_1_24_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_24_")(relu15_1_24_)
        # conv16_1_24_, output shape: {[56,56,4]}

        batchnorm16_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_24_",)(conv16_1_24_) #TODO: fix_gamma=True

        relu16_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_24_")(batchnorm16_1_24_)

        conv17_1_24_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_24_")(relu16_1_24_)
        # conv17_1_24_, output shape: {[56,56,256]}

        batchnorm17_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_24_",)(conv17_1_24_) #TODO: fix_gamma=True

        conv15_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_25_")(relu13_)
        # conv15_1_25_, output shape: {[56,56,4]}

        batchnorm15_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_25_",)(conv15_1_25_) #TODO: fix_gamma=True

        relu15_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_25_")(batchnorm15_1_25_)

        conv16_1_25_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_25_")(relu15_1_25_)
        # conv16_1_25_, output shape: {[56,56,4]}

        batchnorm16_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_25_",)(conv16_1_25_) #TODO: fix_gamma=True

        relu16_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_25_")(batchnorm16_1_25_)

        conv17_1_25_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_25_")(relu16_1_25_)
        # conv17_1_25_, output shape: {[56,56,256]}

        batchnorm17_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_25_",)(conv17_1_25_) #TODO: fix_gamma=True

        conv15_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_26_")(relu13_)
        # conv15_1_26_, output shape: {[56,56,4]}

        batchnorm15_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_26_",)(conv15_1_26_) #TODO: fix_gamma=True

        relu15_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_26_")(batchnorm15_1_26_)

        conv16_1_26_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_26_")(relu15_1_26_)
        # conv16_1_26_, output shape: {[56,56,4]}

        batchnorm16_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_26_",)(conv16_1_26_) #TODO: fix_gamma=True

        relu16_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_26_")(batchnorm16_1_26_)

        conv17_1_26_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_26_")(relu16_1_26_)
        # conv17_1_26_, output shape: {[56,56,256]}

        batchnorm17_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_26_",)(conv17_1_26_) #TODO: fix_gamma=True

        conv15_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_27_")(relu13_)
        # conv15_1_27_, output shape: {[56,56,4]}

        batchnorm15_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_27_",)(conv15_1_27_) #TODO: fix_gamma=True

        relu15_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_27_")(batchnorm15_1_27_)

        conv16_1_27_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_27_")(relu15_1_27_)
        # conv16_1_27_, output shape: {[56,56,4]}

        batchnorm16_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_27_",)(conv16_1_27_) #TODO: fix_gamma=True

        relu16_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_27_")(batchnorm16_1_27_)

        conv17_1_27_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_27_")(relu16_1_27_)
        # conv17_1_27_, output shape: {[56,56,256]}

        batchnorm17_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_27_",)(conv17_1_27_) #TODO: fix_gamma=True

        conv15_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_28_")(relu13_)
        # conv15_1_28_, output shape: {[56,56,4]}

        batchnorm15_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_28_",)(conv15_1_28_) #TODO: fix_gamma=True

        relu15_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_28_")(batchnorm15_1_28_)

        conv16_1_28_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_28_")(relu15_1_28_)
        # conv16_1_28_, output shape: {[56,56,4]}

        batchnorm16_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_28_",)(conv16_1_28_) #TODO: fix_gamma=True

        relu16_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_28_")(batchnorm16_1_28_)

        conv17_1_28_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_28_")(relu16_1_28_)
        # conv17_1_28_, output shape: {[56,56,256]}

        batchnorm17_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_28_",)(conv17_1_28_) #TODO: fix_gamma=True

        conv15_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_29_")(relu13_)
        # conv15_1_29_, output shape: {[56,56,4]}

        batchnorm15_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_29_",)(conv15_1_29_) #TODO: fix_gamma=True

        relu15_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_29_")(batchnorm15_1_29_)

        conv16_1_29_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_29_")(relu15_1_29_)
        # conv16_1_29_, output shape: {[56,56,4]}

        batchnorm16_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_29_",)(conv16_1_29_) #TODO: fix_gamma=True

        relu16_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_29_")(batchnorm16_1_29_)

        conv17_1_29_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_29_")(relu16_1_29_)
        # conv17_1_29_, output shape: {[56,56,256]}

        batchnorm17_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_29_",)(conv17_1_29_) #TODO: fix_gamma=True

        conv15_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_30_")(relu13_)
        # conv15_1_30_, output shape: {[56,56,4]}

        batchnorm15_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_30_",)(conv15_1_30_) #TODO: fix_gamma=True

        relu15_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_30_")(batchnorm15_1_30_)

        conv16_1_30_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_30_")(relu15_1_30_)
        # conv16_1_30_, output shape: {[56,56,4]}

        batchnorm16_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_30_",)(conv16_1_30_) #TODO: fix_gamma=True

        relu16_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_30_")(batchnorm16_1_30_)

        conv17_1_30_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_30_")(relu16_1_30_)
        # conv17_1_30_, output shape: {[56,56,256]}

        batchnorm17_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_30_",)(conv17_1_30_) #TODO: fix_gamma=True

        conv15_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_31_")(relu13_)
        # conv15_1_31_, output shape: {[56,56,4]}

        batchnorm15_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_31_",)(conv15_1_31_) #TODO: fix_gamma=True

        relu15_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_31_")(batchnorm15_1_31_)

        conv16_1_31_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_31_")(relu15_1_31_)
        # conv16_1_31_, output shape: {[56,56,4]}

        batchnorm16_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_31_",)(conv16_1_31_) #TODO: fix_gamma=True

        relu16_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_31_")(batchnorm16_1_31_)

        conv17_1_31_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_31_")(relu16_1_31_)
        # conv17_1_31_, output shape: {[56,56,256]}

        batchnorm17_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_31_",)(conv17_1_31_) #TODO: fix_gamma=True

        conv15_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv15_1_32_")(relu13_)
        # conv15_1_32_, output shape: {[56,56,4]}

        batchnorm15_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm15_1_32_",)(conv15_1_32_) #TODO: fix_gamma=True

        relu15_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu15_1_32_")(batchnorm15_1_32_)

        conv16_1_32_ = tf.keras.layers.Conv2D(4, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv16_1_32_")(relu15_1_32_)
        # conv16_1_32_, output shape: {[56,56,4]}

        batchnorm16_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm16_1_32_",)(conv16_1_32_) #TODO: fix_gamma=True

        relu16_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu16_1_32_")(batchnorm16_1_32_)

        conv17_1_32_ = tf.keras.layers.Conv2D(256, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv17_1_32_")(relu16_1_32_)
        # conv17_1_32_, output shape: {[56,56,256]}

        batchnorm17_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm17_1_32_",)(conv17_1_32_) #TODO: fix_gamma=True

        add18_1_ = tf.keras.layers.Add()([batchnorm17_1_1_,  batchnorm17_1_2_,  batchnorm17_1_3_,  batchnorm17_1_4_,  batchnorm17_1_5_,  batchnorm17_1_6_,  batchnorm17_1_7_,  batchnorm17_1_8_,  batchnorm17_1_9_,  batchnorm17_1_10_,  batchnorm17_1_11_,  batchnorm17_1_12_,  batchnorm17_1_13_,  batchnorm17_1_14_,  batchnorm17_1_15_,  batchnorm17_1_16_,  batchnorm17_1_17_,  batchnorm17_1_18_,  batchnorm17_1_19_,  batchnorm17_1_20_,  batchnorm17_1_21_,  batchnorm17_1_22_,  batchnorm17_1_23_,  batchnorm17_1_24_,  batchnorm17_1_25_,  batchnorm17_1_26_,  batchnorm17_1_27_,  batchnorm17_1_28_,  batchnorm17_1_29_,  batchnorm17_1_30_,  batchnorm17_1_31_,  batchnorm17_1_32_])
        # add18_1_, output shape: {[56,56,256]}

        add19_ = tf.keras.layers.Add()([add18_1_,  relu13_])
        # add19_, output shape: {[56,56,256]}

        relu19_ = tf.keras.layers.Activation(activation = "relu", name="relu19_")(add19_)

        conv21_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_1_")(relu19_)
        # conv21_1_1_, output shape: {[56,56,8]}

        batchnorm21_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_1_",)(conv21_1_1_) #TODO: fix_gamma=True

        relu21_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_1_")(batchnorm21_1_1_)

        conv22_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_1_")(relu21_1_1_)
        # conv22_1_1_, output shape: {[28,28,8]}

        batchnorm22_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_1_",)(conv22_1_1_) #TODO: fix_gamma=True

        relu22_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_1_")(batchnorm22_1_1_)

        conv23_1_1_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_1_")(relu22_1_1_)
        # conv23_1_1_, output shape: {[28,28,512]}

        batchnorm23_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_1_",)(conv23_1_1_) #TODO: fix_gamma=True

        conv21_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_2_")(relu19_)
        # conv21_1_2_, output shape: {[56,56,8]}

        batchnorm21_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_2_",)(conv21_1_2_) #TODO: fix_gamma=True

        relu21_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_2_")(batchnorm21_1_2_)

        conv22_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_2_")(relu21_1_2_)
        # conv22_1_2_, output shape: {[28,28,8]}

        batchnorm22_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_2_",)(conv22_1_2_) #TODO: fix_gamma=True

        relu22_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_2_")(batchnorm22_1_2_)

        conv23_1_2_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_2_")(relu22_1_2_)
        # conv23_1_2_, output shape: {[28,28,512]}

        batchnorm23_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_2_",)(conv23_1_2_) #TODO: fix_gamma=True

        conv21_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_3_")(relu19_)
        # conv21_1_3_, output shape: {[56,56,8]}

        batchnorm21_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_3_",)(conv21_1_3_) #TODO: fix_gamma=True

        relu21_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_3_")(batchnorm21_1_3_)

        conv22_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_3_")(relu21_1_3_)
        # conv22_1_3_, output shape: {[28,28,8]}

        batchnorm22_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_3_",)(conv22_1_3_) #TODO: fix_gamma=True

        relu22_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_3_")(batchnorm22_1_3_)

        conv23_1_3_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_3_")(relu22_1_3_)
        # conv23_1_3_, output shape: {[28,28,512]}

        batchnorm23_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_3_",)(conv23_1_3_) #TODO: fix_gamma=True

        conv21_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_4_")(relu19_)
        # conv21_1_4_, output shape: {[56,56,8]}

        batchnorm21_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_4_",)(conv21_1_4_) #TODO: fix_gamma=True

        relu21_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_4_")(batchnorm21_1_4_)

        conv22_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_4_")(relu21_1_4_)
        # conv22_1_4_, output shape: {[28,28,8]}

        batchnorm22_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_4_",)(conv22_1_4_) #TODO: fix_gamma=True

        relu22_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_4_")(batchnorm22_1_4_)

        conv23_1_4_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_4_")(relu22_1_4_)
        # conv23_1_4_, output shape: {[28,28,512]}

        batchnorm23_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_4_",)(conv23_1_4_) #TODO: fix_gamma=True

        conv21_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_5_")(relu19_)
        # conv21_1_5_, output shape: {[56,56,8]}

        batchnorm21_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_5_",)(conv21_1_5_) #TODO: fix_gamma=True

        relu21_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_5_")(batchnorm21_1_5_)

        conv22_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_5_")(relu21_1_5_)
        # conv22_1_5_, output shape: {[28,28,8]}

        batchnorm22_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_5_",)(conv22_1_5_) #TODO: fix_gamma=True

        relu22_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_5_")(batchnorm22_1_5_)

        conv23_1_5_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_5_")(relu22_1_5_)
        # conv23_1_5_, output shape: {[28,28,512]}

        batchnorm23_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_5_",)(conv23_1_5_) #TODO: fix_gamma=True

        conv21_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_6_")(relu19_)
        # conv21_1_6_, output shape: {[56,56,8]}

        batchnorm21_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_6_",)(conv21_1_6_) #TODO: fix_gamma=True

        relu21_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_6_")(batchnorm21_1_6_)

        conv22_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_6_")(relu21_1_6_)
        # conv22_1_6_, output shape: {[28,28,8]}

        batchnorm22_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_6_",)(conv22_1_6_) #TODO: fix_gamma=True

        relu22_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_6_")(batchnorm22_1_6_)

        conv23_1_6_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_6_")(relu22_1_6_)
        # conv23_1_6_, output shape: {[28,28,512]}

        batchnorm23_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_6_",)(conv23_1_6_) #TODO: fix_gamma=True

        conv21_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_7_")(relu19_)
        # conv21_1_7_, output shape: {[56,56,8]}

        batchnorm21_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_7_",)(conv21_1_7_) #TODO: fix_gamma=True

        relu21_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_7_")(batchnorm21_1_7_)

        conv22_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_7_")(relu21_1_7_)
        # conv22_1_7_, output shape: {[28,28,8]}

        batchnorm22_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_7_",)(conv22_1_7_) #TODO: fix_gamma=True

        relu22_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_7_")(batchnorm22_1_7_)

        conv23_1_7_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_7_")(relu22_1_7_)
        # conv23_1_7_, output shape: {[28,28,512]}

        batchnorm23_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_7_",)(conv23_1_7_) #TODO: fix_gamma=True

        conv21_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_8_")(relu19_)
        # conv21_1_8_, output shape: {[56,56,8]}

        batchnorm21_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_8_",)(conv21_1_8_) #TODO: fix_gamma=True

        relu21_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_8_")(batchnorm21_1_8_)

        conv22_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_8_")(relu21_1_8_)
        # conv22_1_8_, output shape: {[28,28,8]}

        batchnorm22_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_8_",)(conv22_1_8_) #TODO: fix_gamma=True

        relu22_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_8_")(batchnorm22_1_8_)

        conv23_1_8_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_8_")(relu22_1_8_)
        # conv23_1_8_, output shape: {[28,28,512]}

        batchnorm23_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_8_",)(conv23_1_8_) #TODO: fix_gamma=True

        conv21_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_9_")(relu19_)
        # conv21_1_9_, output shape: {[56,56,8]}

        batchnorm21_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_9_",)(conv21_1_9_) #TODO: fix_gamma=True

        relu21_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_9_")(batchnorm21_1_9_)

        conv22_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_9_")(relu21_1_9_)
        # conv22_1_9_, output shape: {[28,28,8]}

        batchnorm22_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_9_",)(conv22_1_9_) #TODO: fix_gamma=True

        relu22_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_9_")(batchnorm22_1_9_)

        conv23_1_9_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_9_")(relu22_1_9_)
        # conv23_1_9_, output shape: {[28,28,512]}

        batchnorm23_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_9_",)(conv23_1_9_) #TODO: fix_gamma=True

        conv21_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_10_")(relu19_)
        # conv21_1_10_, output shape: {[56,56,8]}

        batchnorm21_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_10_",)(conv21_1_10_) #TODO: fix_gamma=True

        relu21_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_10_")(batchnorm21_1_10_)

        conv22_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_10_")(relu21_1_10_)
        # conv22_1_10_, output shape: {[28,28,8]}

        batchnorm22_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_10_",)(conv22_1_10_) #TODO: fix_gamma=True

        relu22_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_10_")(batchnorm22_1_10_)

        conv23_1_10_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_10_")(relu22_1_10_)
        # conv23_1_10_, output shape: {[28,28,512]}

        batchnorm23_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_10_",)(conv23_1_10_) #TODO: fix_gamma=True

        conv21_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_11_")(relu19_)
        # conv21_1_11_, output shape: {[56,56,8]}

        batchnorm21_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_11_",)(conv21_1_11_) #TODO: fix_gamma=True

        relu21_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_11_")(batchnorm21_1_11_)

        conv22_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_11_")(relu21_1_11_)
        # conv22_1_11_, output shape: {[28,28,8]}

        batchnorm22_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_11_",)(conv22_1_11_) #TODO: fix_gamma=True

        relu22_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_11_")(batchnorm22_1_11_)

        conv23_1_11_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_11_")(relu22_1_11_)
        # conv23_1_11_, output shape: {[28,28,512]}

        batchnorm23_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_11_",)(conv23_1_11_) #TODO: fix_gamma=True

        conv21_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_12_")(relu19_)
        # conv21_1_12_, output shape: {[56,56,8]}

        batchnorm21_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_12_",)(conv21_1_12_) #TODO: fix_gamma=True

        relu21_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_12_")(batchnorm21_1_12_)

        conv22_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_12_")(relu21_1_12_)
        # conv22_1_12_, output shape: {[28,28,8]}

        batchnorm22_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_12_",)(conv22_1_12_) #TODO: fix_gamma=True

        relu22_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_12_")(batchnorm22_1_12_)

        conv23_1_12_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_12_")(relu22_1_12_)
        # conv23_1_12_, output shape: {[28,28,512]}

        batchnorm23_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_12_",)(conv23_1_12_) #TODO: fix_gamma=True

        conv21_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_13_")(relu19_)
        # conv21_1_13_, output shape: {[56,56,8]}

        batchnorm21_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_13_",)(conv21_1_13_) #TODO: fix_gamma=True

        relu21_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_13_")(batchnorm21_1_13_)

        conv22_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_13_")(relu21_1_13_)
        # conv22_1_13_, output shape: {[28,28,8]}

        batchnorm22_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_13_",)(conv22_1_13_) #TODO: fix_gamma=True

        relu22_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_13_")(batchnorm22_1_13_)

        conv23_1_13_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_13_")(relu22_1_13_)
        # conv23_1_13_, output shape: {[28,28,512]}

        batchnorm23_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_13_",)(conv23_1_13_) #TODO: fix_gamma=True

        conv21_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_14_")(relu19_)
        # conv21_1_14_, output shape: {[56,56,8]}

        batchnorm21_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_14_",)(conv21_1_14_) #TODO: fix_gamma=True

        relu21_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_14_")(batchnorm21_1_14_)

        conv22_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_14_")(relu21_1_14_)
        # conv22_1_14_, output shape: {[28,28,8]}

        batchnorm22_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_14_",)(conv22_1_14_) #TODO: fix_gamma=True

        relu22_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_14_")(batchnorm22_1_14_)

        conv23_1_14_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_14_")(relu22_1_14_)
        # conv23_1_14_, output shape: {[28,28,512]}

        batchnorm23_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_14_",)(conv23_1_14_) #TODO: fix_gamma=True

        conv21_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_15_")(relu19_)
        # conv21_1_15_, output shape: {[56,56,8]}

        batchnorm21_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_15_",)(conv21_1_15_) #TODO: fix_gamma=True

        relu21_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_15_")(batchnorm21_1_15_)

        conv22_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_15_")(relu21_1_15_)
        # conv22_1_15_, output shape: {[28,28,8]}

        batchnorm22_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_15_",)(conv22_1_15_) #TODO: fix_gamma=True

        relu22_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_15_")(batchnorm22_1_15_)

        conv23_1_15_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_15_")(relu22_1_15_)
        # conv23_1_15_, output shape: {[28,28,512]}

        batchnorm23_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_15_",)(conv23_1_15_) #TODO: fix_gamma=True

        conv21_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_16_")(relu19_)
        # conv21_1_16_, output shape: {[56,56,8]}

        batchnorm21_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_16_",)(conv21_1_16_) #TODO: fix_gamma=True

        relu21_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_16_")(batchnorm21_1_16_)

        conv22_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_16_")(relu21_1_16_)
        # conv22_1_16_, output shape: {[28,28,8]}

        batchnorm22_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_16_",)(conv22_1_16_) #TODO: fix_gamma=True

        relu22_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_16_")(batchnorm22_1_16_)

        conv23_1_16_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_16_")(relu22_1_16_)
        # conv23_1_16_, output shape: {[28,28,512]}

        batchnorm23_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_16_",)(conv23_1_16_) #TODO: fix_gamma=True

        conv21_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_17_")(relu19_)
        # conv21_1_17_, output shape: {[56,56,8]}

        batchnorm21_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_17_",)(conv21_1_17_) #TODO: fix_gamma=True

        relu21_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_17_")(batchnorm21_1_17_)

        conv22_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_17_")(relu21_1_17_)
        # conv22_1_17_, output shape: {[28,28,8]}

        batchnorm22_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_17_",)(conv22_1_17_) #TODO: fix_gamma=True

        relu22_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_17_")(batchnorm22_1_17_)

        conv23_1_17_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_17_")(relu22_1_17_)
        # conv23_1_17_, output shape: {[28,28,512]}

        batchnorm23_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_17_",)(conv23_1_17_) #TODO: fix_gamma=True

        conv21_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_18_")(relu19_)
        # conv21_1_18_, output shape: {[56,56,8]}

        batchnorm21_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_18_",)(conv21_1_18_) #TODO: fix_gamma=True

        relu21_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_18_")(batchnorm21_1_18_)

        conv22_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_18_")(relu21_1_18_)
        # conv22_1_18_, output shape: {[28,28,8]}

        batchnorm22_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_18_",)(conv22_1_18_) #TODO: fix_gamma=True

        relu22_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_18_")(batchnorm22_1_18_)

        conv23_1_18_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_18_")(relu22_1_18_)
        # conv23_1_18_, output shape: {[28,28,512]}

        batchnorm23_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_18_",)(conv23_1_18_) #TODO: fix_gamma=True

        conv21_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_19_")(relu19_)
        # conv21_1_19_, output shape: {[56,56,8]}

        batchnorm21_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_19_",)(conv21_1_19_) #TODO: fix_gamma=True

        relu21_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_19_")(batchnorm21_1_19_)

        conv22_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_19_")(relu21_1_19_)
        # conv22_1_19_, output shape: {[28,28,8]}

        batchnorm22_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_19_",)(conv22_1_19_) #TODO: fix_gamma=True

        relu22_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_19_")(batchnorm22_1_19_)

        conv23_1_19_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_19_")(relu22_1_19_)
        # conv23_1_19_, output shape: {[28,28,512]}

        batchnorm23_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_19_",)(conv23_1_19_) #TODO: fix_gamma=True

        conv21_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_20_")(relu19_)
        # conv21_1_20_, output shape: {[56,56,8]}

        batchnorm21_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_20_",)(conv21_1_20_) #TODO: fix_gamma=True

        relu21_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_20_")(batchnorm21_1_20_)

        conv22_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_20_")(relu21_1_20_)
        # conv22_1_20_, output shape: {[28,28,8]}

        batchnorm22_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_20_",)(conv22_1_20_) #TODO: fix_gamma=True

        relu22_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_20_")(batchnorm22_1_20_)

        conv23_1_20_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_20_")(relu22_1_20_)
        # conv23_1_20_, output shape: {[28,28,512]}

        batchnorm23_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_20_",)(conv23_1_20_) #TODO: fix_gamma=True

        conv21_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_21_")(relu19_)
        # conv21_1_21_, output shape: {[56,56,8]}

        batchnorm21_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_21_",)(conv21_1_21_) #TODO: fix_gamma=True

        relu21_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_21_")(batchnorm21_1_21_)

        conv22_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_21_")(relu21_1_21_)
        # conv22_1_21_, output shape: {[28,28,8]}

        batchnorm22_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_21_",)(conv22_1_21_) #TODO: fix_gamma=True

        relu22_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_21_")(batchnorm22_1_21_)

        conv23_1_21_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_21_")(relu22_1_21_)
        # conv23_1_21_, output shape: {[28,28,512]}

        batchnorm23_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_21_",)(conv23_1_21_) #TODO: fix_gamma=True

        conv21_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_22_")(relu19_)
        # conv21_1_22_, output shape: {[56,56,8]}

        batchnorm21_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_22_",)(conv21_1_22_) #TODO: fix_gamma=True

        relu21_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_22_")(batchnorm21_1_22_)

        conv22_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_22_")(relu21_1_22_)
        # conv22_1_22_, output shape: {[28,28,8]}

        batchnorm22_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_22_",)(conv22_1_22_) #TODO: fix_gamma=True

        relu22_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_22_")(batchnorm22_1_22_)

        conv23_1_22_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_22_")(relu22_1_22_)
        # conv23_1_22_, output shape: {[28,28,512]}

        batchnorm23_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_22_",)(conv23_1_22_) #TODO: fix_gamma=True

        conv21_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_23_")(relu19_)
        # conv21_1_23_, output shape: {[56,56,8]}

        batchnorm21_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_23_",)(conv21_1_23_) #TODO: fix_gamma=True

        relu21_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_23_")(batchnorm21_1_23_)

        conv22_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_23_")(relu21_1_23_)
        # conv22_1_23_, output shape: {[28,28,8]}

        batchnorm22_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_23_",)(conv22_1_23_) #TODO: fix_gamma=True

        relu22_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_23_")(batchnorm22_1_23_)

        conv23_1_23_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_23_")(relu22_1_23_)
        # conv23_1_23_, output shape: {[28,28,512]}

        batchnorm23_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_23_",)(conv23_1_23_) #TODO: fix_gamma=True

        conv21_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_24_")(relu19_)
        # conv21_1_24_, output shape: {[56,56,8]}

        batchnorm21_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_24_",)(conv21_1_24_) #TODO: fix_gamma=True

        relu21_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_24_")(batchnorm21_1_24_)

        conv22_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_24_")(relu21_1_24_)
        # conv22_1_24_, output shape: {[28,28,8]}

        batchnorm22_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_24_",)(conv22_1_24_) #TODO: fix_gamma=True

        relu22_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_24_")(batchnorm22_1_24_)

        conv23_1_24_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_24_")(relu22_1_24_)
        # conv23_1_24_, output shape: {[28,28,512]}

        batchnorm23_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_24_",)(conv23_1_24_) #TODO: fix_gamma=True

        conv21_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_25_")(relu19_)
        # conv21_1_25_, output shape: {[56,56,8]}

        batchnorm21_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_25_",)(conv21_1_25_) #TODO: fix_gamma=True

        relu21_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_25_")(batchnorm21_1_25_)

        conv22_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_25_")(relu21_1_25_)
        # conv22_1_25_, output shape: {[28,28,8]}

        batchnorm22_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_25_",)(conv22_1_25_) #TODO: fix_gamma=True

        relu22_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_25_")(batchnorm22_1_25_)

        conv23_1_25_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_25_")(relu22_1_25_)
        # conv23_1_25_, output shape: {[28,28,512]}

        batchnorm23_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_25_",)(conv23_1_25_) #TODO: fix_gamma=True

        conv21_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_26_")(relu19_)
        # conv21_1_26_, output shape: {[56,56,8]}

        batchnorm21_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_26_",)(conv21_1_26_) #TODO: fix_gamma=True

        relu21_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_26_")(batchnorm21_1_26_)

        conv22_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_26_")(relu21_1_26_)
        # conv22_1_26_, output shape: {[28,28,8]}

        batchnorm22_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_26_",)(conv22_1_26_) #TODO: fix_gamma=True

        relu22_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_26_")(batchnorm22_1_26_)

        conv23_1_26_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_26_")(relu22_1_26_)
        # conv23_1_26_, output shape: {[28,28,512]}

        batchnorm23_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_26_",)(conv23_1_26_) #TODO: fix_gamma=True

        conv21_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_27_")(relu19_)
        # conv21_1_27_, output shape: {[56,56,8]}

        batchnorm21_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_27_",)(conv21_1_27_) #TODO: fix_gamma=True

        relu21_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_27_")(batchnorm21_1_27_)

        conv22_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_27_")(relu21_1_27_)
        # conv22_1_27_, output shape: {[28,28,8]}

        batchnorm22_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_27_",)(conv22_1_27_) #TODO: fix_gamma=True

        relu22_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_27_")(batchnorm22_1_27_)

        conv23_1_27_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_27_")(relu22_1_27_)
        # conv23_1_27_, output shape: {[28,28,512]}

        batchnorm23_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_27_",)(conv23_1_27_) #TODO: fix_gamma=True

        conv21_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_28_")(relu19_)
        # conv21_1_28_, output shape: {[56,56,8]}

        batchnorm21_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_28_",)(conv21_1_28_) #TODO: fix_gamma=True

        relu21_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_28_")(batchnorm21_1_28_)

        conv22_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_28_")(relu21_1_28_)
        # conv22_1_28_, output shape: {[28,28,8]}

        batchnorm22_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_28_",)(conv22_1_28_) #TODO: fix_gamma=True

        relu22_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_28_")(batchnorm22_1_28_)

        conv23_1_28_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_28_")(relu22_1_28_)
        # conv23_1_28_, output shape: {[28,28,512]}

        batchnorm23_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_28_",)(conv23_1_28_) #TODO: fix_gamma=True

        conv21_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_29_")(relu19_)
        # conv21_1_29_, output shape: {[56,56,8]}

        batchnorm21_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_29_",)(conv21_1_29_) #TODO: fix_gamma=True

        relu21_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_29_")(batchnorm21_1_29_)

        conv22_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_29_")(relu21_1_29_)
        # conv22_1_29_, output shape: {[28,28,8]}

        batchnorm22_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_29_",)(conv22_1_29_) #TODO: fix_gamma=True

        relu22_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_29_")(batchnorm22_1_29_)

        conv23_1_29_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_29_")(relu22_1_29_)
        # conv23_1_29_, output shape: {[28,28,512]}

        batchnorm23_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_29_",)(conv23_1_29_) #TODO: fix_gamma=True

        conv21_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_30_")(relu19_)
        # conv21_1_30_, output shape: {[56,56,8]}

        batchnorm21_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_30_",)(conv21_1_30_) #TODO: fix_gamma=True

        relu21_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_30_")(batchnorm21_1_30_)

        conv22_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_30_")(relu21_1_30_)
        # conv22_1_30_, output shape: {[28,28,8]}

        batchnorm22_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_30_",)(conv22_1_30_) #TODO: fix_gamma=True

        relu22_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_30_")(batchnorm22_1_30_)

        conv23_1_30_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_30_")(relu22_1_30_)
        # conv23_1_30_, output shape: {[28,28,512]}

        batchnorm23_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_30_",)(conv23_1_30_) #TODO: fix_gamma=True

        conv21_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_31_")(relu19_)
        # conv21_1_31_, output shape: {[56,56,8]}

        batchnorm21_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_31_",)(conv21_1_31_) #TODO: fix_gamma=True

        relu21_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_31_")(batchnorm21_1_31_)

        conv22_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_31_")(relu21_1_31_)
        # conv22_1_31_, output shape: {[28,28,8]}

        batchnorm22_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_31_",)(conv22_1_31_) #TODO: fix_gamma=True

        relu22_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_31_")(batchnorm22_1_31_)

        conv23_1_31_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_31_")(relu22_1_31_)
        # conv23_1_31_, output shape: {[28,28,512]}

        batchnorm23_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_31_",)(conv23_1_31_) #TODO: fix_gamma=True

        conv21_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv21_1_32_")(relu19_)
        # conv21_1_32_, output shape: {[56,56,8]}

        batchnorm21_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm21_1_32_",)(conv21_1_32_) #TODO: fix_gamma=True

        relu21_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu21_1_32_")(batchnorm21_1_32_)

        conv22_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv22_1_32_")(relu21_1_32_)
        # conv22_1_32_, output shape: {[28,28,8]}

        batchnorm22_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm22_1_32_",)(conv22_1_32_) #TODO: fix_gamma=True

        relu22_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu22_1_32_")(batchnorm22_1_32_)

        conv23_1_32_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv23_1_32_")(relu22_1_32_)
        # conv23_1_32_, output shape: {[28,28,512]}

        batchnorm23_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm23_1_32_",)(conv23_1_32_) #TODO: fix_gamma=True

        add24_1_ = tf.keras.layers.Add()([batchnorm23_1_1_,  batchnorm23_1_2_,  batchnorm23_1_3_,  batchnorm23_1_4_,  batchnorm23_1_5_,  batchnorm23_1_6_,  batchnorm23_1_7_,  batchnorm23_1_8_,  batchnorm23_1_9_,  batchnorm23_1_10_,  batchnorm23_1_11_,  batchnorm23_1_12_,  batchnorm23_1_13_,  batchnorm23_1_14_,  batchnorm23_1_15_,  batchnorm23_1_16_,  batchnorm23_1_17_,  batchnorm23_1_18_,  batchnorm23_1_19_,  batchnorm23_1_20_,  batchnorm23_1_21_,  batchnorm23_1_22_,  batchnorm23_1_23_,  batchnorm23_1_24_,  batchnorm23_1_25_,  batchnorm23_1_26_,  batchnorm23_1_27_,  batchnorm23_1_28_,  batchnorm23_1_29_,  batchnorm23_1_30_,  batchnorm23_1_31_,  batchnorm23_1_32_])
        # add24_1_, output shape: {[28,28,512]}

        conv20_2_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv20_2_")(relu19_)
        # conv20_2_, output shape: {[28,28,512]}

        batchnorm20_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm20_2_",)(conv20_2_) #TODO: fix_gamma=True

        add25_ = tf.keras.layers.Add()([add24_1_,  batchnorm20_2_])
        # add25_, output shape: {[28,28,512]}

        relu25_ = tf.keras.layers.Activation(activation = "relu", name="relu25_")(add25_)

        conv27_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_1_")(relu25_)
        # conv27_1_1_, output shape: {[28,28,8]}

        batchnorm27_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_1_",)(conv27_1_1_) #TODO: fix_gamma=True

        relu27_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_1_")(batchnorm27_1_1_)

        conv28_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_1_")(relu27_1_1_)
        # conv28_1_1_, output shape: {[28,28,8]}

        batchnorm28_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_1_",)(conv28_1_1_) #TODO: fix_gamma=True

        relu28_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_1_")(batchnorm28_1_1_)

        conv29_1_1_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_1_")(relu28_1_1_)
        # conv29_1_1_, output shape: {[28,28,512]}

        batchnorm29_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_1_",)(conv29_1_1_) #TODO: fix_gamma=True

        conv27_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_2_")(relu25_)
        # conv27_1_2_, output shape: {[28,28,8]}

        batchnorm27_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_2_",)(conv27_1_2_) #TODO: fix_gamma=True

        relu27_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_2_")(batchnorm27_1_2_)

        conv28_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_2_")(relu27_1_2_)
        # conv28_1_2_, output shape: {[28,28,8]}

        batchnorm28_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_2_",)(conv28_1_2_) #TODO: fix_gamma=True

        relu28_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_2_")(batchnorm28_1_2_)

        conv29_1_2_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_2_")(relu28_1_2_)
        # conv29_1_2_, output shape: {[28,28,512]}

        batchnorm29_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_2_",)(conv29_1_2_) #TODO: fix_gamma=True

        conv27_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_3_")(relu25_)
        # conv27_1_3_, output shape: {[28,28,8]}

        batchnorm27_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_3_",)(conv27_1_3_) #TODO: fix_gamma=True

        relu27_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_3_")(batchnorm27_1_3_)

        conv28_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_3_")(relu27_1_3_)
        # conv28_1_3_, output shape: {[28,28,8]}

        batchnorm28_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_3_",)(conv28_1_3_) #TODO: fix_gamma=True

        relu28_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_3_")(batchnorm28_1_3_)

        conv29_1_3_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_3_")(relu28_1_3_)
        # conv29_1_3_, output shape: {[28,28,512]}

        batchnorm29_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_3_",)(conv29_1_3_) #TODO: fix_gamma=True

        conv27_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_4_")(relu25_)
        # conv27_1_4_, output shape: {[28,28,8]}

        batchnorm27_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_4_",)(conv27_1_4_) #TODO: fix_gamma=True

        relu27_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_4_")(batchnorm27_1_4_)

        conv28_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_4_")(relu27_1_4_)
        # conv28_1_4_, output shape: {[28,28,8]}

        batchnorm28_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_4_",)(conv28_1_4_) #TODO: fix_gamma=True

        relu28_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_4_")(batchnorm28_1_4_)

        conv29_1_4_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_4_")(relu28_1_4_)
        # conv29_1_4_, output shape: {[28,28,512]}

        batchnorm29_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_4_",)(conv29_1_4_) #TODO: fix_gamma=True

        conv27_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_5_")(relu25_)
        # conv27_1_5_, output shape: {[28,28,8]}

        batchnorm27_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_5_",)(conv27_1_5_) #TODO: fix_gamma=True

        relu27_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_5_")(batchnorm27_1_5_)

        conv28_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_5_")(relu27_1_5_)
        # conv28_1_5_, output shape: {[28,28,8]}

        batchnorm28_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_5_",)(conv28_1_5_) #TODO: fix_gamma=True

        relu28_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_5_")(batchnorm28_1_5_)

        conv29_1_5_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_5_")(relu28_1_5_)
        # conv29_1_5_, output shape: {[28,28,512]}

        batchnorm29_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_5_",)(conv29_1_5_) #TODO: fix_gamma=True

        conv27_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_6_")(relu25_)
        # conv27_1_6_, output shape: {[28,28,8]}

        batchnorm27_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_6_",)(conv27_1_6_) #TODO: fix_gamma=True

        relu27_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_6_")(batchnorm27_1_6_)

        conv28_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_6_")(relu27_1_6_)
        # conv28_1_6_, output shape: {[28,28,8]}

        batchnorm28_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_6_",)(conv28_1_6_) #TODO: fix_gamma=True

        relu28_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_6_")(batchnorm28_1_6_)

        conv29_1_6_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_6_")(relu28_1_6_)
        # conv29_1_6_, output shape: {[28,28,512]}

        batchnorm29_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_6_",)(conv29_1_6_) #TODO: fix_gamma=True

        conv27_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_7_")(relu25_)
        # conv27_1_7_, output shape: {[28,28,8]}

        batchnorm27_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_7_",)(conv27_1_7_) #TODO: fix_gamma=True

        relu27_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_7_")(batchnorm27_1_7_)

        conv28_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_7_")(relu27_1_7_)
        # conv28_1_7_, output shape: {[28,28,8]}

        batchnorm28_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_7_",)(conv28_1_7_) #TODO: fix_gamma=True

        relu28_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_7_")(batchnorm28_1_7_)

        conv29_1_7_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_7_")(relu28_1_7_)
        # conv29_1_7_, output shape: {[28,28,512]}

        batchnorm29_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_7_",)(conv29_1_7_) #TODO: fix_gamma=True

        conv27_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_8_")(relu25_)
        # conv27_1_8_, output shape: {[28,28,8]}

        batchnorm27_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_8_",)(conv27_1_8_) #TODO: fix_gamma=True

        relu27_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_8_")(batchnorm27_1_8_)

        conv28_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_8_")(relu27_1_8_)
        # conv28_1_8_, output shape: {[28,28,8]}

        batchnorm28_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_8_",)(conv28_1_8_) #TODO: fix_gamma=True

        relu28_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_8_")(batchnorm28_1_8_)

        conv29_1_8_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_8_")(relu28_1_8_)
        # conv29_1_8_, output shape: {[28,28,512]}

        batchnorm29_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_8_",)(conv29_1_8_) #TODO: fix_gamma=True

        conv27_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_9_")(relu25_)
        # conv27_1_9_, output shape: {[28,28,8]}

        batchnorm27_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_9_",)(conv27_1_9_) #TODO: fix_gamma=True

        relu27_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_9_")(batchnorm27_1_9_)

        conv28_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_9_")(relu27_1_9_)
        # conv28_1_9_, output shape: {[28,28,8]}

        batchnorm28_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_9_",)(conv28_1_9_) #TODO: fix_gamma=True

        relu28_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_9_")(batchnorm28_1_9_)

        conv29_1_9_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_9_")(relu28_1_9_)
        # conv29_1_9_, output shape: {[28,28,512]}

        batchnorm29_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_9_",)(conv29_1_9_) #TODO: fix_gamma=True

        conv27_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_10_")(relu25_)
        # conv27_1_10_, output shape: {[28,28,8]}

        batchnorm27_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_10_",)(conv27_1_10_) #TODO: fix_gamma=True

        relu27_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_10_")(batchnorm27_1_10_)

        conv28_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_10_")(relu27_1_10_)
        # conv28_1_10_, output shape: {[28,28,8]}

        batchnorm28_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_10_",)(conv28_1_10_) #TODO: fix_gamma=True

        relu28_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_10_")(batchnorm28_1_10_)

        conv29_1_10_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_10_")(relu28_1_10_)
        # conv29_1_10_, output shape: {[28,28,512]}

        batchnorm29_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_10_",)(conv29_1_10_) #TODO: fix_gamma=True

        conv27_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_11_")(relu25_)
        # conv27_1_11_, output shape: {[28,28,8]}

        batchnorm27_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_11_",)(conv27_1_11_) #TODO: fix_gamma=True

        relu27_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_11_")(batchnorm27_1_11_)

        conv28_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_11_")(relu27_1_11_)
        # conv28_1_11_, output shape: {[28,28,8]}

        batchnorm28_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_11_",)(conv28_1_11_) #TODO: fix_gamma=True

        relu28_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_11_")(batchnorm28_1_11_)

        conv29_1_11_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_11_")(relu28_1_11_)
        # conv29_1_11_, output shape: {[28,28,512]}

        batchnorm29_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_11_",)(conv29_1_11_) #TODO: fix_gamma=True

        conv27_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_12_")(relu25_)
        # conv27_1_12_, output shape: {[28,28,8]}

        batchnorm27_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_12_",)(conv27_1_12_) #TODO: fix_gamma=True

        relu27_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_12_")(batchnorm27_1_12_)

        conv28_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_12_")(relu27_1_12_)
        # conv28_1_12_, output shape: {[28,28,8]}

        batchnorm28_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_12_",)(conv28_1_12_) #TODO: fix_gamma=True

        relu28_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_12_")(batchnorm28_1_12_)

        conv29_1_12_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_12_")(relu28_1_12_)
        # conv29_1_12_, output shape: {[28,28,512]}

        batchnorm29_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_12_",)(conv29_1_12_) #TODO: fix_gamma=True

        conv27_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_13_")(relu25_)
        # conv27_1_13_, output shape: {[28,28,8]}

        batchnorm27_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_13_",)(conv27_1_13_) #TODO: fix_gamma=True

        relu27_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_13_")(batchnorm27_1_13_)

        conv28_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_13_")(relu27_1_13_)
        # conv28_1_13_, output shape: {[28,28,8]}

        batchnorm28_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_13_",)(conv28_1_13_) #TODO: fix_gamma=True

        relu28_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_13_")(batchnorm28_1_13_)

        conv29_1_13_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_13_")(relu28_1_13_)
        # conv29_1_13_, output shape: {[28,28,512]}

        batchnorm29_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_13_",)(conv29_1_13_) #TODO: fix_gamma=True

        conv27_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_14_")(relu25_)
        # conv27_1_14_, output shape: {[28,28,8]}

        batchnorm27_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_14_",)(conv27_1_14_) #TODO: fix_gamma=True

        relu27_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_14_")(batchnorm27_1_14_)

        conv28_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_14_")(relu27_1_14_)
        # conv28_1_14_, output shape: {[28,28,8]}

        batchnorm28_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_14_",)(conv28_1_14_) #TODO: fix_gamma=True

        relu28_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_14_")(batchnorm28_1_14_)

        conv29_1_14_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_14_")(relu28_1_14_)
        # conv29_1_14_, output shape: {[28,28,512]}

        batchnorm29_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_14_",)(conv29_1_14_) #TODO: fix_gamma=True

        conv27_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_15_")(relu25_)
        # conv27_1_15_, output shape: {[28,28,8]}

        batchnorm27_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_15_",)(conv27_1_15_) #TODO: fix_gamma=True

        relu27_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_15_")(batchnorm27_1_15_)

        conv28_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_15_")(relu27_1_15_)
        # conv28_1_15_, output shape: {[28,28,8]}

        batchnorm28_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_15_",)(conv28_1_15_) #TODO: fix_gamma=True

        relu28_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_15_")(batchnorm28_1_15_)

        conv29_1_15_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_15_")(relu28_1_15_)
        # conv29_1_15_, output shape: {[28,28,512]}

        batchnorm29_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_15_",)(conv29_1_15_) #TODO: fix_gamma=True

        conv27_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_16_")(relu25_)
        # conv27_1_16_, output shape: {[28,28,8]}

        batchnorm27_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_16_",)(conv27_1_16_) #TODO: fix_gamma=True

        relu27_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_16_")(batchnorm27_1_16_)

        conv28_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_16_")(relu27_1_16_)
        # conv28_1_16_, output shape: {[28,28,8]}

        batchnorm28_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_16_",)(conv28_1_16_) #TODO: fix_gamma=True

        relu28_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_16_")(batchnorm28_1_16_)

        conv29_1_16_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_16_")(relu28_1_16_)
        # conv29_1_16_, output shape: {[28,28,512]}

        batchnorm29_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_16_",)(conv29_1_16_) #TODO: fix_gamma=True

        conv27_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_17_")(relu25_)
        # conv27_1_17_, output shape: {[28,28,8]}

        batchnorm27_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_17_",)(conv27_1_17_) #TODO: fix_gamma=True

        relu27_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_17_")(batchnorm27_1_17_)

        conv28_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_17_")(relu27_1_17_)
        # conv28_1_17_, output shape: {[28,28,8]}

        batchnorm28_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_17_",)(conv28_1_17_) #TODO: fix_gamma=True

        relu28_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_17_")(batchnorm28_1_17_)

        conv29_1_17_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_17_")(relu28_1_17_)
        # conv29_1_17_, output shape: {[28,28,512]}

        batchnorm29_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_17_",)(conv29_1_17_) #TODO: fix_gamma=True

        conv27_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_18_")(relu25_)
        # conv27_1_18_, output shape: {[28,28,8]}

        batchnorm27_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_18_",)(conv27_1_18_) #TODO: fix_gamma=True

        relu27_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_18_")(batchnorm27_1_18_)

        conv28_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_18_")(relu27_1_18_)
        # conv28_1_18_, output shape: {[28,28,8]}

        batchnorm28_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_18_",)(conv28_1_18_) #TODO: fix_gamma=True

        relu28_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_18_")(batchnorm28_1_18_)

        conv29_1_18_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_18_")(relu28_1_18_)
        # conv29_1_18_, output shape: {[28,28,512]}

        batchnorm29_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_18_",)(conv29_1_18_) #TODO: fix_gamma=True

        conv27_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_19_")(relu25_)
        # conv27_1_19_, output shape: {[28,28,8]}

        batchnorm27_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_19_",)(conv27_1_19_) #TODO: fix_gamma=True

        relu27_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_19_")(batchnorm27_1_19_)

        conv28_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_19_")(relu27_1_19_)
        # conv28_1_19_, output shape: {[28,28,8]}

        batchnorm28_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_19_",)(conv28_1_19_) #TODO: fix_gamma=True

        relu28_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_19_")(batchnorm28_1_19_)

        conv29_1_19_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_19_")(relu28_1_19_)
        # conv29_1_19_, output shape: {[28,28,512]}

        batchnorm29_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_19_",)(conv29_1_19_) #TODO: fix_gamma=True

        conv27_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_20_")(relu25_)
        # conv27_1_20_, output shape: {[28,28,8]}

        batchnorm27_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_20_",)(conv27_1_20_) #TODO: fix_gamma=True

        relu27_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_20_")(batchnorm27_1_20_)

        conv28_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_20_")(relu27_1_20_)
        # conv28_1_20_, output shape: {[28,28,8]}

        batchnorm28_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_20_",)(conv28_1_20_) #TODO: fix_gamma=True

        relu28_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_20_")(batchnorm28_1_20_)

        conv29_1_20_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_20_")(relu28_1_20_)
        # conv29_1_20_, output shape: {[28,28,512]}

        batchnorm29_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_20_",)(conv29_1_20_) #TODO: fix_gamma=True

        conv27_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_21_")(relu25_)
        # conv27_1_21_, output shape: {[28,28,8]}

        batchnorm27_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_21_",)(conv27_1_21_) #TODO: fix_gamma=True

        relu27_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_21_")(batchnorm27_1_21_)

        conv28_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_21_")(relu27_1_21_)
        # conv28_1_21_, output shape: {[28,28,8]}

        batchnorm28_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_21_",)(conv28_1_21_) #TODO: fix_gamma=True

        relu28_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_21_")(batchnorm28_1_21_)

        conv29_1_21_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_21_")(relu28_1_21_)
        # conv29_1_21_, output shape: {[28,28,512]}

        batchnorm29_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_21_",)(conv29_1_21_) #TODO: fix_gamma=True

        conv27_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_22_")(relu25_)
        # conv27_1_22_, output shape: {[28,28,8]}

        batchnorm27_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_22_",)(conv27_1_22_) #TODO: fix_gamma=True

        relu27_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_22_")(batchnorm27_1_22_)

        conv28_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_22_")(relu27_1_22_)
        # conv28_1_22_, output shape: {[28,28,8]}

        batchnorm28_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_22_",)(conv28_1_22_) #TODO: fix_gamma=True

        relu28_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_22_")(batchnorm28_1_22_)

        conv29_1_22_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_22_")(relu28_1_22_)
        # conv29_1_22_, output shape: {[28,28,512]}

        batchnorm29_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_22_",)(conv29_1_22_) #TODO: fix_gamma=True

        conv27_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_23_")(relu25_)
        # conv27_1_23_, output shape: {[28,28,8]}

        batchnorm27_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_23_",)(conv27_1_23_) #TODO: fix_gamma=True

        relu27_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_23_")(batchnorm27_1_23_)

        conv28_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_23_")(relu27_1_23_)
        # conv28_1_23_, output shape: {[28,28,8]}

        batchnorm28_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_23_",)(conv28_1_23_) #TODO: fix_gamma=True

        relu28_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_23_")(batchnorm28_1_23_)

        conv29_1_23_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_23_")(relu28_1_23_)
        # conv29_1_23_, output shape: {[28,28,512]}

        batchnorm29_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_23_",)(conv29_1_23_) #TODO: fix_gamma=True

        conv27_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_24_")(relu25_)
        # conv27_1_24_, output shape: {[28,28,8]}

        batchnorm27_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_24_",)(conv27_1_24_) #TODO: fix_gamma=True

        relu27_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_24_")(batchnorm27_1_24_)

        conv28_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_24_")(relu27_1_24_)
        # conv28_1_24_, output shape: {[28,28,8]}

        batchnorm28_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_24_",)(conv28_1_24_) #TODO: fix_gamma=True

        relu28_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_24_")(batchnorm28_1_24_)

        conv29_1_24_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_24_")(relu28_1_24_)
        # conv29_1_24_, output shape: {[28,28,512]}

        batchnorm29_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_24_",)(conv29_1_24_) #TODO: fix_gamma=True

        conv27_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_25_")(relu25_)
        # conv27_1_25_, output shape: {[28,28,8]}

        batchnorm27_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_25_",)(conv27_1_25_) #TODO: fix_gamma=True

        relu27_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_25_")(batchnorm27_1_25_)

        conv28_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_25_")(relu27_1_25_)
        # conv28_1_25_, output shape: {[28,28,8]}

        batchnorm28_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_25_",)(conv28_1_25_) #TODO: fix_gamma=True

        relu28_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_25_")(batchnorm28_1_25_)

        conv29_1_25_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_25_")(relu28_1_25_)
        # conv29_1_25_, output shape: {[28,28,512]}

        batchnorm29_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_25_",)(conv29_1_25_) #TODO: fix_gamma=True

        conv27_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_26_")(relu25_)
        # conv27_1_26_, output shape: {[28,28,8]}

        batchnorm27_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_26_",)(conv27_1_26_) #TODO: fix_gamma=True

        relu27_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_26_")(batchnorm27_1_26_)

        conv28_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_26_")(relu27_1_26_)
        # conv28_1_26_, output shape: {[28,28,8]}

        batchnorm28_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_26_",)(conv28_1_26_) #TODO: fix_gamma=True

        relu28_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_26_")(batchnorm28_1_26_)

        conv29_1_26_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_26_")(relu28_1_26_)
        # conv29_1_26_, output shape: {[28,28,512]}

        batchnorm29_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_26_",)(conv29_1_26_) #TODO: fix_gamma=True

        conv27_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_27_")(relu25_)
        # conv27_1_27_, output shape: {[28,28,8]}

        batchnorm27_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_27_",)(conv27_1_27_) #TODO: fix_gamma=True

        relu27_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_27_")(batchnorm27_1_27_)

        conv28_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_27_")(relu27_1_27_)
        # conv28_1_27_, output shape: {[28,28,8]}

        batchnorm28_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_27_",)(conv28_1_27_) #TODO: fix_gamma=True

        relu28_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_27_")(batchnorm28_1_27_)

        conv29_1_27_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_27_")(relu28_1_27_)
        # conv29_1_27_, output shape: {[28,28,512]}

        batchnorm29_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_27_",)(conv29_1_27_) #TODO: fix_gamma=True

        conv27_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_28_")(relu25_)
        # conv27_1_28_, output shape: {[28,28,8]}

        batchnorm27_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_28_",)(conv27_1_28_) #TODO: fix_gamma=True

        relu27_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_28_")(batchnorm27_1_28_)

        conv28_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_28_")(relu27_1_28_)
        # conv28_1_28_, output shape: {[28,28,8]}

        batchnorm28_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_28_",)(conv28_1_28_) #TODO: fix_gamma=True

        relu28_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_28_")(batchnorm28_1_28_)

        conv29_1_28_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_28_")(relu28_1_28_)
        # conv29_1_28_, output shape: {[28,28,512]}

        batchnorm29_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_28_",)(conv29_1_28_) #TODO: fix_gamma=True

        conv27_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_29_")(relu25_)
        # conv27_1_29_, output shape: {[28,28,8]}

        batchnorm27_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_29_",)(conv27_1_29_) #TODO: fix_gamma=True

        relu27_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_29_")(batchnorm27_1_29_)

        conv28_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_29_")(relu27_1_29_)
        # conv28_1_29_, output shape: {[28,28,8]}

        batchnorm28_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_29_",)(conv28_1_29_) #TODO: fix_gamma=True

        relu28_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_29_")(batchnorm28_1_29_)

        conv29_1_29_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_29_")(relu28_1_29_)
        # conv29_1_29_, output shape: {[28,28,512]}

        batchnorm29_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_29_",)(conv29_1_29_) #TODO: fix_gamma=True

        conv27_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_30_")(relu25_)
        # conv27_1_30_, output shape: {[28,28,8]}

        batchnorm27_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_30_",)(conv27_1_30_) #TODO: fix_gamma=True

        relu27_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_30_")(batchnorm27_1_30_)

        conv28_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_30_")(relu27_1_30_)
        # conv28_1_30_, output shape: {[28,28,8]}

        batchnorm28_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_30_",)(conv28_1_30_) #TODO: fix_gamma=True

        relu28_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_30_")(batchnorm28_1_30_)

        conv29_1_30_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_30_")(relu28_1_30_)
        # conv29_1_30_, output shape: {[28,28,512]}

        batchnorm29_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_30_",)(conv29_1_30_) #TODO: fix_gamma=True

        conv27_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_31_")(relu25_)
        # conv27_1_31_, output shape: {[28,28,8]}

        batchnorm27_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_31_",)(conv27_1_31_) #TODO: fix_gamma=True

        relu27_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_31_")(batchnorm27_1_31_)

        conv28_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_31_")(relu27_1_31_)
        # conv28_1_31_, output shape: {[28,28,8]}

        batchnorm28_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_31_",)(conv28_1_31_) #TODO: fix_gamma=True

        relu28_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_31_")(batchnorm28_1_31_)

        conv29_1_31_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_31_")(relu28_1_31_)
        # conv29_1_31_, output shape: {[28,28,512]}

        batchnorm29_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_31_",)(conv29_1_31_) #TODO: fix_gamma=True

        conv27_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv27_1_32_")(relu25_)
        # conv27_1_32_, output shape: {[28,28,8]}

        batchnorm27_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm27_1_32_",)(conv27_1_32_) #TODO: fix_gamma=True

        relu27_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu27_1_32_")(batchnorm27_1_32_)

        conv28_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv28_1_32_")(relu27_1_32_)
        # conv28_1_32_, output shape: {[28,28,8]}

        batchnorm28_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm28_1_32_",)(conv28_1_32_) #TODO: fix_gamma=True

        relu28_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu28_1_32_")(batchnorm28_1_32_)

        conv29_1_32_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv29_1_32_")(relu28_1_32_)
        # conv29_1_32_, output shape: {[28,28,512]}

        batchnorm29_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm29_1_32_",)(conv29_1_32_) #TODO: fix_gamma=True

        add30_1_ = tf.keras.layers.Add()([batchnorm29_1_1_,  batchnorm29_1_2_,  batchnorm29_1_3_,  batchnorm29_1_4_,  batchnorm29_1_5_,  batchnorm29_1_6_,  batchnorm29_1_7_,  batchnorm29_1_8_,  batchnorm29_1_9_,  batchnorm29_1_10_,  batchnorm29_1_11_,  batchnorm29_1_12_,  batchnorm29_1_13_,  batchnorm29_1_14_,  batchnorm29_1_15_,  batchnorm29_1_16_,  batchnorm29_1_17_,  batchnorm29_1_18_,  batchnorm29_1_19_,  batchnorm29_1_20_,  batchnorm29_1_21_,  batchnorm29_1_22_,  batchnorm29_1_23_,  batchnorm29_1_24_,  batchnorm29_1_25_,  batchnorm29_1_26_,  batchnorm29_1_27_,  batchnorm29_1_28_,  batchnorm29_1_29_,  batchnorm29_1_30_,  batchnorm29_1_31_,  batchnorm29_1_32_])
        # add30_1_, output shape: {[28,28,512]}

        add31_ = tf.keras.layers.Add()([add30_1_,  relu25_])
        # add31_, output shape: {[28,28,512]}

        relu31_ = tf.keras.layers.Activation(activation = "relu", name="relu31_")(add31_)

        conv33_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_1_")(relu31_)
        # conv33_1_1_, output shape: {[28,28,8]}

        batchnorm33_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_1_",)(conv33_1_1_) #TODO: fix_gamma=True

        relu33_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_1_")(batchnorm33_1_1_)

        conv34_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_1_")(relu33_1_1_)
        # conv34_1_1_, output shape: {[28,28,8]}

        batchnorm34_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_1_",)(conv34_1_1_) #TODO: fix_gamma=True

        relu34_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_1_")(batchnorm34_1_1_)

        conv35_1_1_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_1_")(relu34_1_1_)
        # conv35_1_1_, output shape: {[28,28,512]}

        batchnorm35_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_1_",)(conv35_1_1_) #TODO: fix_gamma=True

        conv33_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_2_")(relu31_)
        # conv33_1_2_, output shape: {[28,28,8]}

        batchnorm33_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_2_",)(conv33_1_2_) #TODO: fix_gamma=True

        relu33_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_2_")(batchnorm33_1_2_)

        conv34_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_2_")(relu33_1_2_)
        # conv34_1_2_, output shape: {[28,28,8]}

        batchnorm34_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_2_",)(conv34_1_2_) #TODO: fix_gamma=True

        relu34_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_2_")(batchnorm34_1_2_)

        conv35_1_2_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_2_")(relu34_1_2_)
        # conv35_1_2_, output shape: {[28,28,512]}

        batchnorm35_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_2_",)(conv35_1_2_) #TODO: fix_gamma=True

        conv33_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_3_")(relu31_)
        # conv33_1_3_, output shape: {[28,28,8]}

        batchnorm33_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_3_",)(conv33_1_3_) #TODO: fix_gamma=True

        relu33_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_3_")(batchnorm33_1_3_)

        conv34_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_3_")(relu33_1_3_)
        # conv34_1_3_, output shape: {[28,28,8]}

        batchnorm34_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_3_",)(conv34_1_3_) #TODO: fix_gamma=True

        relu34_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_3_")(batchnorm34_1_3_)

        conv35_1_3_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_3_")(relu34_1_3_)
        # conv35_1_3_, output shape: {[28,28,512]}

        batchnorm35_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_3_",)(conv35_1_3_) #TODO: fix_gamma=True

        conv33_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_4_")(relu31_)
        # conv33_1_4_, output shape: {[28,28,8]}

        batchnorm33_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_4_",)(conv33_1_4_) #TODO: fix_gamma=True

        relu33_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_4_")(batchnorm33_1_4_)

        conv34_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_4_")(relu33_1_4_)
        # conv34_1_4_, output shape: {[28,28,8]}

        batchnorm34_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_4_",)(conv34_1_4_) #TODO: fix_gamma=True

        relu34_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_4_")(batchnorm34_1_4_)

        conv35_1_4_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_4_")(relu34_1_4_)
        # conv35_1_4_, output shape: {[28,28,512]}

        batchnorm35_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_4_",)(conv35_1_4_) #TODO: fix_gamma=True

        conv33_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_5_")(relu31_)
        # conv33_1_5_, output shape: {[28,28,8]}

        batchnorm33_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_5_",)(conv33_1_5_) #TODO: fix_gamma=True

        relu33_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_5_")(batchnorm33_1_5_)

        conv34_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_5_")(relu33_1_5_)
        # conv34_1_5_, output shape: {[28,28,8]}

        batchnorm34_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_5_",)(conv34_1_5_) #TODO: fix_gamma=True

        relu34_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_5_")(batchnorm34_1_5_)

        conv35_1_5_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_5_")(relu34_1_5_)
        # conv35_1_5_, output shape: {[28,28,512]}

        batchnorm35_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_5_",)(conv35_1_5_) #TODO: fix_gamma=True

        conv33_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_6_")(relu31_)
        # conv33_1_6_, output shape: {[28,28,8]}

        batchnorm33_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_6_",)(conv33_1_6_) #TODO: fix_gamma=True

        relu33_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_6_")(batchnorm33_1_6_)

        conv34_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_6_")(relu33_1_6_)
        # conv34_1_6_, output shape: {[28,28,8]}

        batchnorm34_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_6_",)(conv34_1_6_) #TODO: fix_gamma=True

        relu34_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_6_")(batchnorm34_1_6_)

        conv35_1_6_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_6_")(relu34_1_6_)
        # conv35_1_6_, output shape: {[28,28,512]}

        batchnorm35_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_6_",)(conv35_1_6_) #TODO: fix_gamma=True

        conv33_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_7_")(relu31_)
        # conv33_1_7_, output shape: {[28,28,8]}

        batchnorm33_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_7_",)(conv33_1_7_) #TODO: fix_gamma=True

        relu33_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_7_")(batchnorm33_1_7_)

        conv34_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_7_")(relu33_1_7_)
        # conv34_1_7_, output shape: {[28,28,8]}

        batchnorm34_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_7_",)(conv34_1_7_) #TODO: fix_gamma=True

        relu34_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_7_")(batchnorm34_1_7_)

        conv35_1_7_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_7_")(relu34_1_7_)
        # conv35_1_7_, output shape: {[28,28,512]}

        batchnorm35_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_7_",)(conv35_1_7_) #TODO: fix_gamma=True

        conv33_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_8_")(relu31_)
        # conv33_1_8_, output shape: {[28,28,8]}

        batchnorm33_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_8_",)(conv33_1_8_) #TODO: fix_gamma=True

        relu33_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_8_")(batchnorm33_1_8_)

        conv34_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_8_")(relu33_1_8_)
        # conv34_1_8_, output shape: {[28,28,8]}

        batchnorm34_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_8_",)(conv34_1_8_) #TODO: fix_gamma=True

        relu34_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_8_")(batchnorm34_1_8_)

        conv35_1_8_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_8_")(relu34_1_8_)
        # conv35_1_8_, output shape: {[28,28,512]}

        batchnorm35_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_8_",)(conv35_1_8_) #TODO: fix_gamma=True

        conv33_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_9_")(relu31_)
        # conv33_1_9_, output shape: {[28,28,8]}

        batchnorm33_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_9_",)(conv33_1_9_) #TODO: fix_gamma=True

        relu33_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_9_")(batchnorm33_1_9_)

        conv34_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_9_")(relu33_1_9_)
        # conv34_1_9_, output shape: {[28,28,8]}

        batchnorm34_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_9_",)(conv34_1_9_) #TODO: fix_gamma=True

        relu34_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_9_")(batchnorm34_1_9_)

        conv35_1_9_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_9_")(relu34_1_9_)
        # conv35_1_9_, output shape: {[28,28,512]}

        batchnorm35_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_9_",)(conv35_1_9_) #TODO: fix_gamma=True

        conv33_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_10_")(relu31_)
        # conv33_1_10_, output shape: {[28,28,8]}

        batchnorm33_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_10_",)(conv33_1_10_) #TODO: fix_gamma=True

        relu33_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_10_")(batchnorm33_1_10_)

        conv34_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_10_")(relu33_1_10_)
        # conv34_1_10_, output shape: {[28,28,8]}

        batchnorm34_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_10_",)(conv34_1_10_) #TODO: fix_gamma=True

        relu34_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_10_")(batchnorm34_1_10_)

        conv35_1_10_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_10_")(relu34_1_10_)
        # conv35_1_10_, output shape: {[28,28,512]}

        batchnorm35_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_10_",)(conv35_1_10_) #TODO: fix_gamma=True

        conv33_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_11_")(relu31_)
        # conv33_1_11_, output shape: {[28,28,8]}

        batchnorm33_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_11_",)(conv33_1_11_) #TODO: fix_gamma=True

        relu33_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_11_")(batchnorm33_1_11_)

        conv34_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_11_")(relu33_1_11_)
        # conv34_1_11_, output shape: {[28,28,8]}

        batchnorm34_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_11_",)(conv34_1_11_) #TODO: fix_gamma=True

        relu34_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_11_")(batchnorm34_1_11_)

        conv35_1_11_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_11_")(relu34_1_11_)
        # conv35_1_11_, output shape: {[28,28,512]}

        batchnorm35_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_11_",)(conv35_1_11_) #TODO: fix_gamma=True

        conv33_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_12_")(relu31_)
        # conv33_1_12_, output shape: {[28,28,8]}

        batchnorm33_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_12_",)(conv33_1_12_) #TODO: fix_gamma=True

        relu33_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_12_")(batchnorm33_1_12_)

        conv34_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_12_")(relu33_1_12_)
        # conv34_1_12_, output shape: {[28,28,8]}

        batchnorm34_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_12_",)(conv34_1_12_) #TODO: fix_gamma=True

        relu34_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_12_")(batchnorm34_1_12_)

        conv35_1_12_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_12_")(relu34_1_12_)
        # conv35_1_12_, output shape: {[28,28,512]}

        batchnorm35_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_12_",)(conv35_1_12_) #TODO: fix_gamma=True

        conv33_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_13_")(relu31_)
        # conv33_1_13_, output shape: {[28,28,8]}

        batchnorm33_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_13_",)(conv33_1_13_) #TODO: fix_gamma=True

        relu33_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_13_")(batchnorm33_1_13_)

        conv34_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_13_")(relu33_1_13_)
        # conv34_1_13_, output shape: {[28,28,8]}

        batchnorm34_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_13_",)(conv34_1_13_) #TODO: fix_gamma=True

        relu34_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_13_")(batchnorm34_1_13_)

        conv35_1_13_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_13_")(relu34_1_13_)
        # conv35_1_13_, output shape: {[28,28,512]}

        batchnorm35_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_13_",)(conv35_1_13_) #TODO: fix_gamma=True

        conv33_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_14_")(relu31_)
        # conv33_1_14_, output shape: {[28,28,8]}

        batchnorm33_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_14_",)(conv33_1_14_) #TODO: fix_gamma=True

        relu33_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_14_")(batchnorm33_1_14_)

        conv34_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_14_")(relu33_1_14_)
        # conv34_1_14_, output shape: {[28,28,8]}

        batchnorm34_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_14_",)(conv34_1_14_) #TODO: fix_gamma=True

        relu34_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_14_")(batchnorm34_1_14_)

        conv35_1_14_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_14_")(relu34_1_14_)
        # conv35_1_14_, output shape: {[28,28,512]}

        batchnorm35_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_14_",)(conv35_1_14_) #TODO: fix_gamma=True

        conv33_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_15_")(relu31_)
        # conv33_1_15_, output shape: {[28,28,8]}

        batchnorm33_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_15_",)(conv33_1_15_) #TODO: fix_gamma=True

        relu33_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_15_")(batchnorm33_1_15_)

        conv34_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_15_")(relu33_1_15_)
        # conv34_1_15_, output shape: {[28,28,8]}

        batchnorm34_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_15_",)(conv34_1_15_) #TODO: fix_gamma=True

        relu34_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_15_")(batchnorm34_1_15_)

        conv35_1_15_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_15_")(relu34_1_15_)
        # conv35_1_15_, output shape: {[28,28,512]}

        batchnorm35_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_15_",)(conv35_1_15_) #TODO: fix_gamma=True

        conv33_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_16_")(relu31_)
        # conv33_1_16_, output shape: {[28,28,8]}

        batchnorm33_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_16_",)(conv33_1_16_) #TODO: fix_gamma=True

        relu33_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_16_")(batchnorm33_1_16_)

        conv34_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_16_")(relu33_1_16_)
        # conv34_1_16_, output shape: {[28,28,8]}

        batchnorm34_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_16_",)(conv34_1_16_) #TODO: fix_gamma=True

        relu34_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_16_")(batchnorm34_1_16_)

        conv35_1_16_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_16_")(relu34_1_16_)
        # conv35_1_16_, output shape: {[28,28,512]}

        batchnorm35_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_16_",)(conv35_1_16_) #TODO: fix_gamma=True

        conv33_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_17_")(relu31_)
        # conv33_1_17_, output shape: {[28,28,8]}

        batchnorm33_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_17_",)(conv33_1_17_) #TODO: fix_gamma=True

        relu33_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_17_")(batchnorm33_1_17_)

        conv34_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_17_")(relu33_1_17_)
        # conv34_1_17_, output shape: {[28,28,8]}

        batchnorm34_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_17_",)(conv34_1_17_) #TODO: fix_gamma=True

        relu34_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_17_")(batchnorm34_1_17_)

        conv35_1_17_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_17_")(relu34_1_17_)
        # conv35_1_17_, output shape: {[28,28,512]}

        batchnorm35_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_17_",)(conv35_1_17_) #TODO: fix_gamma=True

        conv33_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_18_")(relu31_)
        # conv33_1_18_, output shape: {[28,28,8]}

        batchnorm33_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_18_",)(conv33_1_18_) #TODO: fix_gamma=True

        relu33_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_18_")(batchnorm33_1_18_)

        conv34_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_18_")(relu33_1_18_)
        # conv34_1_18_, output shape: {[28,28,8]}

        batchnorm34_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_18_",)(conv34_1_18_) #TODO: fix_gamma=True

        relu34_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_18_")(batchnorm34_1_18_)

        conv35_1_18_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_18_")(relu34_1_18_)
        # conv35_1_18_, output shape: {[28,28,512]}

        batchnorm35_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_18_",)(conv35_1_18_) #TODO: fix_gamma=True

        conv33_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_19_")(relu31_)
        # conv33_1_19_, output shape: {[28,28,8]}

        batchnorm33_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_19_",)(conv33_1_19_) #TODO: fix_gamma=True

        relu33_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_19_")(batchnorm33_1_19_)

        conv34_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_19_")(relu33_1_19_)
        # conv34_1_19_, output shape: {[28,28,8]}

        batchnorm34_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_19_",)(conv34_1_19_) #TODO: fix_gamma=True

        relu34_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_19_")(batchnorm34_1_19_)

        conv35_1_19_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_19_")(relu34_1_19_)
        # conv35_1_19_, output shape: {[28,28,512]}

        batchnorm35_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_19_",)(conv35_1_19_) #TODO: fix_gamma=True

        conv33_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_20_")(relu31_)
        # conv33_1_20_, output shape: {[28,28,8]}

        batchnorm33_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_20_",)(conv33_1_20_) #TODO: fix_gamma=True

        relu33_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_20_")(batchnorm33_1_20_)

        conv34_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_20_")(relu33_1_20_)
        # conv34_1_20_, output shape: {[28,28,8]}

        batchnorm34_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_20_",)(conv34_1_20_) #TODO: fix_gamma=True

        relu34_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_20_")(batchnorm34_1_20_)

        conv35_1_20_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_20_")(relu34_1_20_)
        # conv35_1_20_, output shape: {[28,28,512]}

        batchnorm35_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_20_",)(conv35_1_20_) #TODO: fix_gamma=True

        conv33_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_21_")(relu31_)
        # conv33_1_21_, output shape: {[28,28,8]}

        batchnorm33_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_21_",)(conv33_1_21_) #TODO: fix_gamma=True

        relu33_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_21_")(batchnorm33_1_21_)

        conv34_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_21_")(relu33_1_21_)
        # conv34_1_21_, output shape: {[28,28,8]}

        batchnorm34_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_21_",)(conv34_1_21_) #TODO: fix_gamma=True

        relu34_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_21_")(batchnorm34_1_21_)

        conv35_1_21_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_21_")(relu34_1_21_)
        # conv35_1_21_, output shape: {[28,28,512]}

        batchnorm35_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_21_",)(conv35_1_21_) #TODO: fix_gamma=True

        conv33_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_22_")(relu31_)
        # conv33_1_22_, output shape: {[28,28,8]}

        batchnorm33_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_22_",)(conv33_1_22_) #TODO: fix_gamma=True

        relu33_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_22_")(batchnorm33_1_22_)

        conv34_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_22_")(relu33_1_22_)
        # conv34_1_22_, output shape: {[28,28,8]}

        batchnorm34_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_22_",)(conv34_1_22_) #TODO: fix_gamma=True

        relu34_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_22_")(batchnorm34_1_22_)

        conv35_1_22_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_22_")(relu34_1_22_)
        # conv35_1_22_, output shape: {[28,28,512]}

        batchnorm35_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_22_",)(conv35_1_22_) #TODO: fix_gamma=True

        conv33_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_23_")(relu31_)
        # conv33_1_23_, output shape: {[28,28,8]}

        batchnorm33_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_23_",)(conv33_1_23_) #TODO: fix_gamma=True

        relu33_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_23_")(batchnorm33_1_23_)

        conv34_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_23_")(relu33_1_23_)
        # conv34_1_23_, output shape: {[28,28,8]}

        batchnorm34_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_23_",)(conv34_1_23_) #TODO: fix_gamma=True

        relu34_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_23_")(batchnorm34_1_23_)

        conv35_1_23_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_23_")(relu34_1_23_)
        # conv35_1_23_, output shape: {[28,28,512]}

        batchnorm35_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_23_",)(conv35_1_23_) #TODO: fix_gamma=True

        conv33_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_24_")(relu31_)
        # conv33_1_24_, output shape: {[28,28,8]}

        batchnorm33_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_24_",)(conv33_1_24_) #TODO: fix_gamma=True

        relu33_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_24_")(batchnorm33_1_24_)

        conv34_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_24_")(relu33_1_24_)
        # conv34_1_24_, output shape: {[28,28,8]}

        batchnorm34_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_24_",)(conv34_1_24_) #TODO: fix_gamma=True

        relu34_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_24_")(batchnorm34_1_24_)

        conv35_1_24_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_24_")(relu34_1_24_)
        # conv35_1_24_, output shape: {[28,28,512]}

        batchnorm35_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_24_",)(conv35_1_24_) #TODO: fix_gamma=True

        conv33_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_25_")(relu31_)
        # conv33_1_25_, output shape: {[28,28,8]}

        batchnorm33_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_25_",)(conv33_1_25_) #TODO: fix_gamma=True

        relu33_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_25_")(batchnorm33_1_25_)

        conv34_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_25_")(relu33_1_25_)
        # conv34_1_25_, output shape: {[28,28,8]}

        batchnorm34_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_25_",)(conv34_1_25_) #TODO: fix_gamma=True

        relu34_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_25_")(batchnorm34_1_25_)

        conv35_1_25_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_25_")(relu34_1_25_)
        # conv35_1_25_, output shape: {[28,28,512]}

        batchnorm35_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_25_",)(conv35_1_25_) #TODO: fix_gamma=True

        conv33_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_26_")(relu31_)
        # conv33_1_26_, output shape: {[28,28,8]}

        batchnorm33_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_26_",)(conv33_1_26_) #TODO: fix_gamma=True

        relu33_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_26_")(batchnorm33_1_26_)

        conv34_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_26_")(relu33_1_26_)
        # conv34_1_26_, output shape: {[28,28,8]}

        batchnorm34_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_26_",)(conv34_1_26_) #TODO: fix_gamma=True

        relu34_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_26_")(batchnorm34_1_26_)

        conv35_1_26_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_26_")(relu34_1_26_)
        # conv35_1_26_, output shape: {[28,28,512]}

        batchnorm35_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_26_",)(conv35_1_26_) #TODO: fix_gamma=True

        conv33_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_27_")(relu31_)
        # conv33_1_27_, output shape: {[28,28,8]}

        batchnorm33_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_27_",)(conv33_1_27_) #TODO: fix_gamma=True

        relu33_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_27_")(batchnorm33_1_27_)

        conv34_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_27_")(relu33_1_27_)
        # conv34_1_27_, output shape: {[28,28,8]}

        batchnorm34_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_27_",)(conv34_1_27_) #TODO: fix_gamma=True

        relu34_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_27_")(batchnorm34_1_27_)

        conv35_1_27_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_27_")(relu34_1_27_)
        # conv35_1_27_, output shape: {[28,28,512]}

        batchnorm35_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_27_",)(conv35_1_27_) #TODO: fix_gamma=True

        conv33_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_28_")(relu31_)
        # conv33_1_28_, output shape: {[28,28,8]}

        batchnorm33_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_28_",)(conv33_1_28_) #TODO: fix_gamma=True

        relu33_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_28_")(batchnorm33_1_28_)

        conv34_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_28_")(relu33_1_28_)
        # conv34_1_28_, output shape: {[28,28,8]}

        batchnorm34_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_28_",)(conv34_1_28_) #TODO: fix_gamma=True

        relu34_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_28_")(batchnorm34_1_28_)

        conv35_1_28_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_28_")(relu34_1_28_)
        # conv35_1_28_, output shape: {[28,28,512]}

        batchnorm35_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_28_",)(conv35_1_28_) #TODO: fix_gamma=True

        conv33_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_29_")(relu31_)
        # conv33_1_29_, output shape: {[28,28,8]}

        batchnorm33_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_29_",)(conv33_1_29_) #TODO: fix_gamma=True

        relu33_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_29_")(batchnorm33_1_29_)

        conv34_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_29_")(relu33_1_29_)
        # conv34_1_29_, output shape: {[28,28,8]}

        batchnorm34_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_29_",)(conv34_1_29_) #TODO: fix_gamma=True

        relu34_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_29_")(batchnorm34_1_29_)

        conv35_1_29_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_29_")(relu34_1_29_)
        # conv35_1_29_, output shape: {[28,28,512]}

        batchnorm35_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_29_",)(conv35_1_29_) #TODO: fix_gamma=True

        conv33_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_30_")(relu31_)
        # conv33_1_30_, output shape: {[28,28,8]}

        batchnorm33_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_30_",)(conv33_1_30_) #TODO: fix_gamma=True

        relu33_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_30_")(batchnorm33_1_30_)

        conv34_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_30_")(relu33_1_30_)
        # conv34_1_30_, output shape: {[28,28,8]}

        batchnorm34_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_30_",)(conv34_1_30_) #TODO: fix_gamma=True

        relu34_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_30_")(batchnorm34_1_30_)

        conv35_1_30_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_30_")(relu34_1_30_)
        # conv35_1_30_, output shape: {[28,28,512]}

        batchnorm35_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_30_",)(conv35_1_30_) #TODO: fix_gamma=True

        conv33_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_31_")(relu31_)
        # conv33_1_31_, output shape: {[28,28,8]}

        batchnorm33_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_31_",)(conv33_1_31_) #TODO: fix_gamma=True

        relu33_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_31_")(batchnorm33_1_31_)

        conv34_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_31_")(relu33_1_31_)
        # conv34_1_31_, output shape: {[28,28,8]}

        batchnorm34_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_31_",)(conv34_1_31_) #TODO: fix_gamma=True

        relu34_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_31_")(batchnorm34_1_31_)

        conv35_1_31_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_31_")(relu34_1_31_)
        # conv35_1_31_, output shape: {[28,28,512]}

        batchnorm35_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_31_",)(conv35_1_31_) #TODO: fix_gamma=True

        conv33_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv33_1_32_")(relu31_)
        # conv33_1_32_, output shape: {[28,28,8]}

        batchnorm33_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm33_1_32_",)(conv33_1_32_) #TODO: fix_gamma=True

        relu33_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu33_1_32_")(batchnorm33_1_32_)

        conv34_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv34_1_32_")(relu33_1_32_)
        # conv34_1_32_, output shape: {[28,28,8]}

        batchnorm34_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm34_1_32_",)(conv34_1_32_) #TODO: fix_gamma=True

        relu34_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu34_1_32_")(batchnorm34_1_32_)

        conv35_1_32_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv35_1_32_")(relu34_1_32_)
        # conv35_1_32_, output shape: {[28,28,512]}

        batchnorm35_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm35_1_32_",)(conv35_1_32_) #TODO: fix_gamma=True

        add36_1_ = tf.keras.layers.Add()([batchnorm35_1_1_,  batchnorm35_1_2_,  batchnorm35_1_3_,  batchnorm35_1_4_,  batchnorm35_1_5_,  batchnorm35_1_6_,  batchnorm35_1_7_,  batchnorm35_1_8_,  batchnorm35_1_9_,  batchnorm35_1_10_,  batchnorm35_1_11_,  batchnorm35_1_12_,  batchnorm35_1_13_,  batchnorm35_1_14_,  batchnorm35_1_15_,  batchnorm35_1_16_,  batchnorm35_1_17_,  batchnorm35_1_18_,  batchnorm35_1_19_,  batchnorm35_1_20_,  batchnorm35_1_21_,  batchnorm35_1_22_,  batchnorm35_1_23_,  batchnorm35_1_24_,  batchnorm35_1_25_,  batchnorm35_1_26_,  batchnorm35_1_27_,  batchnorm35_1_28_,  batchnorm35_1_29_,  batchnorm35_1_30_,  batchnorm35_1_31_,  batchnorm35_1_32_])
        # add36_1_, output shape: {[28,28,512]}

        add37_ = tf.keras.layers.Add()([add36_1_,  relu31_])
        # add37_, output shape: {[28,28,512]}

        relu37_ = tf.keras.layers.Activation(activation = "relu", name="relu37_")(add37_)

        conv39_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_1_")(relu37_)
        # conv39_1_1_, output shape: {[28,28,8]}

        batchnorm39_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_1_",)(conv39_1_1_) #TODO: fix_gamma=True

        relu39_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_1_")(batchnorm39_1_1_)

        conv40_1_1_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_1_")(relu39_1_1_)
        # conv40_1_1_, output shape: {[28,28,8]}

        batchnorm40_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_1_",)(conv40_1_1_) #TODO: fix_gamma=True

        relu40_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_1_")(batchnorm40_1_1_)

        conv41_1_1_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_1_")(relu40_1_1_)
        # conv41_1_1_, output shape: {[28,28,512]}

        batchnorm41_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_1_",)(conv41_1_1_) #TODO: fix_gamma=True

        conv39_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_2_")(relu37_)
        # conv39_1_2_, output shape: {[28,28,8]}

        batchnorm39_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_2_",)(conv39_1_2_) #TODO: fix_gamma=True

        relu39_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_2_")(batchnorm39_1_2_)

        conv40_1_2_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_2_")(relu39_1_2_)
        # conv40_1_2_, output shape: {[28,28,8]}

        batchnorm40_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_2_",)(conv40_1_2_) #TODO: fix_gamma=True

        relu40_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_2_")(batchnorm40_1_2_)

        conv41_1_2_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_2_")(relu40_1_2_)
        # conv41_1_2_, output shape: {[28,28,512]}

        batchnorm41_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_2_",)(conv41_1_2_) #TODO: fix_gamma=True

        conv39_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_3_")(relu37_)
        # conv39_1_3_, output shape: {[28,28,8]}

        batchnorm39_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_3_",)(conv39_1_3_) #TODO: fix_gamma=True

        relu39_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_3_")(batchnorm39_1_3_)

        conv40_1_3_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_3_")(relu39_1_3_)
        # conv40_1_3_, output shape: {[28,28,8]}

        batchnorm40_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_3_",)(conv40_1_3_) #TODO: fix_gamma=True

        relu40_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_3_")(batchnorm40_1_3_)

        conv41_1_3_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_3_")(relu40_1_3_)
        # conv41_1_3_, output shape: {[28,28,512]}

        batchnorm41_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_3_",)(conv41_1_3_) #TODO: fix_gamma=True

        conv39_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_4_")(relu37_)
        # conv39_1_4_, output shape: {[28,28,8]}

        batchnorm39_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_4_",)(conv39_1_4_) #TODO: fix_gamma=True

        relu39_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_4_")(batchnorm39_1_4_)

        conv40_1_4_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_4_")(relu39_1_4_)
        # conv40_1_4_, output shape: {[28,28,8]}

        batchnorm40_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_4_",)(conv40_1_4_) #TODO: fix_gamma=True

        relu40_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_4_")(batchnorm40_1_4_)

        conv41_1_4_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_4_")(relu40_1_4_)
        # conv41_1_4_, output shape: {[28,28,512]}

        batchnorm41_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_4_",)(conv41_1_4_) #TODO: fix_gamma=True

        conv39_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_5_")(relu37_)
        # conv39_1_5_, output shape: {[28,28,8]}

        batchnorm39_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_5_",)(conv39_1_5_) #TODO: fix_gamma=True

        relu39_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_5_")(batchnorm39_1_5_)

        conv40_1_5_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_5_")(relu39_1_5_)
        # conv40_1_5_, output shape: {[28,28,8]}

        batchnorm40_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_5_",)(conv40_1_5_) #TODO: fix_gamma=True

        relu40_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_5_")(batchnorm40_1_5_)

        conv41_1_5_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_5_")(relu40_1_5_)
        # conv41_1_5_, output shape: {[28,28,512]}

        batchnorm41_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_5_",)(conv41_1_5_) #TODO: fix_gamma=True

        conv39_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_6_")(relu37_)
        # conv39_1_6_, output shape: {[28,28,8]}

        batchnorm39_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_6_",)(conv39_1_6_) #TODO: fix_gamma=True

        relu39_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_6_")(batchnorm39_1_6_)

        conv40_1_6_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_6_")(relu39_1_6_)
        # conv40_1_6_, output shape: {[28,28,8]}

        batchnorm40_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_6_",)(conv40_1_6_) #TODO: fix_gamma=True

        relu40_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_6_")(batchnorm40_1_6_)

        conv41_1_6_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_6_")(relu40_1_6_)
        # conv41_1_6_, output shape: {[28,28,512]}

        batchnorm41_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_6_",)(conv41_1_6_) #TODO: fix_gamma=True

        conv39_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_7_")(relu37_)
        # conv39_1_7_, output shape: {[28,28,8]}

        batchnorm39_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_7_",)(conv39_1_7_) #TODO: fix_gamma=True

        relu39_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_7_")(batchnorm39_1_7_)

        conv40_1_7_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_7_")(relu39_1_7_)
        # conv40_1_7_, output shape: {[28,28,8]}

        batchnorm40_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_7_",)(conv40_1_7_) #TODO: fix_gamma=True

        relu40_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_7_")(batchnorm40_1_7_)

        conv41_1_7_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_7_")(relu40_1_7_)
        # conv41_1_7_, output shape: {[28,28,512]}

        batchnorm41_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_7_",)(conv41_1_7_) #TODO: fix_gamma=True

        conv39_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_8_")(relu37_)
        # conv39_1_8_, output shape: {[28,28,8]}

        batchnorm39_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_8_",)(conv39_1_8_) #TODO: fix_gamma=True

        relu39_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_8_")(batchnorm39_1_8_)

        conv40_1_8_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_8_")(relu39_1_8_)
        # conv40_1_8_, output shape: {[28,28,8]}

        batchnorm40_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_8_",)(conv40_1_8_) #TODO: fix_gamma=True

        relu40_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_8_")(batchnorm40_1_8_)

        conv41_1_8_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_8_")(relu40_1_8_)
        # conv41_1_8_, output shape: {[28,28,512]}

        batchnorm41_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_8_",)(conv41_1_8_) #TODO: fix_gamma=True

        conv39_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_9_")(relu37_)
        # conv39_1_9_, output shape: {[28,28,8]}

        batchnorm39_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_9_",)(conv39_1_9_) #TODO: fix_gamma=True

        relu39_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_9_")(batchnorm39_1_9_)

        conv40_1_9_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_9_")(relu39_1_9_)
        # conv40_1_9_, output shape: {[28,28,8]}

        batchnorm40_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_9_",)(conv40_1_9_) #TODO: fix_gamma=True

        relu40_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_9_")(batchnorm40_1_9_)

        conv41_1_9_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_9_")(relu40_1_9_)
        # conv41_1_9_, output shape: {[28,28,512]}

        batchnorm41_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_9_",)(conv41_1_9_) #TODO: fix_gamma=True

        conv39_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_10_")(relu37_)
        # conv39_1_10_, output shape: {[28,28,8]}

        batchnorm39_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_10_",)(conv39_1_10_) #TODO: fix_gamma=True

        relu39_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_10_")(batchnorm39_1_10_)

        conv40_1_10_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_10_")(relu39_1_10_)
        # conv40_1_10_, output shape: {[28,28,8]}

        batchnorm40_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_10_",)(conv40_1_10_) #TODO: fix_gamma=True

        relu40_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_10_")(batchnorm40_1_10_)

        conv41_1_10_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_10_")(relu40_1_10_)
        # conv41_1_10_, output shape: {[28,28,512]}

        batchnorm41_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_10_",)(conv41_1_10_) #TODO: fix_gamma=True

        conv39_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_11_")(relu37_)
        # conv39_1_11_, output shape: {[28,28,8]}

        batchnorm39_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_11_",)(conv39_1_11_) #TODO: fix_gamma=True

        relu39_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_11_")(batchnorm39_1_11_)

        conv40_1_11_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_11_")(relu39_1_11_)
        # conv40_1_11_, output shape: {[28,28,8]}

        batchnorm40_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_11_",)(conv40_1_11_) #TODO: fix_gamma=True

        relu40_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_11_")(batchnorm40_1_11_)

        conv41_1_11_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_11_")(relu40_1_11_)
        # conv41_1_11_, output shape: {[28,28,512]}

        batchnorm41_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_11_",)(conv41_1_11_) #TODO: fix_gamma=True

        conv39_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_12_")(relu37_)
        # conv39_1_12_, output shape: {[28,28,8]}

        batchnorm39_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_12_",)(conv39_1_12_) #TODO: fix_gamma=True

        relu39_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_12_")(batchnorm39_1_12_)

        conv40_1_12_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_12_")(relu39_1_12_)
        # conv40_1_12_, output shape: {[28,28,8]}

        batchnorm40_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_12_",)(conv40_1_12_) #TODO: fix_gamma=True

        relu40_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_12_")(batchnorm40_1_12_)

        conv41_1_12_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_12_")(relu40_1_12_)
        # conv41_1_12_, output shape: {[28,28,512]}

        batchnorm41_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_12_",)(conv41_1_12_) #TODO: fix_gamma=True

        conv39_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_13_")(relu37_)
        # conv39_1_13_, output shape: {[28,28,8]}

        batchnorm39_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_13_",)(conv39_1_13_) #TODO: fix_gamma=True

        relu39_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_13_")(batchnorm39_1_13_)

        conv40_1_13_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_13_")(relu39_1_13_)
        # conv40_1_13_, output shape: {[28,28,8]}

        batchnorm40_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_13_",)(conv40_1_13_) #TODO: fix_gamma=True

        relu40_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_13_")(batchnorm40_1_13_)

        conv41_1_13_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_13_")(relu40_1_13_)
        # conv41_1_13_, output shape: {[28,28,512]}

        batchnorm41_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_13_",)(conv41_1_13_) #TODO: fix_gamma=True

        conv39_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_14_")(relu37_)
        # conv39_1_14_, output shape: {[28,28,8]}

        batchnorm39_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_14_",)(conv39_1_14_) #TODO: fix_gamma=True

        relu39_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_14_")(batchnorm39_1_14_)

        conv40_1_14_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_14_")(relu39_1_14_)
        # conv40_1_14_, output shape: {[28,28,8]}

        batchnorm40_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_14_",)(conv40_1_14_) #TODO: fix_gamma=True

        relu40_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_14_")(batchnorm40_1_14_)

        conv41_1_14_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_14_")(relu40_1_14_)
        # conv41_1_14_, output shape: {[28,28,512]}

        batchnorm41_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_14_",)(conv41_1_14_) #TODO: fix_gamma=True

        conv39_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_15_")(relu37_)
        # conv39_1_15_, output shape: {[28,28,8]}

        batchnorm39_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_15_",)(conv39_1_15_) #TODO: fix_gamma=True

        relu39_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_15_")(batchnorm39_1_15_)

        conv40_1_15_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_15_")(relu39_1_15_)
        # conv40_1_15_, output shape: {[28,28,8]}

        batchnorm40_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_15_",)(conv40_1_15_) #TODO: fix_gamma=True

        relu40_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_15_")(batchnorm40_1_15_)

        conv41_1_15_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_15_")(relu40_1_15_)
        # conv41_1_15_, output shape: {[28,28,512]}

        batchnorm41_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_15_",)(conv41_1_15_) #TODO: fix_gamma=True

        conv39_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_16_")(relu37_)
        # conv39_1_16_, output shape: {[28,28,8]}

        batchnorm39_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_16_",)(conv39_1_16_) #TODO: fix_gamma=True

        relu39_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_16_")(batchnorm39_1_16_)

        conv40_1_16_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_16_")(relu39_1_16_)
        # conv40_1_16_, output shape: {[28,28,8]}

        batchnorm40_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_16_",)(conv40_1_16_) #TODO: fix_gamma=True

        relu40_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_16_")(batchnorm40_1_16_)

        conv41_1_16_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_16_")(relu40_1_16_)
        # conv41_1_16_, output shape: {[28,28,512]}

        batchnorm41_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_16_",)(conv41_1_16_) #TODO: fix_gamma=True

        conv39_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_17_")(relu37_)
        # conv39_1_17_, output shape: {[28,28,8]}

        batchnorm39_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_17_",)(conv39_1_17_) #TODO: fix_gamma=True

        relu39_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_17_")(batchnorm39_1_17_)

        conv40_1_17_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_17_")(relu39_1_17_)
        # conv40_1_17_, output shape: {[28,28,8]}

        batchnorm40_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_17_",)(conv40_1_17_) #TODO: fix_gamma=True

        relu40_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_17_")(batchnorm40_1_17_)

        conv41_1_17_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_17_")(relu40_1_17_)
        # conv41_1_17_, output shape: {[28,28,512]}

        batchnorm41_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_17_",)(conv41_1_17_) #TODO: fix_gamma=True

        conv39_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_18_")(relu37_)
        # conv39_1_18_, output shape: {[28,28,8]}

        batchnorm39_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_18_",)(conv39_1_18_) #TODO: fix_gamma=True

        relu39_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_18_")(batchnorm39_1_18_)

        conv40_1_18_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_18_")(relu39_1_18_)
        # conv40_1_18_, output shape: {[28,28,8]}

        batchnorm40_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_18_",)(conv40_1_18_) #TODO: fix_gamma=True

        relu40_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_18_")(batchnorm40_1_18_)

        conv41_1_18_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_18_")(relu40_1_18_)
        # conv41_1_18_, output shape: {[28,28,512]}

        batchnorm41_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_18_",)(conv41_1_18_) #TODO: fix_gamma=True

        conv39_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_19_")(relu37_)
        # conv39_1_19_, output shape: {[28,28,8]}

        batchnorm39_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_19_",)(conv39_1_19_) #TODO: fix_gamma=True

        relu39_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_19_")(batchnorm39_1_19_)

        conv40_1_19_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_19_")(relu39_1_19_)
        # conv40_1_19_, output shape: {[28,28,8]}

        batchnorm40_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_19_",)(conv40_1_19_) #TODO: fix_gamma=True

        relu40_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_19_")(batchnorm40_1_19_)

        conv41_1_19_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_19_")(relu40_1_19_)
        # conv41_1_19_, output shape: {[28,28,512]}

        batchnorm41_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_19_",)(conv41_1_19_) #TODO: fix_gamma=True

        conv39_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_20_")(relu37_)
        # conv39_1_20_, output shape: {[28,28,8]}

        batchnorm39_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_20_",)(conv39_1_20_) #TODO: fix_gamma=True

        relu39_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_20_")(batchnorm39_1_20_)

        conv40_1_20_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_20_")(relu39_1_20_)
        # conv40_1_20_, output shape: {[28,28,8]}

        batchnorm40_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_20_",)(conv40_1_20_) #TODO: fix_gamma=True

        relu40_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_20_")(batchnorm40_1_20_)

        conv41_1_20_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_20_")(relu40_1_20_)
        # conv41_1_20_, output shape: {[28,28,512]}

        batchnorm41_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_20_",)(conv41_1_20_) #TODO: fix_gamma=True

        conv39_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_21_")(relu37_)
        # conv39_1_21_, output shape: {[28,28,8]}

        batchnorm39_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_21_",)(conv39_1_21_) #TODO: fix_gamma=True

        relu39_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_21_")(batchnorm39_1_21_)

        conv40_1_21_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_21_")(relu39_1_21_)
        # conv40_1_21_, output shape: {[28,28,8]}

        batchnorm40_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_21_",)(conv40_1_21_) #TODO: fix_gamma=True

        relu40_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_21_")(batchnorm40_1_21_)

        conv41_1_21_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_21_")(relu40_1_21_)
        # conv41_1_21_, output shape: {[28,28,512]}

        batchnorm41_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_21_",)(conv41_1_21_) #TODO: fix_gamma=True

        conv39_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_22_")(relu37_)
        # conv39_1_22_, output shape: {[28,28,8]}

        batchnorm39_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_22_",)(conv39_1_22_) #TODO: fix_gamma=True

        relu39_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_22_")(batchnorm39_1_22_)

        conv40_1_22_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_22_")(relu39_1_22_)
        # conv40_1_22_, output shape: {[28,28,8]}

        batchnorm40_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_22_",)(conv40_1_22_) #TODO: fix_gamma=True

        relu40_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_22_")(batchnorm40_1_22_)

        conv41_1_22_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_22_")(relu40_1_22_)
        # conv41_1_22_, output shape: {[28,28,512]}

        batchnorm41_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_22_",)(conv41_1_22_) #TODO: fix_gamma=True

        conv39_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_23_")(relu37_)
        # conv39_1_23_, output shape: {[28,28,8]}

        batchnorm39_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_23_",)(conv39_1_23_) #TODO: fix_gamma=True

        relu39_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_23_")(batchnorm39_1_23_)

        conv40_1_23_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_23_")(relu39_1_23_)
        # conv40_1_23_, output shape: {[28,28,8]}

        batchnorm40_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_23_",)(conv40_1_23_) #TODO: fix_gamma=True

        relu40_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_23_")(batchnorm40_1_23_)

        conv41_1_23_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_23_")(relu40_1_23_)
        # conv41_1_23_, output shape: {[28,28,512]}

        batchnorm41_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_23_",)(conv41_1_23_) #TODO: fix_gamma=True

        conv39_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_24_")(relu37_)
        # conv39_1_24_, output shape: {[28,28,8]}

        batchnorm39_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_24_",)(conv39_1_24_) #TODO: fix_gamma=True

        relu39_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_24_")(batchnorm39_1_24_)

        conv40_1_24_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_24_")(relu39_1_24_)
        # conv40_1_24_, output shape: {[28,28,8]}

        batchnorm40_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_24_",)(conv40_1_24_) #TODO: fix_gamma=True

        relu40_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_24_")(batchnorm40_1_24_)

        conv41_1_24_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_24_")(relu40_1_24_)
        # conv41_1_24_, output shape: {[28,28,512]}

        batchnorm41_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_24_",)(conv41_1_24_) #TODO: fix_gamma=True

        conv39_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_25_")(relu37_)
        # conv39_1_25_, output shape: {[28,28,8]}

        batchnorm39_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_25_",)(conv39_1_25_) #TODO: fix_gamma=True

        relu39_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_25_")(batchnorm39_1_25_)

        conv40_1_25_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_25_")(relu39_1_25_)
        # conv40_1_25_, output shape: {[28,28,8]}

        batchnorm40_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_25_",)(conv40_1_25_) #TODO: fix_gamma=True

        relu40_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_25_")(batchnorm40_1_25_)

        conv41_1_25_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_25_")(relu40_1_25_)
        # conv41_1_25_, output shape: {[28,28,512]}

        batchnorm41_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_25_",)(conv41_1_25_) #TODO: fix_gamma=True

        conv39_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_26_")(relu37_)
        # conv39_1_26_, output shape: {[28,28,8]}

        batchnorm39_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_26_",)(conv39_1_26_) #TODO: fix_gamma=True

        relu39_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_26_")(batchnorm39_1_26_)

        conv40_1_26_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_26_")(relu39_1_26_)
        # conv40_1_26_, output shape: {[28,28,8]}

        batchnorm40_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_26_",)(conv40_1_26_) #TODO: fix_gamma=True

        relu40_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_26_")(batchnorm40_1_26_)

        conv41_1_26_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_26_")(relu40_1_26_)
        # conv41_1_26_, output shape: {[28,28,512]}

        batchnorm41_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_26_",)(conv41_1_26_) #TODO: fix_gamma=True

        conv39_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_27_")(relu37_)
        # conv39_1_27_, output shape: {[28,28,8]}

        batchnorm39_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_27_",)(conv39_1_27_) #TODO: fix_gamma=True

        relu39_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_27_")(batchnorm39_1_27_)

        conv40_1_27_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_27_")(relu39_1_27_)
        # conv40_1_27_, output shape: {[28,28,8]}

        batchnorm40_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_27_",)(conv40_1_27_) #TODO: fix_gamma=True

        relu40_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_27_")(batchnorm40_1_27_)

        conv41_1_27_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_27_")(relu40_1_27_)
        # conv41_1_27_, output shape: {[28,28,512]}

        batchnorm41_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_27_",)(conv41_1_27_) #TODO: fix_gamma=True

        conv39_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_28_")(relu37_)
        # conv39_1_28_, output shape: {[28,28,8]}

        batchnorm39_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_28_",)(conv39_1_28_) #TODO: fix_gamma=True

        relu39_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_28_")(batchnorm39_1_28_)

        conv40_1_28_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_28_")(relu39_1_28_)
        # conv40_1_28_, output shape: {[28,28,8]}

        batchnorm40_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_28_",)(conv40_1_28_) #TODO: fix_gamma=True

        relu40_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_28_")(batchnorm40_1_28_)

        conv41_1_28_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_28_")(relu40_1_28_)
        # conv41_1_28_, output shape: {[28,28,512]}

        batchnorm41_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_28_",)(conv41_1_28_) #TODO: fix_gamma=True

        conv39_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_29_")(relu37_)
        # conv39_1_29_, output shape: {[28,28,8]}

        batchnorm39_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_29_",)(conv39_1_29_) #TODO: fix_gamma=True

        relu39_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_29_")(batchnorm39_1_29_)

        conv40_1_29_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_29_")(relu39_1_29_)
        # conv40_1_29_, output shape: {[28,28,8]}

        batchnorm40_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_29_",)(conv40_1_29_) #TODO: fix_gamma=True

        relu40_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_29_")(batchnorm40_1_29_)

        conv41_1_29_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_29_")(relu40_1_29_)
        # conv41_1_29_, output shape: {[28,28,512]}

        batchnorm41_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_29_",)(conv41_1_29_) #TODO: fix_gamma=True

        conv39_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_30_")(relu37_)
        # conv39_1_30_, output shape: {[28,28,8]}

        batchnorm39_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_30_",)(conv39_1_30_) #TODO: fix_gamma=True

        relu39_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_30_")(batchnorm39_1_30_)

        conv40_1_30_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_30_")(relu39_1_30_)
        # conv40_1_30_, output shape: {[28,28,8]}

        batchnorm40_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_30_",)(conv40_1_30_) #TODO: fix_gamma=True

        relu40_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_30_")(batchnorm40_1_30_)

        conv41_1_30_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_30_")(relu40_1_30_)
        # conv41_1_30_, output shape: {[28,28,512]}

        batchnorm41_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_30_",)(conv41_1_30_) #TODO: fix_gamma=True

        conv39_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_31_")(relu37_)
        # conv39_1_31_, output shape: {[28,28,8]}

        batchnorm39_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_31_",)(conv39_1_31_) #TODO: fix_gamma=True

        relu39_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_31_")(batchnorm39_1_31_)

        conv40_1_31_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_31_")(relu39_1_31_)
        # conv40_1_31_, output shape: {[28,28,8]}

        batchnorm40_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_31_",)(conv40_1_31_) #TODO: fix_gamma=True

        relu40_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_31_")(batchnorm40_1_31_)

        conv41_1_31_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_31_")(relu40_1_31_)
        # conv41_1_31_, output shape: {[28,28,512]}

        batchnorm41_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_31_",)(conv41_1_31_) #TODO: fix_gamma=True

        conv39_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv39_1_32_")(relu37_)
        # conv39_1_32_, output shape: {[28,28,8]}

        batchnorm39_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm39_1_32_",)(conv39_1_32_) #TODO: fix_gamma=True

        relu39_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu39_1_32_")(batchnorm39_1_32_)

        conv40_1_32_ = tf.keras.layers.Conv2D(8, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv40_1_32_")(relu39_1_32_)
        # conv40_1_32_, output shape: {[28,28,8]}

        batchnorm40_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm40_1_32_",)(conv40_1_32_) #TODO: fix_gamma=True

        relu40_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu40_1_32_")(batchnorm40_1_32_)

        conv41_1_32_ = tf.keras.layers.Conv2D(512, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv41_1_32_")(relu40_1_32_)
        # conv41_1_32_, output shape: {[28,28,512]}

        batchnorm41_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm41_1_32_",)(conv41_1_32_) #TODO: fix_gamma=True

        add42_1_ = tf.keras.layers.Add()([batchnorm41_1_1_,  batchnorm41_1_2_,  batchnorm41_1_3_,  batchnorm41_1_4_,  batchnorm41_1_5_,  batchnorm41_1_6_,  batchnorm41_1_7_,  batchnorm41_1_8_,  batchnorm41_1_9_,  batchnorm41_1_10_,  batchnorm41_1_11_,  batchnorm41_1_12_,  batchnorm41_1_13_,  batchnorm41_1_14_,  batchnorm41_1_15_,  batchnorm41_1_16_,  batchnorm41_1_17_,  batchnorm41_1_18_,  batchnorm41_1_19_,  batchnorm41_1_20_,  batchnorm41_1_21_,  batchnorm41_1_22_,  batchnorm41_1_23_,  batchnorm41_1_24_,  batchnorm41_1_25_,  batchnorm41_1_26_,  batchnorm41_1_27_,  batchnorm41_1_28_,  batchnorm41_1_29_,  batchnorm41_1_30_,  batchnorm41_1_31_,  batchnorm41_1_32_])
        # add42_1_, output shape: {[28,28,512]}

        add43_ = tf.keras.layers.Add()([add42_1_,  relu37_])
        # add43_, output shape: {[28,28,512]}

        relu43_ = tf.keras.layers.Activation(activation = "relu", name="relu43_")(add43_)

        conv45_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_1_")(relu43_)
        # conv45_1_1_, output shape: {[28,28,16]}

        batchnorm45_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_1_",)(conv45_1_1_) #TODO: fix_gamma=True

        relu45_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_1_")(batchnorm45_1_1_)

        conv46_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_1_")(relu45_1_1_)
        # conv46_1_1_, output shape: {[14,14,16]}

        batchnorm46_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_1_",)(conv46_1_1_) #TODO: fix_gamma=True

        relu46_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_1_")(batchnorm46_1_1_)

        conv47_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_1_")(relu46_1_1_)
        # conv47_1_1_, output shape: {[14,14,1024]}

        batchnorm47_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_1_",)(conv47_1_1_) #TODO: fix_gamma=True

        conv45_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_2_")(relu43_)
        # conv45_1_2_, output shape: {[28,28,16]}

        batchnorm45_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_2_",)(conv45_1_2_) #TODO: fix_gamma=True

        relu45_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_2_")(batchnorm45_1_2_)

        conv46_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_2_")(relu45_1_2_)
        # conv46_1_2_, output shape: {[14,14,16]}

        batchnorm46_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_2_",)(conv46_1_2_) #TODO: fix_gamma=True

        relu46_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_2_")(batchnorm46_1_2_)

        conv47_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_2_")(relu46_1_2_)
        # conv47_1_2_, output shape: {[14,14,1024]}

        batchnorm47_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_2_",)(conv47_1_2_) #TODO: fix_gamma=True

        conv45_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_3_")(relu43_)
        # conv45_1_3_, output shape: {[28,28,16]}

        batchnorm45_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_3_",)(conv45_1_3_) #TODO: fix_gamma=True

        relu45_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_3_")(batchnorm45_1_3_)

        conv46_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_3_")(relu45_1_3_)
        # conv46_1_3_, output shape: {[14,14,16]}

        batchnorm46_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_3_",)(conv46_1_3_) #TODO: fix_gamma=True

        relu46_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_3_")(batchnorm46_1_3_)

        conv47_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_3_")(relu46_1_3_)
        # conv47_1_3_, output shape: {[14,14,1024]}

        batchnorm47_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_3_",)(conv47_1_3_) #TODO: fix_gamma=True

        conv45_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_4_")(relu43_)
        # conv45_1_4_, output shape: {[28,28,16]}

        batchnorm45_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_4_",)(conv45_1_4_) #TODO: fix_gamma=True

        relu45_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_4_")(batchnorm45_1_4_)

        conv46_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_4_")(relu45_1_4_)
        # conv46_1_4_, output shape: {[14,14,16]}

        batchnorm46_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_4_",)(conv46_1_4_) #TODO: fix_gamma=True

        relu46_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_4_")(batchnorm46_1_4_)

        conv47_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_4_")(relu46_1_4_)
        # conv47_1_4_, output shape: {[14,14,1024]}

        batchnorm47_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_4_",)(conv47_1_4_) #TODO: fix_gamma=True

        conv45_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_5_")(relu43_)
        # conv45_1_5_, output shape: {[28,28,16]}

        batchnorm45_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_5_",)(conv45_1_5_) #TODO: fix_gamma=True

        relu45_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_5_")(batchnorm45_1_5_)

        conv46_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_5_")(relu45_1_5_)
        # conv46_1_5_, output shape: {[14,14,16]}

        batchnorm46_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_5_",)(conv46_1_5_) #TODO: fix_gamma=True

        relu46_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_5_")(batchnorm46_1_5_)

        conv47_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_5_")(relu46_1_5_)
        # conv47_1_5_, output shape: {[14,14,1024]}

        batchnorm47_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_5_",)(conv47_1_5_) #TODO: fix_gamma=True

        conv45_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_6_")(relu43_)
        # conv45_1_6_, output shape: {[28,28,16]}

        batchnorm45_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_6_",)(conv45_1_6_) #TODO: fix_gamma=True

        relu45_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_6_")(batchnorm45_1_6_)

        conv46_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_6_")(relu45_1_6_)
        # conv46_1_6_, output shape: {[14,14,16]}

        batchnorm46_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_6_",)(conv46_1_6_) #TODO: fix_gamma=True

        relu46_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_6_")(batchnorm46_1_6_)

        conv47_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_6_")(relu46_1_6_)
        # conv47_1_6_, output shape: {[14,14,1024]}

        batchnorm47_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_6_",)(conv47_1_6_) #TODO: fix_gamma=True

        conv45_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_7_")(relu43_)
        # conv45_1_7_, output shape: {[28,28,16]}

        batchnorm45_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_7_",)(conv45_1_7_) #TODO: fix_gamma=True

        relu45_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_7_")(batchnorm45_1_7_)

        conv46_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_7_")(relu45_1_7_)
        # conv46_1_7_, output shape: {[14,14,16]}

        batchnorm46_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_7_",)(conv46_1_7_) #TODO: fix_gamma=True

        relu46_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_7_")(batchnorm46_1_7_)

        conv47_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_7_")(relu46_1_7_)
        # conv47_1_7_, output shape: {[14,14,1024]}

        batchnorm47_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_7_",)(conv47_1_7_) #TODO: fix_gamma=True

        conv45_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_8_")(relu43_)
        # conv45_1_8_, output shape: {[28,28,16]}

        batchnorm45_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_8_",)(conv45_1_8_) #TODO: fix_gamma=True

        relu45_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_8_")(batchnorm45_1_8_)

        conv46_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_8_")(relu45_1_8_)
        # conv46_1_8_, output shape: {[14,14,16]}

        batchnorm46_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_8_",)(conv46_1_8_) #TODO: fix_gamma=True

        relu46_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_8_")(batchnorm46_1_8_)

        conv47_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_8_")(relu46_1_8_)
        # conv47_1_8_, output shape: {[14,14,1024]}

        batchnorm47_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_8_",)(conv47_1_8_) #TODO: fix_gamma=True

        conv45_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_9_")(relu43_)
        # conv45_1_9_, output shape: {[28,28,16]}

        batchnorm45_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_9_",)(conv45_1_9_) #TODO: fix_gamma=True

        relu45_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_9_")(batchnorm45_1_9_)

        conv46_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_9_")(relu45_1_9_)
        # conv46_1_9_, output shape: {[14,14,16]}

        batchnorm46_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_9_",)(conv46_1_9_) #TODO: fix_gamma=True

        relu46_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_9_")(batchnorm46_1_9_)

        conv47_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_9_")(relu46_1_9_)
        # conv47_1_9_, output shape: {[14,14,1024]}

        batchnorm47_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_9_",)(conv47_1_9_) #TODO: fix_gamma=True

        conv45_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_10_")(relu43_)
        # conv45_1_10_, output shape: {[28,28,16]}

        batchnorm45_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_10_",)(conv45_1_10_) #TODO: fix_gamma=True

        relu45_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_10_")(batchnorm45_1_10_)

        conv46_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_10_")(relu45_1_10_)
        # conv46_1_10_, output shape: {[14,14,16]}

        batchnorm46_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_10_",)(conv46_1_10_) #TODO: fix_gamma=True

        relu46_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_10_")(batchnorm46_1_10_)

        conv47_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_10_")(relu46_1_10_)
        # conv47_1_10_, output shape: {[14,14,1024]}

        batchnorm47_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_10_",)(conv47_1_10_) #TODO: fix_gamma=True

        conv45_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_11_")(relu43_)
        # conv45_1_11_, output shape: {[28,28,16]}

        batchnorm45_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_11_",)(conv45_1_11_) #TODO: fix_gamma=True

        relu45_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_11_")(batchnorm45_1_11_)

        conv46_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_11_")(relu45_1_11_)
        # conv46_1_11_, output shape: {[14,14,16]}

        batchnorm46_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_11_",)(conv46_1_11_) #TODO: fix_gamma=True

        relu46_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_11_")(batchnorm46_1_11_)

        conv47_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_11_")(relu46_1_11_)
        # conv47_1_11_, output shape: {[14,14,1024]}

        batchnorm47_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_11_",)(conv47_1_11_) #TODO: fix_gamma=True

        conv45_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_12_")(relu43_)
        # conv45_1_12_, output shape: {[28,28,16]}

        batchnorm45_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_12_",)(conv45_1_12_) #TODO: fix_gamma=True

        relu45_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_12_")(batchnorm45_1_12_)

        conv46_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_12_")(relu45_1_12_)
        # conv46_1_12_, output shape: {[14,14,16]}

        batchnorm46_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_12_",)(conv46_1_12_) #TODO: fix_gamma=True

        relu46_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_12_")(batchnorm46_1_12_)

        conv47_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_12_")(relu46_1_12_)
        # conv47_1_12_, output shape: {[14,14,1024]}

        batchnorm47_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_12_",)(conv47_1_12_) #TODO: fix_gamma=True

        conv45_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_13_")(relu43_)
        # conv45_1_13_, output shape: {[28,28,16]}

        batchnorm45_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_13_",)(conv45_1_13_) #TODO: fix_gamma=True

        relu45_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_13_")(batchnorm45_1_13_)

        conv46_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_13_")(relu45_1_13_)
        # conv46_1_13_, output shape: {[14,14,16]}

        batchnorm46_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_13_",)(conv46_1_13_) #TODO: fix_gamma=True

        relu46_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_13_")(batchnorm46_1_13_)

        conv47_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_13_")(relu46_1_13_)
        # conv47_1_13_, output shape: {[14,14,1024]}

        batchnorm47_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_13_",)(conv47_1_13_) #TODO: fix_gamma=True

        conv45_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_14_")(relu43_)
        # conv45_1_14_, output shape: {[28,28,16]}

        batchnorm45_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_14_",)(conv45_1_14_) #TODO: fix_gamma=True

        relu45_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_14_")(batchnorm45_1_14_)

        conv46_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_14_")(relu45_1_14_)
        # conv46_1_14_, output shape: {[14,14,16]}

        batchnorm46_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_14_",)(conv46_1_14_) #TODO: fix_gamma=True

        relu46_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_14_")(batchnorm46_1_14_)

        conv47_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_14_")(relu46_1_14_)
        # conv47_1_14_, output shape: {[14,14,1024]}

        batchnorm47_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_14_",)(conv47_1_14_) #TODO: fix_gamma=True

        conv45_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_15_")(relu43_)
        # conv45_1_15_, output shape: {[28,28,16]}

        batchnorm45_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_15_",)(conv45_1_15_) #TODO: fix_gamma=True

        relu45_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_15_")(batchnorm45_1_15_)

        conv46_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_15_")(relu45_1_15_)
        # conv46_1_15_, output shape: {[14,14,16]}

        batchnorm46_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_15_",)(conv46_1_15_) #TODO: fix_gamma=True

        relu46_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_15_")(batchnorm46_1_15_)

        conv47_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_15_")(relu46_1_15_)
        # conv47_1_15_, output shape: {[14,14,1024]}

        batchnorm47_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_15_",)(conv47_1_15_) #TODO: fix_gamma=True

        conv45_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_16_")(relu43_)
        # conv45_1_16_, output shape: {[28,28,16]}

        batchnorm45_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_16_",)(conv45_1_16_) #TODO: fix_gamma=True

        relu45_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_16_")(batchnorm45_1_16_)

        conv46_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_16_")(relu45_1_16_)
        # conv46_1_16_, output shape: {[14,14,16]}

        batchnorm46_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_16_",)(conv46_1_16_) #TODO: fix_gamma=True

        relu46_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_16_")(batchnorm46_1_16_)

        conv47_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_16_")(relu46_1_16_)
        # conv47_1_16_, output shape: {[14,14,1024]}

        batchnorm47_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_16_",)(conv47_1_16_) #TODO: fix_gamma=True

        conv45_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_17_")(relu43_)
        # conv45_1_17_, output shape: {[28,28,16]}

        batchnorm45_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_17_",)(conv45_1_17_) #TODO: fix_gamma=True

        relu45_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_17_")(batchnorm45_1_17_)

        conv46_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_17_")(relu45_1_17_)
        # conv46_1_17_, output shape: {[14,14,16]}

        batchnorm46_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_17_",)(conv46_1_17_) #TODO: fix_gamma=True

        relu46_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_17_")(batchnorm46_1_17_)

        conv47_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_17_")(relu46_1_17_)
        # conv47_1_17_, output shape: {[14,14,1024]}

        batchnorm47_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_17_",)(conv47_1_17_) #TODO: fix_gamma=True

        conv45_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_18_")(relu43_)
        # conv45_1_18_, output shape: {[28,28,16]}

        batchnorm45_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_18_",)(conv45_1_18_) #TODO: fix_gamma=True

        relu45_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_18_")(batchnorm45_1_18_)

        conv46_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_18_")(relu45_1_18_)
        # conv46_1_18_, output shape: {[14,14,16]}

        batchnorm46_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_18_",)(conv46_1_18_) #TODO: fix_gamma=True

        relu46_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_18_")(batchnorm46_1_18_)

        conv47_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_18_")(relu46_1_18_)
        # conv47_1_18_, output shape: {[14,14,1024]}

        batchnorm47_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_18_",)(conv47_1_18_) #TODO: fix_gamma=True

        conv45_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_19_")(relu43_)
        # conv45_1_19_, output shape: {[28,28,16]}

        batchnorm45_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_19_",)(conv45_1_19_) #TODO: fix_gamma=True

        relu45_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_19_")(batchnorm45_1_19_)

        conv46_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_19_")(relu45_1_19_)
        # conv46_1_19_, output shape: {[14,14,16]}

        batchnorm46_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_19_",)(conv46_1_19_) #TODO: fix_gamma=True

        relu46_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_19_")(batchnorm46_1_19_)

        conv47_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_19_")(relu46_1_19_)
        # conv47_1_19_, output shape: {[14,14,1024]}

        batchnorm47_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_19_",)(conv47_1_19_) #TODO: fix_gamma=True

        conv45_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_20_")(relu43_)
        # conv45_1_20_, output shape: {[28,28,16]}

        batchnorm45_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_20_",)(conv45_1_20_) #TODO: fix_gamma=True

        relu45_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_20_")(batchnorm45_1_20_)

        conv46_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_20_")(relu45_1_20_)
        # conv46_1_20_, output shape: {[14,14,16]}

        batchnorm46_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_20_",)(conv46_1_20_) #TODO: fix_gamma=True

        relu46_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_20_")(batchnorm46_1_20_)

        conv47_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_20_")(relu46_1_20_)
        # conv47_1_20_, output shape: {[14,14,1024]}

        batchnorm47_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_20_",)(conv47_1_20_) #TODO: fix_gamma=True

        conv45_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_21_")(relu43_)
        # conv45_1_21_, output shape: {[28,28,16]}

        batchnorm45_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_21_",)(conv45_1_21_) #TODO: fix_gamma=True

        relu45_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_21_")(batchnorm45_1_21_)

        conv46_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_21_")(relu45_1_21_)
        # conv46_1_21_, output shape: {[14,14,16]}

        batchnorm46_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_21_",)(conv46_1_21_) #TODO: fix_gamma=True

        relu46_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_21_")(batchnorm46_1_21_)

        conv47_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_21_")(relu46_1_21_)
        # conv47_1_21_, output shape: {[14,14,1024]}

        batchnorm47_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_21_",)(conv47_1_21_) #TODO: fix_gamma=True

        conv45_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_22_")(relu43_)
        # conv45_1_22_, output shape: {[28,28,16]}

        batchnorm45_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_22_",)(conv45_1_22_) #TODO: fix_gamma=True

        relu45_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_22_")(batchnorm45_1_22_)

        conv46_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_22_")(relu45_1_22_)
        # conv46_1_22_, output shape: {[14,14,16]}

        batchnorm46_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_22_",)(conv46_1_22_) #TODO: fix_gamma=True

        relu46_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_22_")(batchnorm46_1_22_)

        conv47_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_22_")(relu46_1_22_)
        # conv47_1_22_, output shape: {[14,14,1024]}

        batchnorm47_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_22_",)(conv47_1_22_) #TODO: fix_gamma=True

        conv45_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_23_")(relu43_)
        # conv45_1_23_, output shape: {[28,28,16]}

        batchnorm45_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_23_",)(conv45_1_23_) #TODO: fix_gamma=True

        relu45_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_23_")(batchnorm45_1_23_)

        conv46_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_23_")(relu45_1_23_)
        # conv46_1_23_, output shape: {[14,14,16]}

        batchnorm46_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_23_",)(conv46_1_23_) #TODO: fix_gamma=True

        relu46_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_23_")(batchnorm46_1_23_)

        conv47_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_23_")(relu46_1_23_)
        # conv47_1_23_, output shape: {[14,14,1024]}

        batchnorm47_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_23_",)(conv47_1_23_) #TODO: fix_gamma=True

        conv45_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_24_")(relu43_)
        # conv45_1_24_, output shape: {[28,28,16]}

        batchnorm45_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_24_",)(conv45_1_24_) #TODO: fix_gamma=True

        relu45_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_24_")(batchnorm45_1_24_)

        conv46_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_24_")(relu45_1_24_)
        # conv46_1_24_, output shape: {[14,14,16]}

        batchnorm46_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_24_",)(conv46_1_24_) #TODO: fix_gamma=True

        relu46_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_24_")(batchnorm46_1_24_)

        conv47_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_24_")(relu46_1_24_)
        # conv47_1_24_, output shape: {[14,14,1024]}

        batchnorm47_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_24_",)(conv47_1_24_) #TODO: fix_gamma=True

        conv45_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_25_")(relu43_)
        # conv45_1_25_, output shape: {[28,28,16]}

        batchnorm45_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_25_",)(conv45_1_25_) #TODO: fix_gamma=True

        relu45_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_25_")(batchnorm45_1_25_)

        conv46_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_25_")(relu45_1_25_)
        # conv46_1_25_, output shape: {[14,14,16]}

        batchnorm46_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_25_",)(conv46_1_25_) #TODO: fix_gamma=True

        relu46_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_25_")(batchnorm46_1_25_)

        conv47_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_25_")(relu46_1_25_)
        # conv47_1_25_, output shape: {[14,14,1024]}

        batchnorm47_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_25_",)(conv47_1_25_) #TODO: fix_gamma=True

        conv45_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_26_")(relu43_)
        # conv45_1_26_, output shape: {[28,28,16]}

        batchnorm45_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_26_",)(conv45_1_26_) #TODO: fix_gamma=True

        relu45_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_26_")(batchnorm45_1_26_)

        conv46_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_26_")(relu45_1_26_)
        # conv46_1_26_, output shape: {[14,14,16]}

        batchnorm46_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_26_",)(conv46_1_26_) #TODO: fix_gamma=True

        relu46_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_26_")(batchnorm46_1_26_)

        conv47_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_26_")(relu46_1_26_)
        # conv47_1_26_, output shape: {[14,14,1024]}

        batchnorm47_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_26_",)(conv47_1_26_) #TODO: fix_gamma=True

        conv45_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_27_")(relu43_)
        # conv45_1_27_, output shape: {[28,28,16]}

        batchnorm45_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_27_",)(conv45_1_27_) #TODO: fix_gamma=True

        relu45_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_27_")(batchnorm45_1_27_)

        conv46_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_27_")(relu45_1_27_)
        # conv46_1_27_, output shape: {[14,14,16]}

        batchnorm46_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_27_",)(conv46_1_27_) #TODO: fix_gamma=True

        relu46_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_27_")(batchnorm46_1_27_)

        conv47_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_27_")(relu46_1_27_)
        # conv47_1_27_, output shape: {[14,14,1024]}

        batchnorm47_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_27_",)(conv47_1_27_) #TODO: fix_gamma=True

        conv45_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_28_")(relu43_)
        # conv45_1_28_, output shape: {[28,28,16]}

        batchnorm45_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_28_",)(conv45_1_28_) #TODO: fix_gamma=True

        relu45_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_28_")(batchnorm45_1_28_)

        conv46_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_28_")(relu45_1_28_)
        # conv46_1_28_, output shape: {[14,14,16]}

        batchnorm46_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_28_",)(conv46_1_28_) #TODO: fix_gamma=True

        relu46_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_28_")(batchnorm46_1_28_)

        conv47_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_28_")(relu46_1_28_)
        # conv47_1_28_, output shape: {[14,14,1024]}

        batchnorm47_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_28_",)(conv47_1_28_) #TODO: fix_gamma=True

        conv45_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_29_")(relu43_)
        # conv45_1_29_, output shape: {[28,28,16]}

        batchnorm45_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_29_",)(conv45_1_29_) #TODO: fix_gamma=True

        relu45_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_29_")(batchnorm45_1_29_)

        conv46_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_29_")(relu45_1_29_)
        # conv46_1_29_, output shape: {[14,14,16]}

        batchnorm46_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_29_",)(conv46_1_29_) #TODO: fix_gamma=True

        relu46_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_29_")(batchnorm46_1_29_)

        conv47_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_29_")(relu46_1_29_)
        # conv47_1_29_, output shape: {[14,14,1024]}

        batchnorm47_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_29_",)(conv47_1_29_) #TODO: fix_gamma=True

        conv45_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_30_")(relu43_)
        # conv45_1_30_, output shape: {[28,28,16]}

        batchnorm45_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_30_",)(conv45_1_30_) #TODO: fix_gamma=True

        relu45_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_30_")(batchnorm45_1_30_)

        conv46_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_30_")(relu45_1_30_)
        # conv46_1_30_, output shape: {[14,14,16]}

        batchnorm46_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_30_",)(conv46_1_30_) #TODO: fix_gamma=True

        relu46_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_30_")(batchnorm46_1_30_)

        conv47_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_30_")(relu46_1_30_)
        # conv47_1_30_, output shape: {[14,14,1024]}

        batchnorm47_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_30_",)(conv47_1_30_) #TODO: fix_gamma=True

        conv45_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_31_")(relu43_)
        # conv45_1_31_, output shape: {[28,28,16]}

        batchnorm45_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_31_",)(conv45_1_31_) #TODO: fix_gamma=True

        relu45_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_31_")(batchnorm45_1_31_)

        conv46_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_31_")(relu45_1_31_)
        # conv46_1_31_, output shape: {[14,14,16]}

        batchnorm46_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_31_",)(conv46_1_31_) #TODO: fix_gamma=True

        relu46_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_31_")(batchnorm46_1_31_)

        conv47_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_31_")(relu46_1_31_)
        # conv47_1_31_, output shape: {[14,14,1024]}

        batchnorm47_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_31_",)(conv47_1_31_) #TODO: fix_gamma=True

        conv45_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv45_1_32_")(relu43_)
        # conv45_1_32_, output shape: {[28,28,16]}

        batchnorm45_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm45_1_32_",)(conv45_1_32_) #TODO: fix_gamma=True

        relu45_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu45_1_32_")(batchnorm45_1_32_)

        conv46_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv46_1_32_")(relu45_1_32_)
        # conv46_1_32_, output shape: {[14,14,16]}

        batchnorm46_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm46_1_32_",)(conv46_1_32_) #TODO: fix_gamma=True

        relu46_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu46_1_32_")(batchnorm46_1_32_)

        conv47_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv47_1_32_")(relu46_1_32_)
        # conv47_1_32_, output shape: {[14,14,1024]}

        batchnorm47_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm47_1_32_",)(conv47_1_32_) #TODO: fix_gamma=True

        add48_1_ = tf.keras.layers.Add()([batchnorm47_1_1_,  batchnorm47_1_2_,  batchnorm47_1_3_,  batchnorm47_1_4_,  batchnorm47_1_5_,  batchnorm47_1_6_,  batchnorm47_1_7_,  batchnorm47_1_8_,  batchnorm47_1_9_,  batchnorm47_1_10_,  batchnorm47_1_11_,  batchnorm47_1_12_,  batchnorm47_1_13_,  batchnorm47_1_14_,  batchnorm47_1_15_,  batchnorm47_1_16_,  batchnorm47_1_17_,  batchnorm47_1_18_,  batchnorm47_1_19_,  batchnorm47_1_20_,  batchnorm47_1_21_,  batchnorm47_1_22_,  batchnorm47_1_23_,  batchnorm47_1_24_,  batchnorm47_1_25_,  batchnorm47_1_26_,  batchnorm47_1_27_,  batchnorm47_1_28_,  batchnorm47_1_29_,  batchnorm47_1_30_,  batchnorm47_1_31_,  batchnorm47_1_32_])
        # add48_1_, output shape: {[14,14,1024]}

        conv44_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv44_2_")(relu43_)
        # conv44_2_, output shape: {[14,14,1024]}

        batchnorm44_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm44_2_",)(conv44_2_) #TODO: fix_gamma=True

        add49_ = tf.keras.layers.Add()([add48_1_,  batchnorm44_2_])
        # add49_, output shape: {[14,14,1024]}

        relu49_ = tf.keras.layers.Activation(activation = "relu", name="relu49_")(add49_)

        conv51_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_1_")(relu49_)
        # conv51_1_1_, output shape: {[14,14,16]}

        batchnorm51_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_1_",)(conv51_1_1_) #TODO: fix_gamma=True

        relu51_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_1_")(batchnorm51_1_1_)

        conv52_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_1_")(relu51_1_1_)
        # conv52_1_1_, output shape: {[14,14,16]}

        batchnorm52_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_1_",)(conv52_1_1_) #TODO: fix_gamma=True

        relu52_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_1_")(batchnorm52_1_1_)

        conv53_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_1_")(relu52_1_1_)
        # conv53_1_1_, output shape: {[14,14,1024]}

        batchnorm53_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_1_",)(conv53_1_1_) #TODO: fix_gamma=True

        conv51_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_2_")(relu49_)
        # conv51_1_2_, output shape: {[14,14,16]}

        batchnorm51_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_2_",)(conv51_1_2_) #TODO: fix_gamma=True

        relu51_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_2_")(batchnorm51_1_2_)

        conv52_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_2_")(relu51_1_2_)
        # conv52_1_2_, output shape: {[14,14,16]}

        batchnorm52_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_2_",)(conv52_1_2_) #TODO: fix_gamma=True

        relu52_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_2_")(batchnorm52_1_2_)

        conv53_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_2_")(relu52_1_2_)
        # conv53_1_2_, output shape: {[14,14,1024]}

        batchnorm53_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_2_",)(conv53_1_2_) #TODO: fix_gamma=True

        conv51_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_3_")(relu49_)
        # conv51_1_3_, output shape: {[14,14,16]}

        batchnorm51_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_3_",)(conv51_1_3_) #TODO: fix_gamma=True

        relu51_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_3_")(batchnorm51_1_3_)

        conv52_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_3_")(relu51_1_3_)
        # conv52_1_3_, output shape: {[14,14,16]}

        batchnorm52_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_3_",)(conv52_1_3_) #TODO: fix_gamma=True

        relu52_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_3_")(batchnorm52_1_3_)

        conv53_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_3_")(relu52_1_3_)
        # conv53_1_3_, output shape: {[14,14,1024]}

        batchnorm53_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_3_",)(conv53_1_3_) #TODO: fix_gamma=True

        conv51_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_4_")(relu49_)
        # conv51_1_4_, output shape: {[14,14,16]}

        batchnorm51_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_4_",)(conv51_1_4_) #TODO: fix_gamma=True

        relu51_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_4_")(batchnorm51_1_4_)

        conv52_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_4_")(relu51_1_4_)
        # conv52_1_4_, output shape: {[14,14,16]}

        batchnorm52_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_4_",)(conv52_1_4_) #TODO: fix_gamma=True

        relu52_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_4_")(batchnorm52_1_4_)

        conv53_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_4_")(relu52_1_4_)
        # conv53_1_4_, output shape: {[14,14,1024]}

        batchnorm53_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_4_",)(conv53_1_4_) #TODO: fix_gamma=True

        conv51_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_5_")(relu49_)
        # conv51_1_5_, output shape: {[14,14,16]}

        batchnorm51_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_5_",)(conv51_1_5_) #TODO: fix_gamma=True

        relu51_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_5_")(batchnorm51_1_5_)

        conv52_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_5_")(relu51_1_5_)
        # conv52_1_5_, output shape: {[14,14,16]}

        batchnorm52_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_5_",)(conv52_1_5_) #TODO: fix_gamma=True

        relu52_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_5_")(batchnorm52_1_5_)

        conv53_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_5_")(relu52_1_5_)
        # conv53_1_5_, output shape: {[14,14,1024]}

        batchnorm53_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_5_",)(conv53_1_5_) #TODO: fix_gamma=True

        conv51_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_6_")(relu49_)
        # conv51_1_6_, output shape: {[14,14,16]}

        batchnorm51_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_6_",)(conv51_1_6_) #TODO: fix_gamma=True

        relu51_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_6_")(batchnorm51_1_6_)

        conv52_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_6_")(relu51_1_6_)
        # conv52_1_6_, output shape: {[14,14,16]}

        batchnorm52_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_6_",)(conv52_1_6_) #TODO: fix_gamma=True

        relu52_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_6_")(batchnorm52_1_6_)

        conv53_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_6_")(relu52_1_6_)
        # conv53_1_6_, output shape: {[14,14,1024]}

        batchnorm53_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_6_",)(conv53_1_6_) #TODO: fix_gamma=True

        conv51_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_7_")(relu49_)
        # conv51_1_7_, output shape: {[14,14,16]}

        batchnorm51_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_7_",)(conv51_1_7_) #TODO: fix_gamma=True

        relu51_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_7_")(batchnorm51_1_7_)

        conv52_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_7_")(relu51_1_7_)
        # conv52_1_7_, output shape: {[14,14,16]}

        batchnorm52_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_7_",)(conv52_1_7_) #TODO: fix_gamma=True

        relu52_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_7_")(batchnorm52_1_7_)

        conv53_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_7_")(relu52_1_7_)
        # conv53_1_7_, output shape: {[14,14,1024]}

        batchnorm53_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_7_",)(conv53_1_7_) #TODO: fix_gamma=True

        conv51_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_8_")(relu49_)
        # conv51_1_8_, output shape: {[14,14,16]}

        batchnorm51_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_8_",)(conv51_1_8_) #TODO: fix_gamma=True

        relu51_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_8_")(batchnorm51_1_8_)

        conv52_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_8_")(relu51_1_8_)
        # conv52_1_8_, output shape: {[14,14,16]}

        batchnorm52_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_8_",)(conv52_1_8_) #TODO: fix_gamma=True

        relu52_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_8_")(batchnorm52_1_8_)

        conv53_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_8_")(relu52_1_8_)
        # conv53_1_8_, output shape: {[14,14,1024]}

        batchnorm53_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_8_",)(conv53_1_8_) #TODO: fix_gamma=True

        conv51_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_9_")(relu49_)
        # conv51_1_9_, output shape: {[14,14,16]}

        batchnorm51_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_9_",)(conv51_1_9_) #TODO: fix_gamma=True

        relu51_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_9_")(batchnorm51_1_9_)

        conv52_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_9_")(relu51_1_9_)
        # conv52_1_9_, output shape: {[14,14,16]}

        batchnorm52_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_9_",)(conv52_1_9_) #TODO: fix_gamma=True

        relu52_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_9_")(batchnorm52_1_9_)

        conv53_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_9_")(relu52_1_9_)
        # conv53_1_9_, output shape: {[14,14,1024]}

        batchnorm53_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_9_",)(conv53_1_9_) #TODO: fix_gamma=True

        conv51_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_10_")(relu49_)
        # conv51_1_10_, output shape: {[14,14,16]}

        batchnorm51_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_10_",)(conv51_1_10_) #TODO: fix_gamma=True

        relu51_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_10_")(batchnorm51_1_10_)

        conv52_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_10_")(relu51_1_10_)
        # conv52_1_10_, output shape: {[14,14,16]}

        batchnorm52_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_10_",)(conv52_1_10_) #TODO: fix_gamma=True

        relu52_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_10_")(batchnorm52_1_10_)

        conv53_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_10_")(relu52_1_10_)
        # conv53_1_10_, output shape: {[14,14,1024]}

        batchnorm53_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_10_",)(conv53_1_10_) #TODO: fix_gamma=True

        conv51_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_11_")(relu49_)
        # conv51_1_11_, output shape: {[14,14,16]}

        batchnorm51_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_11_",)(conv51_1_11_) #TODO: fix_gamma=True

        relu51_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_11_")(batchnorm51_1_11_)

        conv52_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_11_")(relu51_1_11_)
        # conv52_1_11_, output shape: {[14,14,16]}

        batchnorm52_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_11_",)(conv52_1_11_) #TODO: fix_gamma=True

        relu52_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_11_")(batchnorm52_1_11_)

        conv53_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_11_")(relu52_1_11_)
        # conv53_1_11_, output shape: {[14,14,1024]}

        batchnorm53_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_11_",)(conv53_1_11_) #TODO: fix_gamma=True

        conv51_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_12_")(relu49_)
        # conv51_1_12_, output shape: {[14,14,16]}

        batchnorm51_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_12_",)(conv51_1_12_) #TODO: fix_gamma=True

        relu51_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_12_")(batchnorm51_1_12_)

        conv52_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_12_")(relu51_1_12_)
        # conv52_1_12_, output shape: {[14,14,16]}

        batchnorm52_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_12_",)(conv52_1_12_) #TODO: fix_gamma=True

        relu52_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_12_")(batchnorm52_1_12_)

        conv53_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_12_")(relu52_1_12_)
        # conv53_1_12_, output shape: {[14,14,1024]}

        batchnorm53_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_12_",)(conv53_1_12_) #TODO: fix_gamma=True

        conv51_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_13_")(relu49_)
        # conv51_1_13_, output shape: {[14,14,16]}

        batchnorm51_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_13_",)(conv51_1_13_) #TODO: fix_gamma=True

        relu51_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_13_")(batchnorm51_1_13_)

        conv52_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_13_")(relu51_1_13_)
        # conv52_1_13_, output shape: {[14,14,16]}

        batchnorm52_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_13_",)(conv52_1_13_) #TODO: fix_gamma=True

        relu52_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_13_")(batchnorm52_1_13_)

        conv53_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_13_")(relu52_1_13_)
        # conv53_1_13_, output shape: {[14,14,1024]}

        batchnorm53_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_13_",)(conv53_1_13_) #TODO: fix_gamma=True

        conv51_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_14_")(relu49_)
        # conv51_1_14_, output shape: {[14,14,16]}

        batchnorm51_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_14_",)(conv51_1_14_) #TODO: fix_gamma=True

        relu51_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_14_")(batchnorm51_1_14_)

        conv52_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_14_")(relu51_1_14_)
        # conv52_1_14_, output shape: {[14,14,16]}

        batchnorm52_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_14_",)(conv52_1_14_) #TODO: fix_gamma=True

        relu52_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_14_")(batchnorm52_1_14_)

        conv53_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_14_")(relu52_1_14_)
        # conv53_1_14_, output shape: {[14,14,1024]}

        batchnorm53_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_14_",)(conv53_1_14_) #TODO: fix_gamma=True

        conv51_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_15_")(relu49_)
        # conv51_1_15_, output shape: {[14,14,16]}

        batchnorm51_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_15_",)(conv51_1_15_) #TODO: fix_gamma=True

        relu51_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_15_")(batchnorm51_1_15_)

        conv52_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_15_")(relu51_1_15_)
        # conv52_1_15_, output shape: {[14,14,16]}

        batchnorm52_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_15_",)(conv52_1_15_) #TODO: fix_gamma=True

        relu52_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_15_")(batchnorm52_1_15_)

        conv53_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_15_")(relu52_1_15_)
        # conv53_1_15_, output shape: {[14,14,1024]}

        batchnorm53_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_15_",)(conv53_1_15_) #TODO: fix_gamma=True

        conv51_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_16_")(relu49_)
        # conv51_1_16_, output shape: {[14,14,16]}

        batchnorm51_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_16_",)(conv51_1_16_) #TODO: fix_gamma=True

        relu51_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_16_")(batchnorm51_1_16_)

        conv52_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_16_")(relu51_1_16_)
        # conv52_1_16_, output shape: {[14,14,16]}

        batchnorm52_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_16_",)(conv52_1_16_) #TODO: fix_gamma=True

        relu52_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_16_")(batchnorm52_1_16_)

        conv53_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_16_")(relu52_1_16_)
        # conv53_1_16_, output shape: {[14,14,1024]}

        batchnorm53_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_16_",)(conv53_1_16_) #TODO: fix_gamma=True

        conv51_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_17_")(relu49_)
        # conv51_1_17_, output shape: {[14,14,16]}

        batchnorm51_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_17_",)(conv51_1_17_) #TODO: fix_gamma=True

        relu51_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_17_")(batchnorm51_1_17_)

        conv52_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_17_")(relu51_1_17_)
        # conv52_1_17_, output shape: {[14,14,16]}

        batchnorm52_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_17_",)(conv52_1_17_) #TODO: fix_gamma=True

        relu52_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_17_")(batchnorm52_1_17_)

        conv53_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_17_")(relu52_1_17_)
        # conv53_1_17_, output shape: {[14,14,1024]}

        batchnorm53_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_17_",)(conv53_1_17_) #TODO: fix_gamma=True

        conv51_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_18_")(relu49_)
        # conv51_1_18_, output shape: {[14,14,16]}

        batchnorm51_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_18_",)(conv51_1_18_) #TODO: fix_gamma=True

        relu51_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_18_")(batchnorm51_1_18_)

        conv52_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_18_")(relu51_1_18_)
        # conv52_1_18_, output shape: {[14,14,16]}

        batchnorm52_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_18_",)(conv52_1_18_) #TODO: fix_gamma=True

        relu52_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_18_")(batchnorm52_1_18_)

        conv53_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_18_")(relu52_1_18_)
        # conv53_1_18_, output shape: {[14,14,1024]}

        batchnorm53_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_18_",)(conv53_1_18_) #TODO: fix_gamma=True

        conv51_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_19_")(relu49_)
        # conv51_1_19_, output shape: {[14,14,16]}

        batchnorm51_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_19_",)(conv51_1_19_) #TODO: fix_gamma=True

        relu51_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_19_")(batchnorm51_1_19_)

        conv52_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_19_")(relu51_1_19_)
        # conv52_1_19_, output shape: {[14,14,16]}

        batchnorm52_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_19_",)(conv52_1_19_) #TODO: fix_gamma=True

        relu52_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_19_")(batchnorm52_1_19_)

        conv53_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_19_")(relu52_1_19_)
        # conv53_1_19_, output shape: {[14,14,1024]}

        batchnorm53_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_19_",)(conv53_1_19_) #TODO: fix_gamma=True

        conv51_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_20_")(relu49_)
        # conv51_1_20_, output shape: {[14,14,16]}

        batchnorm51_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_20_",)(conv51_1_20_) #TODO: fix_gamma=True

        relu51_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_20_")(batchnorm51_1_20_)

        conv52_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_20_")(relu51_1_20_)
        # conv52_1_20_, output shape: {[14,14,16]}

        batchnorm52_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_20_",)(conv52_1_20_) #TODO: fix_gamma=True

        relu52_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_20_")(batchnorm52_1_20_)

        conv53_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_20_")(relu52_1_20_)
        # conv53_1_20_, output shape: {[14,14,1024]}

        batchnorm53_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_20_",)(conv53_1_20_) #TODO: fix_gamma=True

        conv51_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_21_")(relu49_)
        # conv51_1_21_, output shape: {[14,14,16]}

        batchnorm51_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_21_",)(conv51_1_21_) #TODO: fix_gamma=True

        relu51_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_21_")(batchnorm51_1_21_)

        conv52_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_21_")(relu51_1_21_)
        # conv52_1_21_, output shape: {[14,14,16]}

        batchnorm52_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_21_",)(conv52_1_21_) #TODO: fix_gamma=True

        relu52_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_21_")(batchnorm52_1_21_)

        conv53_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_21_")(relu52_1_21_)
        # conv53_1_21_, output shape: {[14,14,1024]}

        batchnorm53_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_21_",)(conv53_1_21_) #TODO: fix_gamma=True

        conv51_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_22_")(relu49_)
        # conv51_1_22_, output shape: {[14,14,16]}

        batchnorm51_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_22_",)(conv51_1_22_) #TODO: fix_gamma=True

        relu51_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_22_")(batchnorm51_1_22_)

        conv52_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_22_")(relu51_1_22_)
        # conv52_1_22_, output shape: {[14,14,16]}

        batchnorm52_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_22_",)(conv52_1_22_) #TODO: fix_gamma=True

        relu52_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_22_")(batchnorm52_1_22_)

        conv53_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_22_")(relu52_1_22_)
        # conv53_1_22_, output shape: {[14,14,1024]}

        batchnorm53_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_22_",)(conv53_1_22_) #TODO: fix_gamma=True

        conv51_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_23_")(relu49_)
        # conv51_1_23_, output shape: {[14,14,16]}

        batchnorm51_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_23_",)(conv51_1_23_) #TODO: fix_gamma=True

        relu51_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_23_")(batchnorm51_1_23_)

        conv52_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_23_")(relu51_1_23_)
        # conv52_1_23_, output shape: {[14,14,16]}

        batchnorm52_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_23_",)(conv52_1_23_) #TODO: fix_gamma=True

        relu52_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_23_")(batchnorm52_1_23_)

        conv53_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_23_")(relu52_1_23_)
        # conv53_1_23_, output shape: {[14,14,1024]}

        batchnorm53_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_23_",)(conv53_1_23_) #TODO: fix_gamma=True

        conv51_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_24_")(relu49_)
        # conv51_1_24_, output shape: {[14,14,16]}

        batchnorm51_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_24_",)(conv51_1_24_) #TODO: fix_gamma=True

        relu51_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_24_")(batchnorm51_1_24_)

        conv52_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_24_")(relu51_1_24_)
        # conv52_1_24_, output shape: {[14,14,16]}

        batchnorm52_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_24_",)(conv52_1_24_) #TODO: fix_gamma=True

        relu52_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_24_")(batchnorm52_1_24_)

        conv53_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_24_")(relu52_1_24_)
        # conv53_1_24_, output shape: {[14,14,1024]}

        batchnorm53_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_24_",)(conv53_1_24_) #TODO: fix_gamma=True

        conv51_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_25_")(relu49_)
        # conv51_1_25_, output shape: {[14,14,16]}

        batchnorm51_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_25_",)(conv51_1_25_) #TODO: fix_gamma=True

        relu51_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_25_")(batchnorm51_1_25_)

        conv52_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_25_")(relu51_1_25_)
        # conv52_1_25_, output shape: {[14,14,16]}

        batchnorm52_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_25_",)(conv52_1_25_) #TODO: fix_gamma=True

        relu52_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_25_")(batchnorm52_1_25_)

        conv53_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_25_")(relu52_1_25_)
        # conv53_1_25_, output shape: {[14,14,1024]}

        batchnorm53_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_25_",)(conv53_1_25_) #TODO: fix_gamma=True

        conv51_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_26_")(relu49_)
        # conv51_1_26_, output shape: {[14,14,16]}

        batchnorm51_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_26_",)(conv51_1_26_) #TODO: fix_gamma=True

        relu51_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_26_")(batchnorm51_1_26_)

        conv52_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_26_")(relu51_1_26_)
        # conv52_1_26_, output shape: {[14,14,16]}

        batchnorm52_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_26_",)(conv52_1_26_) #TODO: fix_gamma=True

        relu52_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_26_")(batchnorm52_1_26_)

        conv53_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_26_")(relu52_1_26_)
        # conv53_1_26_, output shape: {[14,14,1024]}

        batchnorm53_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_26_",)(conv53_1_26_) #TODO: fix_gamma=True

        conv51_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_27_")(relu49_)
        # conv51_1_27_, output shape: {[14,14,16]}

        batchnorm51_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_27_",)(conv51_1_27_) #TODO: fix_gamma=True

        relu51_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_27_")(batchnorm51_1_27_)

        conv52_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_27_")(relu51_1_27_)
        # conv52_1_27_, output shape: {[14,14,16]}

        batchnorm52_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_27_",)(conv52_1_27_) #TODO: fix_gamma=True

        relu52_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_27_")(batchnorm52_1_27_)

        conv53_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_27_")(relu52_1_27_)
        # conv53_1_27_, output shape: {[14,14,1024]}

        batchnorm53_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_27_",)(conv53_1_27_) #TODO: fix_gamma=True

        conv51_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_28_")(relu49_)
        # conv51_1_28_, output shape: {[14,14,16]}

        batchnorm51_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_28_",)(conv51_1_28_) #TODO: fix_gamma=True

        relu51_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_28_")(batchnorm51_1_28_)

        conv52_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_28_")(relu51_1_28_)
        # conv52_1_28_, output shape: {[14,14,16]}

        batchnorm52_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_28_",)(conv52_1_28_) #TODO: fix_gamma=True

        relu52_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_28_")(batchnorm52_1_28_)

        conv53_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_28_")(relu52_1_28_)
        # conv53_1_28_, output shape: {[14,14,1024]}

        batchnorm53_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_28_",)(conv53_1_28_) #TODO: fix_gamma=True

        conv51_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_29_")(relu49_)
        # conv51_1_29_, output shape: {[14,14,16]}

        batchnorm51_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_29_",)(conv51_1_29_) #TODO: fix_gamma=True

        relu51_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_29_")(batchnorm51_1_29_)

        conv52_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_29_")(relu51_1_29_)
        # conv52_1_29_, output shape: {[14,14,16]}

        batchnorm52_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_29_",)(conv52_1_29_) #TODO: fix_gamma=True

        relu52_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_29_")(batchnorm52_1_29_)

        conv53_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_29_")(relu52_1_29_)
        # conv53_1_29_, output shape: {[14,14,1024]}

        batchnorm53_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_29_",)(conv53_1_29_) #TODO: fix_gamma=True

        conv51_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_30_")(relu49_)
        # conv51_1_30_, output shape: {[14,14,16]}

        batchnorm51_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_30_",)(conv51_1_30_) #TODO: fix_gamma=True

        relu51_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_30_")(batchnorm51_1_30_)

        conv52_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_30_")(relu51_1_30_)
        # conv52_1_30_, output shape: {[14,14,16]}

        batchnorm52_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_30_",)(conv52_1_30_) #TODO: fix_gamma=True

        relu52_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_30_")(batchnorm52_1_30_)

        conv53_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_30_")(relu52_1_30_)
        # conv53_1_30_, output shape: {[14,14,1024]}

        batchnorm53_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_30_",)(conv53_1_30_) #TODO: fix_gamma=True

        conv51_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_31_")(relu49_)
        # conv51_1_31_, output shape: {[14,14,16]}

        batchnorm51_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_31_",)(conv51_1_31_) #TODO: fix_gamma=True

        relu51_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_31_")(batchnorm51_1_31_)

        conv52_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_31_")(relu51_1_31_)
        # conv52_1_31_, output shape: {[14,14,16]}

        batchnorm52_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_31_",)(conv52_1_31_) #TODO: fix_gamma=True

        relu52_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_31_")(batchnorm52_1_31_)

        conv53_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_31_")(relu52_1_31_)
        # conv53_1_31_, output shape: {[14,14,1024]}

        batchnorm53_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_31_",)(conv53_1_31_) #TODO: fix_gamma=True

        conv51_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv51_1_32_")(relu49_)
        # conv51_1_32_, output shape: {[14,14,16]}

        batchnorm51_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm51_1_32_",)(conv51_1_32_) #TODO: fix_gamma=True

        relu51_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu51_1_32_")(batchnorm51_1_32_)

        conv52_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv52_1_32_")(relu51_1_32_)
        # conv52_1_32_, output shape: {[14,14,16]}

        batchnorm52_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm52_1_32_",)(conv52_1_32_) #TODO: fix_gamma=True

        relu52_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu52_1_32_")(batchnorm52_1_32_)

        conv53_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv53_1_32_")(relu52_1_32_)
        # conv53_1_32_, output shape: {[14,14,1024]}

        batchnorm53_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm53_1_32_",)(conv53_1_32_) #TODO: fix_gamma=True

        add54_1_ = tf.keras.layers.Add()([batchnorm53_1_1_,  batchnorm53_1_2_,  batchnorm53_1_3_,  batchnorm53_1_4_,  batchnorm53_1_5_,  batchnorm53_1_6_,  batchnorm53_1_7_,  batchnorm53_1_8_,  batchnorm53_1_9_,  batchnorm53_1_10_,  batchnorm53_1_11_,  batchnorm53_1_12_,  batchnorm53_1_13_,  batchnorm53_1_14_,  batchnorm53_1_15_,  batchnorm53_1_16_,  batchnorm53_1_17_,  batchnorm53_1_18_,  batchnorm53_1_19_,  batchnorm53_1_20_,  batchnorm53_1_21_,  batchnorm53_1_22_,  batchnorm53_1_23_,  batchnorm53_1_24_,  batchnorm53_1_25_,  batchnorm53_1_26_,  batchnorm53_1_27_,  batchnorm53_1_28_,  batchnorm53_1_29_,  batchnorm53_1_30_,  batchnorm53_1_31_,  batchnorm53_1_32_])
        # add54_1_, output shape: {[14,14,1024]}

        add55_ = tf.keras.layers.Add()([add54_1_,  relu49_])
        # add55_, output shape: {[14,14,1024]}

        relu55_ = tf.keras.layers.Activation(activation = "relu", name="relu55_")(add55_)

        conv57_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_1_")(relu55_)
        # conv57_1_1_, output shape: {[14,14,16]}

        batchnorm57_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_1_",)(conv57_1_1_) #TODO: fix_gamma=True

        relu57_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_1_")(batchnorm57_1_1_)

        conv58_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_1_")(relu57_1_1_)
        # conv58_1_1_, output shape: {[14,14,16]}

        batchnorm58_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_1_",)(conv58_1_1_) #TODO: fix_gamma=True

        relu58_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_1_")(batchnorm58_1_1_)

        conv59_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_1_")(relu58_1_1_)
        # conv59_1_1_, output shape: {[14,14,1024]}

        batchnorm59_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_1_",)(conv59_1_1_) #TODO: fix_gamma=True

        conv57_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_2_")(relu55_)
        # conv57_1_2_, output shape: {[14,14,16]}

        batchnorm57_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_2_",)(conv57_1_2_) #TODO: fix_gamma=True

        relu57_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_2_")(batchnorm57_1_2_)

        conv58_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_2_")(relu57_1_2_)
        # conv58_1_2_, output shape: {[14,14,16]}

        batchnorm58_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_2_",)(conv58_1_2_) #TODO: fix_gamma=True

        relu58_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_2_")(batchnorm58_1_2_)

        conv59_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_2_")(relu58_1_2_)
        # conv59_1_2_, output shape: {[14,14,1024]}

        batchnorm59_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_2_",)(conv59_1_2_) #TODO: fix_gamma=True

        conv57_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_3_")(relu55_)
        # conv57_1_3_, output shape: {[14,14,16]}

        batchnorm57_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_3_",)(conv57_1_3_) #TODO: fix_gamma=True

        relu57_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_3_")(batchnorm57_1_3_)

        conv58_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_3_")(relu57_1_3_)
        # conv58_1_3_, output shape: {[14,14,16]}

        batchnorm58_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_3_",)(conv58_1_3_) #TODO: fix_gamma=True

        relu58_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_3_")(batchnorm58_1_3_)

        conv59_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_3_")(relu58_1_3_)
        # conv59_1_3_, output shape: {[14,14,1024]}

        batchnorm59_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_3_",)(conv59_1_3_) #TODO: fix_gamma=True

        conv57_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_4_")(relu55_)
        # conv57_1_4_, output shape: {[14,14,16]}

        batchnorm57_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_4_",)(conv57_1_4_) #TODO: fix_gamma=True

        relu57_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_4_")(batchnorm57_1_4_)

        conv58_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_4_")(relu57_1_4_)
        # conv58_1_4_, output shape: {[14,14,16]}

        batchnorm58_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_4_",)(conv58_1_4_) #TODO: fix_gamma=True

        relu58_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_4_")(batchnorm58_1_4_)

        conv59_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_4_")(relu58_1_4_)
        # conv59_1_4_, output shape: {[14,14,1024]}

        batchnorm59_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_4_",)(conv59_1_4_) #TODO: fix_gamma=True

        conv57_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_5_")(relu55_)
        # conv57_1_5_, output shape: {[14,14,16]}

        batchnorm57_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_5_",)(conv57_1_5_) #TODO: fix_gamma=True

        relu57_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_5_")(batchnorm57_1_5_)

        conv58_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_5_")(relu57_1_5_)
        # conv58_1_5_, output shape: {[14,14,16]}

        batchnorm58_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_5_",)(conv58_1_5_) #TODO: fix_gamma=True

        relu58_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_5_")(batchnorm58_1_5_)

        conv59_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_5_")(relu58_1_5_)
        # conv59_1_5_, output shape: {[14,14,1024]}

        batchnorm59_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_5_",)(conv59_1_5_) #TODO: fix_gamma=True

        conv57_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_6_")(relu55_)
        # conv57_1_6_, output shape: {[14,14,16]}

        batchnorm57_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_6_",)(conv57_1_6_) #TODO: fix_gamma=True

        relu57_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_6_")(batchnorm57_1_6_)

        conv58_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_6_")(relu57_1_6_)
        # conv58_1_6_, output shape: {[14,14,16]}

        batchnorm58_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_6_",)(conv58_1_6_) #TODO: fix_gamma=True

        relu58_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_6_")(batchnorm58_1_6_)

        conv59_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_6_")(relu58_1_6_)
        # conv59_1_6_, output shape: {[14,14,1024]}

        batchnorm59_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_6_",)(conv59_1_6_) #TODO: fix_gamma=True

        conv57_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_7_")(relu55_)
        # conv57_1_7_, output shape: {[14,14,16]}

        batchnorm57_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_7_",)(conv57_1_7_) #TODO: fix_gamma=True

        relu57_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_7_")(batchnorm57_1_7_)

        conv58_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_7_")(relu57_1_7_)
        # conv58_1_7_, output shape: {[14,14,16]}

        batchnorm58_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_7_",)(conv58_1_7_) #TODO: fix_gamma=True

        relu58_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_7_")(batchnorm58_1_7_)

        conv59_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_7_")(relu58_1_7_)
        # conv59_1_7_, output shape: {[14,14,1024]}

        batchnorm59_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_7_",)(conv59_1_7_) #TODO: fix_gamma=True

        conv57_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_8_")(relu55_)
        # conv57_1_8_, output shape: {[14,14,16]}

        batchnorm57_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_8_",)(conv57_1_8_) #TODO: fix_gamma=True

        relu57_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_8_")(batchnorm57_1_8_)

        conv58_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_8_")(relu57_1_8_)
        # conv58_1_8_, output shape: {[14,14,16]}

        batchnorm58_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_8_",)(conv58_1_8_) #TODO: fix_gamma=True

        relu58_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_8_")(batchnorm58_1_8_)

        conv59_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_8_")(relu58_1_8_)
        # conv59_1_8_, output shape: {[14,14,1024]}

        batchnorm59_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_8_",)(conv59_1_8_) #TODO: fix_gamma=True

        conv57_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_9_")(relu55_)
        # conv57_1_9_, output shape: {[14,14,16]}

        batchnorm57_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_9_",)(conv57_1_9_) #TODO: fix_gamma=True

        relu57_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_9_")(batchnorm57_1_9_)

        conv58_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_9_")(relu57_1_9_)
        # conv58_1_9_, output shape: {[14,14,16]}

        batchnorm58_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_9_",)(conv58_1_9_) #TODO: fix_gamma=True

        relu58_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_9_")(batchnorm58_1_9_)

        conv59_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_9_")(relu58_1_9_)
        # conv59_1_9_, output shape: {[14,14,1024]}

        batchnorm59_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_9_",)(conv59_1_9_) #TODO: fix_gamma=True

        conv57_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_10_")(relu55_)
        # conv57_1_10_, output shape: {[14,14,16]}

        batchnorm57_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_10_",)(conv57_1_10_) #TODO: fix_gamma=True

        relu57_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_10_")(batchnorm57_1_10_)

        conv58_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_10_")(relu57_1_10_)
        # conv58_1_10_, output shape: {[14,14,16]}

        batchnorm58_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_10_",)(conv58_1_10_) #TODO: fix_gamma=True

        relu58_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_10_")(batchnorm58_1_10_)

        conv59_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_10_")(relu58_1_10_)
        # conv59_1_10_, output shape: {[14,14,1024]}

        batchnorm59_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_10_",)(conv59_1_10_) #TODO: fix_gamma=True

        conv57_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_11_")(relu55_)
        # conv57_1_11_, output shape: {[14,14,16]}

        batchnorm57_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_11_",)(conv57_1_11_) #TODO: fix_gamma=True

        relu57_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_11_")(batchnorm57_1_11_)

        conv58_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_11_")(relu57_1_11_)
        # conv58_1_11_, output shape: {[14,14,16]}

        batchnorm58_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_11_",)(conv58_1_11_) #TODO: fix_gamma=True

        relu58_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_11_")(batchnorm58_1_11_)

        conv59_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_11_")(relu58_1_11_)
        # conv59_1_11_, output shape: {[14,14,1024]}

        batchnorm59_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_11_",)(conv59_1_11_) #TODO: fix_gamma=True

        conv57_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_12_")(relu55_)
        # conv57_1_12_, output shape: {[14,14,16]}

        batchnorm57_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_12_",)(conv57_1_12_) #TODO: fix_gamma=True

        relu57_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_12_")(batchnorm57_1_12_)

        conv58_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_12_")(relu57_1_12_)
        # conv58_1_12_, output shape: {[14,14,16]}

        batchnorm58_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_12_",)(conv58_1_12_) #TODO: fix_gamma=True

        relu58_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_12_")(batchnorm58_1_12_)

        conv59_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_12_")(relu58_1_12_)
        # conv59_1_12_, output shape: {[14,14,1024]}

        batchnorm59_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_12_",)(conv59_1_12_) #TODO: fix_gamma=True

        conv57_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_13_")(relu55_)
        # conv57_1_13_, output shape: {[14,14,16]}

        batchnorm57_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_13_",)(conv57_1_13_) #TODO: fix_gamma=True

        relu57_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_13_")(batchnorm57_1_13_)

        conv58_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_13_")(relu57_1_13_)
        # conv58_1_13_, output shape: {[14,14,16]}

        batchnorm58_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_13_",)(conv58_1_13_) #TODO: fix_gamma=True

        relu58_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_13_")(batchnorm58_1_13_)

        conv59_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_13_")(relu58_1_13_)
        # conv59_1_13_, output shape: {[14,14,1024]}

        batchnorm59_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_13_",)(conv59_1_13_) #TODO: fix_gamma=True

        conv57_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_14_")(relu55_)
        # conv57_1_14_, output shape: {[14,14,16]}

        batchnorm57_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_14_",)(conv57_1_14_) #TODO: fix_gamma=True

        relu57_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_14_")(batchnorm57_1_14_)

        conv58_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_14_")(relu57_1_14_)
        # conv58_1_14_, output shape: {[14,14,16]}

        batchnorm58_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_14_",)(conv58_1_14_) #TODO: fix_gamma=True

        relu58_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_14_")(batchnorm58_1_14_)

        conv59_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_14_")(relu58_1_14_)
        # conv59_1_14_, output shape: {[14,14,1024]}

        batchnorm59_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_14_",)(conv59_1_14_) #TODO: fix_gamma=True

        conv57_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_15_")(relu55_)
        # conv57_1_15_, output shape: {[14,14,16]}

        batchnorm57_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_15_",)(conv57_1_15_) #TODO: fix_gamma=True

        relu57_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_15_")(batchnorm57_1_15_)

        conv58_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_15_")(relu57_1_15_)
        # conv58_1_15_, output shape: {[14,14,16]}

        batchnorm58_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_15_",)(conv58_1_15_) #TODO: fix_gamma=True

        relu58_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_15_")(batchnorm58_1_15_)

        conv59_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_15_")(relu58_1_15_)
        # conv59_1_15_, output shape: {[14,14,1024]}

        batchnorm59_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_15_",)(conv59_1_15_) #TODO: fix_gamma=True

        conv57_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_16_")(relu55_)
        # conv57_1_16_, output shape: {[14,14,16]}

        batchnorm57_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_16_",)(conv57_1_16_) #TODO: fix_gamma=True

        relu57_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_16_")(batchnorm57_1_16_)

        conv58_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_16_")(relu57_1_16_)
        # conv58_1_16_, output shape: {[14,14,16]}

        batchnorm58_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_16_",)(conv58_1_16_) #TODO: fix_gamma=True

        relu58_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_16_")(batchnorm58_1_16_)

        conv59_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_16_")(relu58_1_16_)
        # conv59_1_16_, output shape: {[14,14,1024]}

        batchnorm59_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_16_",)(conv59_1_16_) #TODO: fix_gamma=True

        conv57_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_17_")(relu55_)
        # conv57_1_17_, output shape: {[14,14,16]}

        batchnorm57_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_17_",)(conv57_1_17_) #TODO: fix_gamma=True

        relu57_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_17_")(batchnorm57_1_17_)

        conv58_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_17_")(relu57_1_17_)
        # conv58_1_17_, output shape: {[14,14,16]}

        batchnorm58_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_17_",)(conv58_1_17_) #TODO: fix_gamma=True

        relu58_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_17_")(batchnorm58_1_17_)

        conv59_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_17_")(relu58_1_17_)
        # conv59_1_17_, output shape: {[14,14,1024]}

        batchnorm59_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_17_",)(conv59_1_17_) #TODO: fix_gamma=True

        conv57_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_18_")(relu55_)
        # conv57_1_18_, output shape: {[14,14,16]}

        batchnorm57_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_18_",)(conv57_1_18_) #TODO: fix_gamma=True

        relu57_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_18_")(batchnorm57_1_18_)

        conv58_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_18_")(relu57_1_18_)
        # conv58_1_18_, output shape: {[14,14,16]}

        batchnorm58_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_18_",)(conv58_1_18_) #TODO: fix_gamma=True

        relu58_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_18_")(batchnorm58_1_18_)

        conv59_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_18_")(relu58_1_18_)
        # conv59_1_18_, output shape: {[14,14,1024]}

        batchnorm59_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_18_",)(conv59_1_18_) #TODO: fix_gamma=True

        conv57_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_19_")(relu55_)
        # conv57_1_19_, output shape: {[14,14,16]}

        batchnorm57_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_19_",)(conv57_1_19_) #TODO: fix_gamma=True

        relu57_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_19_")(batchnorm57_1_19_)

        conv58_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_19_")(relu57_1_19_)
        # conv58_1_19_, output shape: {[14,14,16]}

        batchnorm58_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_19_",)(conv58_1_19_) #TODO: fix_gamma=True

        relu58_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_19_")(batchnorm58_1_19_)

        conv59_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_19_")(relu58_1_19_)
        # conv59_1_19_, output shape: {[14,14,1024]}

        batchnorm59_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_19_",)(conv59_1_19_) #TODO: fix_gamma=True

        conv57_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_20_")(relu55_)
        # conv57_1_20_, output shape: {[14,14,16]}

        batchnorm57_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_20_",)(conv57_1_20_) #TODO: fix_gamma=True

        relu57_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_20_")(batchnorm57_1_20_)

        conv58_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_20_")(relu57_1_20_)
        # conv58_1_20_, output shape: {[14,14,16]}

        batchnorm58_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_20_",)(conv58_1_20_) #TODO: fix_gamma=True

        relu58_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_20_")(batchnorm58_1_20_)

        conv59_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_20_")(relu58_1_20_)
        # conv59_1_20_, output shape: {[14,14,1024]}

        batchnorm59_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_20_",)(conv59_1_20_) #TODO: fix_gamma=True

        conv57_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_21_")(relu55_)
        # conv57_1_21_, output shape: {[14,14,16]}

        batchnorm57_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_21_",)(conv57_1_21_) #TODO: fix_gamma=True

        relu57_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_21_")(batchnorm57_1_21_)

        conv58_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_21_")(relu57_1_21_)
        # conv58_1_21_, output shape: {[14,14,16]}

        batchnorm58_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_21_",)(conv58_1_21_) #TODO: fix_gamma=True

        relu58_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_21_")(batchnorm58_1_21_)

        conv59_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_21_")(relu58_1_21_)
        # conv59_1_21_, output shape: {[14,14,1024]}

        batchnorm59_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_21_",)(conv59_1_21_) #TODO: fix_gamma=True

        conv57_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_22_")(relu55_)
        # conv57_1_22_, output shape: {[14,14,16]}

        batchnorm57_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_22_",)(conv57_1_22_) #TODO: fix_gamma=True

        relu57_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_22_")(batchnorm57_1_22_)

        conv58_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_22_")(relu57_1_22_)
        # conv58_1_22_, output shape: {[14,14,16]}

        batchnorm58_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_22_",)(conv58_1_22_) #TODO: fix_gamma=True

        relu58_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_22_")(batchnorm58_1_22_)

        conv59_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_22_")(relu58_1_22_)
        # conv59_1_22_, output shape: {[14,14,1024]}

        batchnorm59_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_22_",)(conv59_1_22_) #TODO: fix_gamma=True

        conv57_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_23_")(relu55_)
        # conv57_1_23_, output shape: {[14,14,16]}

        batchnorm57_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_23_",)(conv57_1_23_) #TODO: fix_gamma=True

        relu57_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_23_")(batchnorm57_1_23_)

        conv58_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_23_")(relu57_1_23_)
        # conv58_1_23_, output shape: {[14,14,16]}

        batchnorm58_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_23_",)(conv58_1_23_) #TODO: fix_gamma=True

        relu58_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_23_")(batchnorm58_1_23_)

        conv59_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_23_")(relu58_1_23_)
        # conv59_1_23_, output shape: {[14,14,1024]}

        batchnorm59_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_23_",)(conv59_1_23_) #TODO: fix_gamma=True

        conv57_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_24_")(relu55_)
        # conv57_1_24_, output shape: {[14,14,16]}

        batchnorm57_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_24_",)(conv57_1_24_) #TODO: fix_gamma=True

        relu57_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_24_")(batchnorm57_1_24_)

        conv58_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_24_")(relu57_1_24_)
        # conv58_1_24_, output shape: {[14,14,16]}

        batchnorm58_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_24_",)(conv58_1_24_) #TODO: fix_gamma=True

        relu58_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_24_")(batchnorm58_1_24_)

        conv59_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_24_")(relu58_1_24_)
        # conv59_1_24_, output shape: {[14,14,1024]}

        batchnorm59_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_24_",)(conv59_1_24_) #TODO: fix_gamma=True

        conv57_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_25_")(relu55_)
        # conv57_1_25_, output shape: {[14,14,16]}

        batchnorm57_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_25_",)(conv57_1_25_) #TODO: fix_gamma=True

        relu57_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_25_")(batchnorm57_1_25_)

        conv58_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_25_")(relu57_1_25_)
        # conv58_1_25_, output shape: {[14,14,16]}

        batchnorm58_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_25_",)(conv58_1_25_) #TODO: fix_gamma=True

        relu58_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_25_")(batchnorm58_1_25_)

        conv59_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_25_")(relu58_1_25_)
        # conv59_1_25_, output shape: {[14,14,1024]}

        batchnorm59_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_25_",)(conv59_1_25_) #TODO: fix_gamma=True

        conv57_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_26_")(relu55_)
        # conv57_1_26_, output shape: {[14,14,16]}

        batchnorm57_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_26_",)(conv57_1_26_) #TODO: fix_gamma=True

        relu57_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_26_")(batchnorm57_1_26_)

        conv58_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_26_")(relu57_1_26_)
        # conv58_1_26_, output shape: {[14,14,16]}

        batchnorm58_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_26_",)(conv58_1_26_) #TODO: fix_gamma=True

        relu58_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_26_")(batchnorm58_1_26_)

        conv59_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_26_")(relu58_1_26_)
        # conv59_1_26_, output shape: {[14,14,1024]}

        batchnorm59_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_26_",)(conv59_1_26_) #TODO: fix_gamma=True

        conv57_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_27_")(relu55_)
        # conv57_1_27_, output shape: {[14,14,16]}

        batchnorm57_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_27_",)(conv57_1_27_) #TODO: fix_gamma=True

        relu57_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_27_")(batchnorm57_1_27_)

        conv58_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_27_")(relu57_1_27_)
        # conv58_1_27_, output shape: {[14,14,16]}

        batchnorm58_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_27_",)(conv58_1_27_) #TODO: fix_gamma=True

        relu58_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_27_")(batchnorm58_1_27_)

        conv59_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_27_")(relu58_1_27_)
        # conv59_1_27_, output shape: {[14,14,1024]}

        batchnorm59_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_27_",)(conv59_1_27_) #TODO: fix_gamma=True

        conv57_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_28_")(relu55_)
        # conv57_1_28_, output shape: {[14,14,16]}

        batchnorm57_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_28_",)(conv57_1_28_) #TODO: fix_gamma=True

        relu57_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_28_")(batchnorm57_1_28_)

        conv58_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_28_")(relu57_1_28_)
        # conv58_1_28_, output shape: {[14,14,16]}

        batchnorm58_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_28_",)(conv58_1_28_) #TODO: fix_gamma=True

        relu58_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_28_")(batchnorm58_1_28_)

        conv59_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_28_")(relu58_1_28_)
        # conv59_1_28_, output shape: {[14,14,1024]}

        batchnorm59_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_28_",)(conv59_1_28_) #TODO: fix_gamma=True

        conv57_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_29_")(relu55_)
        # conv57_1_29_, output shape: {[14,14,16]}

        batchnorm57_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_29_",)(conv57_1_29_) #TODO: fix_gamma=True

        relu57_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_29_")(batchnorm57_1_29_)

        conv58_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_29_")(relu57_1_29_)
        # conv58_1_29_, output shape: {[14,14,16]}

        batchnorm58_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_29_",)(conv58_1_29_) #TODO: fix_gamma=True

        relu58_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_29_")(batchnorm58_1_29_)

        conv59_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_29_")(relu58_1_29_)
        # conv59_1_29_, output shape: {[14,14,1024]}

        batchnorm59_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_29_",)(conv59_1_29_) #TODO: fix_gamma=True

        conv57_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_30_")(relu55_)
        # conv57_1_30_, output shape: {[14,14,16]}

        batchnorm57_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_30_",)(conv57_1_30_) #TODO: fix_gamma=True

        relu57_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_30_")(batchnorm57_1_30_)

        conv58_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_30_")(relu57_1_30_)
        # conv58_1_30_, output shape: {[14,14,16]}

        batchnorm58_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_30_",)(conv58_1_30_) #TODO: fix_gamma=True

        relu58_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_30_")(batchnorm58_1_30_)

        conv59_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_30_")(relu58_1_30_)
        # conv59_1_30_, output shape: {[14,14,1024]}

        batchnorm59_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_30_",)(conv59_1_30_) #TODO: fix_gamma=True

        conv57_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_31_")(relu55_)
        # conv57_1_31_, output shape: {[14,14,16]}

        batchnorm57_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_31_",)(conv57_1_31_) #TODO: fix_gamma=True

        relu57_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_31_")(batchnorm57_1_31_)

        conv58_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_31_")(relu57_1_31_)
        # conv58_1_31_, output shape: {[14,14,16]}

        batchnorm58_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_31_",)(conv58_1_31_) #TODO: fix_gamma=True

        relu58_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_31_")(batchnorm58_1_31_)

        conv59_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_31_")(relu58_1_31_)
        # conv59_1_31_, output shape: {[14,14,1024]}

        batchnorm59_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_31_",)(conv59_1_31_) #TODO: fix_gamma=True

        conv57_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv57_1_32_")(relu55_)
        # conv57_1_32_, output shape: {[14,14,16]}

        batchnorm57_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm57_1_32_",)(conv57_1_32_) #TODO: fix_gamma=True

        relu57_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu57_1_32_")(batchnorm57_1_32_)

        conv58_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv58_1_32_")(relu57_1_32_)
        # conv58_1_32_, output shape: {[14,14,16]}

        batchnorm58_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm58_1_32_",)(conv58_1_32_) #TODO: fix_gamma=True

        relu58_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu58_1_32_")(batchnorm58_1_32_)

        conv59_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv59_1_32_")(relu58_1_32_)
        # conv59_1_32_, output shape: {[14,14,1024]}

        batchnorm59_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm59_1_32_",)(conv59_1_32_) #TODO: fix_gamma=True

        add60_1_ = tf.keras.layers.Add()([batchnorm59_1_1_,  batchnorm59_1_2_,  batchnorm59_1_3_,  batchnorm59_1_4_,  batchnorm59_1_5_,  batchnorm59_1_6_,  batchnorm59_1_7_,  batchnorm59_1_8_,  batchnorm59_1_9_,  batchnorm59_1_10_,  batchnorm59_1_11_,  batchnorm59_1_12_,  batchnorm59_1_13_,  batchnorm59_1_14_,  batchnorm59_1_15_,  batchnorm59_1_16_,  batchnorm59_1_17_,  batchnorm59_1_18_,  batchnorm59_1_19_,  batchnorm59_1_20_,  batchnorm59_1_21_,  batchnorm59_1_22_,  batchnorm59_1_23_,  batchnorm59_1_24_,  batchnorm59_1_25_,  batchnorm59_1_26_,  batchnorm59_1_27_,  batchnorm59_1_28_,  batchnorm59_1_29_,  batchnorm59_1_30_,  batchnorm59_1_31_,  batchnorm59_1_32_])
        # add60_1_, output shape: {[14,14,1024]}

        add61_ = tf.keras.layers.Add()([add60_1_,  relu55_])
        # add61_, output shape: {[14,14,1024]}

        relu61_ = tf.keras.layers.Activation(activation = "relu", name="relu61_")(add61_)

        conv63_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_1_")(relu61_)
        # conv63_1_1_, output shape: {[14,14,16]}

        batchnorm63_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_1_",)(conv63_1_1_) #TODO: fix_gamma=True

        relu63_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_1_")(batchnorm63_1_1_)

        conv64_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_1_")(relu63_1_1_)
        # conv64_1_1_, output shape: {[14,14,16]}

        batchnorm64_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_1_",)(conv64_1_1_) #TODO: fix_gamma=True

        relu64_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_1_")(batchnorm64_1_1_)

        conv65_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_1_")(relu64_1_1_)
        # conv65_1_1_, output shape: {[14,14,1024]}

        batchnorm65_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_1_",)(conv65_1_1_) #TODO: fix_gamma=True

        conv63_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_2_")(relu61_)
        # conv63_1_2_, output shape: {[14,14,16]}

        batchnorm63_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_2_",)(conv63_1_2_) #TODO: fix_gamma=True

        relu63_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_2_")(batchnorm63_1_2_)

        conv64_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_2_")(relu63_1_2_)
        # conv64_1_2_, output shape: {[14,14,16]}

        batchnorm64_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_2_",)(conv64_1_2_) #TODO: fix_gamma=True

        relu64_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_2_")(batchnorm64_1_2_)

        conv65_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_2_")(relu64_1_2_)
        # conv65_1_2_, output shape: {[14,14,1024]}

        batchnorm65_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_2_",)(conv65_1_2_) #TODO: fix_gamma=True

        conv63_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_3_")(relu61_)
        # conv63_1_3_, output shape: {[14,14,16]}

        batchnorm63_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_3_",)(conv63_1_3_) #TODO: fix_gamma=True

        relu63_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_3_")(batchnorm63_1_3_)

        conv64_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_3_")(relu63_1_3_)
        # conv64_1_3_, output shape: {[14,14,16]}

        batchnorm64_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_3_",)(conv64_1_3_) #TODO: fix_gamma=True

        relu64_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_3_")(batchnorm64_1_3_)

        conv65_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_3_")(relu64_1_3_)
        # conv65_1_3_, output shape: {[14,14,1024]}

        batchnorm65_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_3_",)(conv65_1_3_) #TODO: fix_gamma=True

        conv63_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_4_")(relu61_)
        # conv63_1_4_, output shape: {[14,14,16]}

        batchnorm63_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_4_",)(conv63_1_4_) #TODO: fix_gamma=True

        relu63_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_4_")(batchnorm63_1_4_)

        conv64_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_4_")(relu63_1_4_)
        # conv64_1_4_, output shape: {[14,14,16]}

        batchnorm64_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_4_",)(conv64_1_4_) #TODO: fix_gamma=True

        relu64_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_4_")(batchnorm64_1_4_)

        conv65_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_4_")(relu64_1_4_)
        # conv65_1_4_, output shape: {[14,14,1024]}

        batchnorm65_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_4_",)(conv65_1_4_) #TODO: fix_gamma=True

        conv63_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_5_")(relu61_)
        # conv63_1_5_, output shape: {[14,14,16]}

        batchnorm63_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_5_",)(conv63_1_5_) #TODO: fix_gamma=True

        relu63_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_5_")(batchnorm63_1_5_)

        conv64_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_5_")(relu63_1_5_)
        # conv64_1_5_, output shape: {[14,14,16]}

        batchnorm64_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_5_",)(conv64_1_5_) #TODO: fix_gamma=True

        relu64_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_5_")(batchnorm64_1_5_)

        conv65_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_5_")(relu64_1_5_)
        # conv65_1_5_, output shape: {[14,14,1024]}

        batchnorm65_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_5_",)(conv65_1_5_) #TODO: fix_gamma=True

        conv63_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_6_")(relu61_)
        # conv63_1_6_, output shape: {[14,14,16]}

        batchnorm63_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_6_",)(conv63_1_6_) #TODO: fix_gamma=True

        relu63_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_6_")(batchnorm63_1_6_)

        conv64_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_6_")(relu63_1_6_)
        # conv64_1_6_, output shape: {[14,14,16]}

        batchnorm64_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_6_",)(conv64_1_6_) #TODO: fix_gamma=True

        relu64_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_6_")(batchnorm64_1_6_)

        conv65_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_6_")(relu64_1_6_)
        # conv65_1_6_, output shape: {[14,14,1024]}

        batchnorm65_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_6_",)(conv65_1_6_) #TODO: fix_gamma=True

        conv63_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_7_")(relu61_)
        # conv63_1_7_, output shape: {[14,14,16]}

        batchnorm63_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_7_",)(conv63_1_7_) #TODO: fix_gamma=True

        relu63_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_7_")(batchnorm63_1_7_)

        conv64_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_7_")(relu63_1_7_)
        # conv64_1_7_, output shape: {[14,14,16]}

        batchnorm64_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_7_",)(conv64_1_7_) #TODO: fix_gamma=True

        relu64_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_7_")(batchnorm64_1_7_)

        conv65_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_7_")(relu64_1_7_)
        # conv65_1_7_, output shape: {[14,14,1024]}

        batchnorm65_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_7_",)(conv65_1_7_) #TODO: fix_gamma=True

        conv63_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_8_")(relu61_)
        # conv63_1_8_, output shape: {[14,14,16]}

        batchnorm63_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_8_",)(conv63_1_8_) #TODO: fix_gamma=True

        relu63_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_8_")(batchnorm63_1_8_)

        conv64_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_8_")(relu63_1_8_)
        # conv64_1_8_, output shape: {[14,14,16]}

        batchnorm64_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_8_",)(conv64_1_8_) #TODO: fix_gamma=True

        relu64_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_8_")(batchnorm64_1_8_)

        conv65_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_8_")(relu64_1_8_)
        # conv65_1_8_, output shape: {[14,14,1024]}

        batchnorm65_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_8_",)(conv65_1_8_) #TODO: fix_gamma=True

        conv63_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_9_")(relu61_)
        # conv63_1_9_, output shape: {[14,14,16]}

        batchnorm63_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_9_",)(conv63_1_9_) #TODO: fix_gamma=True

        relu63_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_9_")(batchnorm63_1_9_)

        conv64_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_9_")(relu63_1_9_)
        # conv64_1_9_, output shape: {[14,14,16]}

        batchnorm64_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_9_",)(conv64_1_9_) #TODO: fix_gamma=True

        relu64_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_9_")(batchnorm64_1_9_)

        conv65_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_9_")(relu64_1_9_)
        # conv65_1_9_, output shape: {[14,14,1024]}

        batchnorm65_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_9_",)(conv65_1_9_) #TODO: fix_gamma=True

        conv63_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_10_")(relu61_)
        # conv63_1_10_, output shape: {[14,14,16]}

        batchnorm63_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_10_",)(conv63_1_10_) #TODO: fix_gamma=True

        relu63_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_10_")(batchnorm63_1_10_)

        conv64_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_10_")(relu63_1_10_)
        # conv64_1_10_, output shape: {[14,14,16]}

        batchnorm64_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_10_",)(conv64_1_10_) #TODO: fix_gamma=True

        relu64_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_10_")(batchnorm64_1_10_)

        conv65_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_10_")(relu64_1_10_)
        # conv65_1_10_, output shape: {[14,14,1024]}

        batchnorm65_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_10_",)(conv65_1_10_) #TODO: fix_gamma=True

        conv63_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_11_")(relu61_)
        # conv63_1_11_, output shape: {[14,14,16]}

        batchnorm63_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_11_",)(conv63_1_11_) #TODO: fix_gamma=True

        relu63_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_11_")(batchnorm63_1_11_)

        conv64_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_11_")(relu63_1_11_)
        # conv64_1_11_, output shape: {[14,14,16]}

        batchnorm64_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_11_",)(conv64_1_11_) #TODO: fix_gamma=True

        relu64_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_11_")(batchnorm64_1_11_)

        conv65_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_11_")(relu64_1_11_)
        # conv65_1_11_, output shape: {[14,14,1024]}

        batchnorm65_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_11_",)(conv65_1_11_) #TODO: fix_gamma=True

        conv63_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_12_")(relu61_)
        # conv63_1_12_, output shape: {[14,14,16]}

        batchnorm63_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_12_",)(conv63_1_12_) #TODO: fix_gamma=True

        relu63_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_12_")(batchnorm63_1_12_)

        conv64_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_12_")(relu63_1_12_)
        # conv64_1_12_, output shape: {[14,14,16]}

        batchnorm64_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_12_",)(conv64_1_12_) #TODO: fix_gamma=True

        relu64_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_12_")(batchnorm64_1_12_)

        conv65_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_12_")(relu64_1_12_)
        # conv65_1_12_, output shape: {[14,14,1024]}

        batchnorm65_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_12_",)(conv65_1_12_) #TODO: fix_gamma=True

        conv63_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_13_")(relu61_)
        # conv63_1_13_, output shape: {[14,14,16]}

        batchnorm63_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_13_",)(conv63_1_13_) #TODO: fix_gamma=True

        relu63_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_13_")(batchnorm63_1_13_)

        conv64_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_13_")(relu63_1_13_)
        # conv64_1_13_, output shape: {[14,14,16]}

        batchnorm64_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_13_",)(conv64_1_13_) #TODO: fix_gamma=True

        relu64_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_13_")(batchnorm64_1_13_)

        conv65_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_13_")(relu64_1_13_)
        # conv65_1_13_, output shape: {[14,14,1024]}

        batchnorm65_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_13_",)(conv65_1_13_) #TODO: fix_gamma=True

        conv63_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_14_")(relu61_)
        # conv63_1_14_, output shape: {[14,14,16]}

        batchnorm63_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_14_",)(conv63_1_14_) #TODO: fix_gamma=True

        relu63_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_14_")(batchnorm63_1_14_)

        conv64_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_14_")(relu63_1_14_)
        # conv64_1_14_, output shape: {[14,14,16]}

        batchnorm64_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_14_",)(conv64_1_14_) #TODO: fix_gamma=True

        relu64_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_14_")(batchnorm64_1_14_)

        conv65_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_14_")(relu64_1_14_)
        # conv65_1_14_, output shape: {[14,14,1024]}

        batchnorm65_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_14_",)(conv65_1_14_) #TODO: fix_gamma=True

        conv63_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_15_")(relu61_)
        # conv63_1_15_, output shape: {[14,14,16]}

        batchnorm63_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_15_",)(conv63_1_15_) #TODO: fix_gamma=True

        relu63_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_15_")(batchnorm63_1_15_)

        conv64_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_15_")(relu63_1_15_)
        # conv64_1_15_, output shape: {[14,14,16]}

        batchnorm64_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_15_",)(conv64_1_15_) #TODO: fix_gamma=True

        relu64_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_15_")(batchnorm64_1_15_)

        conv65_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_15_")(relu64_1_15_)
        # conv65_1_15_, output shape: {[14,14,1024]}

        batchnorm65_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_15_",)(conv65_1_15_) #TODO: fix_gamma=True

        conv63_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_16_")(relu61_)
        # conv63_1_16_, output shape: {[14,14,16]}

        batchnorm63_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_16_",)(conv63_1_16_) #TODO: fix_gamma=True

        relu63_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_16_")(batchnorm63_1_16_)

        conv64_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_16_")(relu63_1_16_)
        # conv64_1_16_, output shape: {[14,14,16]}

        batchnorm64_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_16_",)(conv64_1_16_) #TODO: fix_gamma=True

        relu64_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_16_")(batchnorm64_1_16_)

        conv65_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_16_")(relu64_1_16_)
        # conv65_1_16_, output shape: {[14,14,1024]}

        batchnorm65_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_16_",)(conv65_1_16_) #TODO: fix_gamma=True

        conv63_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_17_")(relu61_)
        # conv63_1_17_, output shape: {[14,14,16]}

        batchnorm63_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_17_",)(conv63_1_17_) #TODO: fix_gamma=True

        relu63_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_17_")(batchnorm63_1_17_)

        conv64_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_17_")(relu63_1_17_)
        # conv64_1_17_, output shape: {[14,14,16]}

        batchnorm64_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_17_",)(conv64_1_17_) #TODO: fix_gamma=True

        relu64_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_17_")(batchnorm64_1_17_)

        conv65_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_17_")(relu64_1_17_)
        # conv65_1_17_, output shape: {[14,14,1024]}

        batchnorm65_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_17_",)(conv65_1_17_) #TODO: fix_gamma=True

        conv63_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_18_")(relu61_)
        # conv63_1_18_, output shape: {[14,14,16]}

        batchnorm63_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_18_",)(conv63_1_18_) #TODO: fix_gamma=True

        relu63_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_18_")(batchnorm63_1_18_)

        conv64_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_18_")(relu63_1_18_)
        # conv64_1_18_, output shape: {[14,14,16]}

        batchnorm64_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_18_",)(conv64_1_18_) #TODO: fix_gamma=True

        relu64_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_18_")(batchnorm64_1_18_)

        conv65_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_18_")(relu64_1_18_)
        # conv65_1_18_, output shape: {[14,14,1024]}

        batchnorm65_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_18_",)(conv65_1_18_) #TODO: fix_gamma=True

        conv63_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_19_")(relu61_)
        # conv63_1_19_, output shape: {[14,14,16]}

        batchnorm63_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_19_",)(conv63_1_19_) #TODO: fix_gamma=True

        relu63_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_19_")(batchnorm63_1_19_)

        conv64_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_19_")(relu63_1_19_)
        # conv64_1_19_, output shape: {[14,14,16]}

        batchnorm64_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_19_",)(conv64_1_19_) #TODO: fix_gamma=True

        relu64_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_19_")(batchnorm64_1_19_)

        conv65_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_19_")(relu64_1_19_)
        # conv65_1_19_, output shape: {[14,14,1024]}

        batchnorm65_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_19_",)(conv65_1_19_) #TODO: fix_gamma=True

        conv63_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_20_")(relu61_)
        # conv63_1_20_, output shape: {[14,14,16]}

        batchnorm63_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_20_",)(conv63_1_20_) #TODO: fix_gamma=True

        relu63_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_20_")(batchnorm63_1_20_)

        conv64_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_20_")(relu63_1_20_)
        # conv64_1_20_, output shape: {[14,14,16]}

        batchnorm64_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_20_",)(conv64_1_20_) #TODO: fix_gamma=True

        relu64_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_20_")(batchnorm64_1_20_)

        conv65_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_20_")(relu64_1_20_)
        # conv65_1_20_, output shape: {[14,14,1024]}

        batchnorm65_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_20_",)(conv65_1_20_) #TODO: fix_gamma=True

        conv63_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_21_")(relu61_)
        # conv63_1_21_, output shape: {[14,14,16]}

        batchnorm63_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_21_",)(conv63_1_21_) #TODO: fix_gamma=True

        relu63_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_21_")(batchnorm63_1_21_)

        conv64_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_21_")(relu63_1_21_)
        # conv64_1_21_, output shape: {[14,14,16]}

        batchnorm64_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_21_",)(conv64_1_21_) #TODO: fix_gamma=True

        relu64_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_21_")(batchnorm64_1_21_)

        conv65_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_21_")(relu64_1_21_)
        # conv65_1_21_, output shape: {[14,14,1024]}

        batchnorm65_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_21_",)(conv65_1_21_) #TODO: fix_gamma=True

        conv63_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_22_")(relu61_)
        # conv63_1_22_, output shape: {[14,14,16]}

        batchnorm63_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_22_",)(conv63_1_22_) #TODO: fix_gamma=True

        relu63_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_22_")(batchnorm63_1_22_)

        conv64_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_22_")(relu63_1_22_)
        # conv64_1_22_, output shape: {[14,14,16]}

        batchnorm64_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_22_",)(conv64_1_22_) #TODO: fix_gamma=True

        relu64_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_22_")(batchnorm64_1_22_)

        conv65_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_22_")(relu64_1_22_)
        # conv65_1_22_, output shape: {[14,14,1024]}

        batchnorm65_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_22_",)(conv65_1_22_) #TODO: fix_gamma=True

        conv63_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_23_")(relu61_)
        # conv63_1_23_, output shape: {[14,14,16]}

        batchnorm63_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_23_",)(conv63_1_23_) #TODO: fix_gamma=True

        relu63_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_23_")(batchnorm63_1_23_)

        conv64_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_23_")(relu63_1_23_)
        # conv64_1_23_, output shape: {[14,14,16]}

        batchnorm64_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_23_",)(conv64_1_23_) #TODO: fix_gamma=True

        relu64_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_23_")(batchnorm64_1_23_)

        conv65_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_23_")(relu64_1_23_)
        # conv65_1_23_, output shape: {[14,14,1024]}

        batchnorm65_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_23_",)(conv65_1_23_) #TODO: fix_gamma=True

        conv63_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_24_")(relu61_)
        # conv63_1_24_, output shape: {[14,14,16]}

        batchnorm63_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_24_",)(conv63_1_24_) #TODO: fix_gamma=True

        relu63_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_24_")(batchnorm63_1_24_)

        conv64_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_24_")(relu63_1_24_)
        # conv64_1_24_, output shape: {[14,14,16]}

        batchnorm64_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_24_",)(conv64_1_24_) #TODO: fix_gamma=True

        relu64_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_24_")(batchnorm64_1_24_)

        conv65_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_24_")(relu64_1_24_)
        # conv65_1_24_, output shape: {[14,14,1024]}

        batchnorm65_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_24_",)(conv65_1_24_) #TODO: fix_gamma=True

        conv63_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_25_")(relu61_)
        # conv63_1_25_, output shape: {[14,14,16]}

        batchnorm63_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_25_",)(conv63_1_25_) #TODO: fix_gamma=True

        relu63_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_25_")(batchnorm63_1_25_)

        conv64_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_25_")(relu63_1_25_)
        # conv64_1_25_, output shape: {[14,14,16]}

        batchnorm64_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_25_",)(conv64_1_25_) #TODO: fix_gamma=True

        relu64_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_25_")(batchnorm64_1_25_)

        conv65_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_25_")(relu64_1_25_)
        # conv65_1_25_, output shape: {[14,14,1024]}

        batchnorm65_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_25_",)(conv65_1_25_) #TODO: fix_gamma=True

        conv63_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_26_")(relu61_)
        # conv63_1_26_, output shape: {[14,14,16]}

        batchnorm63_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_26_",)(conv63_1_26_) #TODO: fix_gamma=True

        relu63_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_26_")(batchnorm63_1_26_)

        conv64_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_26_")(relu63_1_26_)
        # conv64_1_26_, output shape: {[14,14,16]}

        batchnorm64_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_26_",)(conv64_1_26_) #TODO: fix_gamma=True

        relu64_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_26_")(batchnorm64_1_26_)

        conv65_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_26_")(relu64_1_26_)
        # conv65_1_26_, output shape: {[14,14,1024]}

        batchnorm65_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_26_",)(conv65_1_26_) #TODO: fix_gamma=True

        conv63_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_27_")(relu61_)
        # conv63_1_27_, output shape: {[14,14,16]}

        batchnorm63_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_27_",)(conv63_1_27_) #TODO: fix_gamma=True

        relu63_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_27_")(batchnorm63_1_27_)

        conv64_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_27_")(relu63_1_27_)
        # conv64_1_27_, output shape: {[14,14,16]}

        batchnorm64_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_27_",)(conv64_1_27_) #TODO: fix_gamma=True

        relu64_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_27_")(batchnorm64_1_27_)

        conv65_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_27_")(relu64_1_27_)
        # conv65_1_27_, output shape: {[14,14,1024]}

        batchnorm65_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_27_",)(conv65_1_27_) #TODO: fix_gamma=True

        conv63_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_28_")(relu61_)
        # conv63_1_28_, output shape: {[14,14,16]}

        batchnorm63_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_28_",)(conv63_1_28_) #TODO: fix_gamma=True

        relu63_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_28_")(batchnorm63_1_28_)

        conv64_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_28_")(relu63_1_28_)
        # conv64_1_28_, output shape: {[14,14,16]}

        batchnorm64_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_28_",)(conv64_1_28_) #TODO: fix_gamma=True

        relu64_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_28_")(batchnorm64_1_28_)

        conv65_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_28_")(relu64_1_28_)
        # conv65_1_28_, output shape: {[14,14,1024]}

        batchnorm65_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_28_",)(conv65_1_28_) #TODO: fix_gamma=True

        conv63_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_29_")(relu61_)
        # conv63_1_29_, output shape: {[14,14,16]}

        batchnorm63_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_29_",)(conv63_1_29_) #TODO: fix_gamma=True

        relu63_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_29_")(batchnorm63_1_29_)

        conv64_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_29_")(relu63_1_29_)
        # conv64_1_29_, output shape: {[14,14,16]}

        batchnorm64_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_29_",)(conv64_1_29_) #TODO: fix_gamma=True

        relu64_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_29_")(batchnorm64_1_29_)

        conv65_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_29_")(relu64_1_29_)
        # conv65_1_29_, output shape: {[14,14,1024]}

        batchnorm65_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_29_",)(conv65_1_29_) #TODO: fix_gamma=True

        conv63_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_30_")(relu61_)
        # conv63_1_30_, output shape: {[14,14,16]}

        batchnorm63_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_30_",)(conv63_1_30_) #TODO: fix_gamma=True

        relu63_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_30_")(batchnorm63_1_30_)

        conv64_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_30_")(relu63_1_30_)
        # conv64_1_30_, output shape: {[14,14,16]}

        batchnorm64_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_30_",)(conv64_1_30_) #TODO: fix_gamma=True

        relu64_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_30_")(batchnorm64_1_30_)

        conv65_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_30_")(relu64_1_30_)
        # conv65_1_30_, output shape: {[14,14,1024]}

        batchnorm65_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_30_",)(conv65_1_30_) #TODO: fix_gamma=True

        conv63_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_31_")(relu61_)
        # conv63_1_31_, output shape: {[14,14,16]}

        batchnorm63_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_31_",)(conv63_1_31_) #TODO: fix_gamma=True

        relu63_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_31_")(batchnorm63_1_31_)

        conv64_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_31_")(relu63_1_31_)
        # conv64_1_31_, output shape: {[14,14,16]}

        batchnorm64_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_31_",)(conv64_1_31_) #TODO: fix_gamma=True

        relu64_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_31_")(batchnorm64_1_31_)

        conv65_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_31_")(relu64_1_31_)
        # conv65_1_31_, output shape: {[14,14,1024]}

        batchnorm65_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_31_",)(conv65_1_31_) #TODO: fix_gamma=True

        conv63_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv63_1_32_")(relu61_)
        # conv63_1_32_, output shape: {[14,14,16]}

        batchnorm63_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm63_1_32_",)(conv63_1_32_) #TODO: fix_gamma=True

        relu63_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu63_1_32_")(batchnorm63_1_32_)

        conv64_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv64_1_32_")(relu63_1_32_)
        # conv64_1_32_, output shape: {[14,14,16]}

        batchnorm64_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm64_1_32_",)(conv64_1_32_) #TODO: fix_gamma=True

        relu64_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu64_1_32_")(batchnorm64_1_32_)

        conv65_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv65_1_32_")(relu64_1_32_)
        # conv65_1_32_, output shape: {[14,14,1024]}

        batchnorm65_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm65_1_32_",)(conv65_1_32_) #TODO: fix_gamma=True

        add66_1_ = tf.keras.layers.Add()([batchnorm65_1_1_,  batchnorm65_1_2_,  batchnorm65_1_3_,  batchnorm65_1_4_,  batchnorm65_1_5_,  batchnorm65_1_6_,  batchnorm65_1_7_,  batchnorm65_1_8_,  batchnorm65_1_9_,  batchnorm65_1_10_,  batchnorm65_1_11_,  batchnorm65_1_12_,  batchnorm65_1_13_,  batchnorm65_1_14_,  batchnorm65_1_15_,  batchnorm65_1_16_,  batchnorm65_1_17_,  batchnorm65_1_18_,  batchnorm65_1_19_,  batchnorm65_1_20_,  batchnorm65_1_21_,  batchnorm65_1_22_,  batchnorm65_1_23_,  batchnorm65_1_24_,  batchnorm65_1_25_,  batchnorm65_1_26_,  batchnorm65_1_27_,  batchnorm65_1_28_,  batchnorm65_1_29_,  batchnorm65_1_30_,  batchnorm65_1_31_,  batchnorm65_1_32_])
        # add66_1_, output shape: {[14,14,1024]}

        add67_ = tf.keras.layers.Add()([add66_1_,  relu61_])
        # add67_, output shape: {[14,14,1024]}

        relu67_ = tf.keras.layers.Activation(activation = "relu", name="relu67_")(add67_)

        conv69_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_1_")(relu67_)
        # conv69_1_1_, output shape: {[14,14,16]}

        batchnorm69_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_1_",)(conv69_1_1_) #TODO: fix_gamma=True

        relu69_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_1_")(batchnorm69_1_1_)

        conv70_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_1_")(relu69_1_1_)
        # conv70_1_1_, output shape: {[14,14,16]}

        batchnorm70_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_1_",)(conv70_1_1_) #TODO: fix_gamma=True

        relu70_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_1_")(batchnorm70_1_1_)

        conv71_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_1_")(relu70_1_1_)
        # conv71_1_1_, output shape: {[14,14,1024]}

        batchnorm71_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_1_",)(conv71_1_1_) #TODO: fix_gamma=True

        conv69_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_2_")(relu67_)
        # conv69_1_2_, output shape: {[14,14,16]}

        batchnorm69_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_2_",)(conv69_1_2_) #TODO: fix_gamma=True

        relu69_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_2_")(batchnorm69_1_2_)

        conv70_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_2_")(relu69_1_2_)
        # conv70_1_2_, output shape: {[14,14,16]}

        batchnorm70_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_2_",)(conv70_1_2_) #TODO: fix_gamma=True

        relu70_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_2_")(batchnorm70_1_2_)

        conv71_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_2_")(relu70_1_2_)
        # conv71_1_2_, output shape: {[14,14,1024]}

        batchnorm71_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_2_",)(conv71_1_2_) #TODO: fix_gamma=True

        conv69_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_3_")(relu67_)
        # conv69_1_3_, output shape: {[14,14,16]}

        batchnorm69_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_3_",)(conv69_1_3_) #TODO: fix_gamma=True

        relu69_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_3_")(batchnorm69_1_3_)

        conv70_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_3_")(relu69_1_3_)
        # conv70_1_3_, output shape: {[14,14,16]}

        batchnorm70_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_3_",)(conv70_1_3_) #TODO: fix_gamma=True

        relu70_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_3_")(batchnorm70_1_3_)

        conv71_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_3_")(relu70_1_3_)
        # conv71_1_3_, output shape: {[14,14,1024]}

        batchnorm71_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_3_",)(conv71_1_3_) #TODO: fix_gamma=True

        conv69_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_4_")(relu67_)
        # conv69_1_4_, output shape: {[14,14,16]}

        batchnorm69_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_4_",)(conv69_1_4_) #TODO: fix_gamma=True

        relu69_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_4_")(batchnorm69_1_4_)

        conv70_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_4_")(relu69_1_4_)
        # conv70_1_4_, output shape: {[14,14,16]}

        batchnorm70_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_4_",)(conv70_1_4_) #TODO: fix_gamma=True

        relu70_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_4_")(batchnorm70_1_4_)

        conv71_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_4_")(relu70_1_4_)
        # conv71_1_4_, output shape: {[14,14,1024]}

        batchnorm71_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_4_",)(conv71_1_4_) #TODO: fix_gamma=True

        conv69_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_5_")(relu67_)
        # conv69_1_5_, output shape: {[14,14,16]}

        batchnorm69_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_5_",)(conv69_1_5_) #TODO: fix_gamma=True

        relu69_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_5_")(batchnorm69_1_5_)

        conv70_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_5_")(relu69_1_5_)
        # conv70_1_5_, output shape: {[14,14,16]}

        batchnorm70_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_5_",)(conv70_1_5_) #TODO: fix_gamma=True

        relu70_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_5_")(batchnorm70_1_5_)

        conv71_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_5_")(relu70_1_5_)
        # conv71_1_5_, output shape: {[14,14,1024]}

        batchnorm71_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_5_",)(conv71_1_5_) #TODO: fix_gamma=True

        conv69_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_6_")(relu67_)
        # conv69_1_6_, output shape: {[14,14,16]}

        batchnorm69_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_6_",)(conv69_1_6_) #TODO: fix_gamma=True

        relu69_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_6_")(batchnorm69_1_6_)

        conv70_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_6_")(relu69_1_6_)
        # conv70_1_6_, output shape: {[14,14,16]}

        batchnorm70_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_6_",)(conv70_1_6_) #TODO: fix_gamma=True

        relu70_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_6_")(batchnorm70_1_6_)

        conv71_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_6_")(relu70_1_6_)
        # conv71_1_6_, output shape: {[14,14,1024]}

        batchnorm71_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_6_",)(conv71_1_6_) #TODO: fix_gamma=True

        conv69_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_7_")(relu67_)
        # conv69_1_7_, output shape: {[14,14,16]}

        batchnorm69_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_7_",)(conv69_1_7_) #TODO: fix_gamma=True

        relu69_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_7_")(batchnorm69_1_7_)

        conv70_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_7_")(relu69_1_7_)
        # conv70_1_7_, output shape: {[14,14,16]}

        batchnorm70_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_7_",)(conv70_1_7_) #TODO: fix_gamma=True

        relu70_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_7_")(batchnorm70_1_7_)

        conv71_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_7_")(relu70_1_7_)
        # conv71_1_7_, output shape: {[14,14,1024]}

        batchnorm71_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_7_",)(conv71_1_7_) #TODO: fix_gamma=True

        conv69_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_8_")(relu67_)
        # conv69_1_8_, output shape: {[14,14,16]}

        batchnorm69_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_8_",)(conv69_1_8_) #TODO: fix_gamma=True

        relu69_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_8_")(batchnorm69_1_8_)

        conv70_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_8_")(relu69_1_8_)
        # conv70_1_8_, output shape: {[14,14,16]}

        batchnorm70_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_8_",)(conv70_1_8_) #TODO: fix_gamma=True

        relu70_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_8_")(batchnorm70_1_8_)

        conv71_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_8_")(relu70_1_8_)
        # conv71_1_8_, output shape: {[14,14,1024]}

        batchnorm71_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_8_",)(conv71_1_8_) #TODO: fix_gamma=True

        conv69_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_9_")(relu67_)
        # conv69_1_9_, output shape: {[14,14,16]}

        batchnorm69_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_9_",)(conv69_1_9_) #TODO: fix_gamma=True

        relu69_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_9_")(batchnorm69_1_9_)

        conv70_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_9_")(relu69_1_9_)
        # conv70_1_9_, output shape: {[14,14,16]}

        batchnorm70_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_9_",)(conv70_1_9_) #TODO: fix_gamma=True

        relu70_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_9_")(batchnorm70_1_9_)

        conv71_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_9_")(relu70_1_9_)
        # conv71_1_9_, output shape: {[14,14,1024]}

        batchnorm71_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_9_",)(conv71_1_9_) #TODO: fix_gamma=True

        conv69_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_10_")(relu67_)
        # conv69_1_10_, output shape: {[14,14,16]}

        batchnorm69_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_10_",)(conv69_1_10_) #TODO: fix_gamma=True

        relu69_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_10_")(batchnorm69_1_10_)

        conv70_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_10_")(relu69_1_10_)
        # conv70_1_10_, output shape: {[14,14,16]}

        batchnorm70_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_10_",)(conv70_1_10_) #TODO: fix_gamma=True

        relu70_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_10_")(batchnorm70_1_10_)

        conv71_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_10_")(relu70_1_10_)
        # conv71_1_10_, output shape: {[14,14,1024]}

        batchnorm71_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_10_",)(conv71_1_10_) #TODO: fix_gamma=True

        conv69_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_11_")(relu67_)
        # conv69_1_11_, output shape: {[14,14,16]}

        batchnorm69_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_11_",)(conv69_1_11_) #TODO: fix_gamma=True

        relu69_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_11_")(batchnorm69_1_11_)

        conv70_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_11_")(relu69_1_11_)
        # conv70_1_11_, output shape: {[14,14,16]}

        batchnorm70_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_11_",)(conv70_1_11_) #TODO: fix_gamma=True

        relu70_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_11_")(batchnorm70_1_11_)

        conv71_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_11_")(relu70_1_11_)
        # conv71_1_11_, output shape: {[14,14,1024]}

        batchnorm71_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_11_",)(conv71_1_11_) #TODO: fix_gamma=True

        conv69_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_12_")(relu67_)
        # conv69_1_12_, output shape: {[14,14,16]}

        batchnorm69_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_12_",)(conv69_1_12_) #TODO: fix_gamma=True

        relu69_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_12_")(batchnorm69_1_12_)

        conv70_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_12_")(relu69_1_12_)
        # conv70_1_12_, output shape: {[14,14,16]}

        batchnorm70_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_12_",)(conv70_1_12_) #TODO: fix_gamma=True

        relu70_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_12_")(batchnorm70_1_12_)

        conv71_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_12_")(relu70_1_12_)
        # conv71_1_12_, output shape: {[14,14,1024]}

        batchnorm71_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_12_",)(conv71_1_12_) #TODO: fix_gamma=True

        conv69_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_13_")(relu67_)
        # conv69_1_13_, output shape: {[14,14,16]}

        batchnorm69_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_13_",)(conv69_1_13_) #TODO: fix_gamma=True

        relu69_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_13_")(batchnorm69_1_13_)

        conv70_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_13_")(relu69_1_13_)
        # conv70_1_13_, output shape: {[14,14,16]}

        batchnorm70_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_13_",)(conv70_1_13_) #TODO: fix_gamma=True

        relu70_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_13_")(batchnorm70_1_13_)

        conv71_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_13_")(relu70_1_13_)
        # conv71_1_13_, output shape: {[14,14,1024]}

        batchnorm71_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_13_",)(conv71_1_13_) #TODO: fix_gamma=True

        conv69_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_14_")(relu67_)
        # conv69_1_14_, output shape: {[14,14,16]}

        batchnorm69_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_14_",)(conv69_1_14_) #TODO: fix_gamma=True

        relu69_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_14_")(batchnorm69_1_14_)

        conv70_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_14_")(relu69_1_14_)
        # conv70_1_14_, output shape: {[14,14,16]}

        batchnorm70_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_14_",)(conv70_1_14_) #TODO: fix_gamma=True

        relu70_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_14_")(batchnorm70_1_14_)

        conv71_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_14_")(relu70_1_14_)
        # conv71_1_14_, output shape: {[14,14,1024]}

        batchnorm71_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_14_",)(conv71_1_14_) #TODO: fix_gamma=True

        conv69_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_15_")(relu67_)
        # conv69_1_15_, output shape: {[14,14,16]}

        batchnorm69_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_15_",)(conv69_1_15_) #TODO: fix_gamma=True

        relu69_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_15_")(batchnorm69_1_15_)

        conv70_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_15_")(relu69_1_15_)
        # conv70_1_15_, output shape: {[14,14,16]}

        batchnorm70_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_15_",)(conv70_1_15_) #TODO: fix_gamma=True

        relu70_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_15_")(batchnorm70_1_15_)

        conv71_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_15_")(relu70_1_15_)
        # conv71_1_15_, output shape: {[14,14,1024]}

        batchnorm71_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_15_",)(conv71_1_15_) #TODO: fix_gamma=True

        conv69_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_16_")(relu67_)
        # conv69_1_16_, output shape: {[14,14,16]}

        batchnorm69_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_16_",)(conv69_1_16_) #TODO: fix_gamma=True

        relu69_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_16_")(batchnorm69_1_16_)

        conv70_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_16_")(relu69_1_16_)
        # conv70_1_16_, output shape: {[14,14,16]}

        batchnorm70_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_16_",)(conv70_1_16_) #TODO: fix_gamma=True

        relu70_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_16_")(batchnorm70_1_16_)

        conv71_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_16_")(relu70_1_16_)
        # conv71_1_16_, output shape: {[14,14,1024]}

        batchnorm71_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_16_",)(conv71_1_16_) #TODO: fix_gamma=True

        conv69_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_17_")(relu67_)
        # conv69_1_17_, output shape: {[14,14,16]}

        batchnorm69_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_17_",)(conv69_1_17_) #TODO: fix_gamma=True

        relu69_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_17_")(batchnorm69_1_17_)

        conv70_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_17_")(relu69_1_17_)
        # conv70_1_17_, output shape: {[14,14,16]}

        batchnorm70_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_17_",)(conv70_1_17_) #TODO: fix_gamma=True

        relu70_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_17_")(batchnorm70_1_17_)

        conv71_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_17_")(relu70_1_17_)
        # conv71_1_17_, output shape: {[14,14,1024]}

        batchnorm71_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_17_",)(conv71_1_17_) #TODO: fix_gamma=True

        conv69_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_18_")(relu67_)
        # conv69_1_18_, output shape: {[14,14,16]}

        batchnorm69_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_18_",)(conv69_1_18_) #TODO: fix_gamma=True

        relu69_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_18_")(batchnorm69_1_18_)

        conv70_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_18_")(relu69_1_18_)
        # conv70_1_18_, output shape: {[14,14,16]}

        batchnorm70_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_18_",)(conv70_1_18_) #TODO: fix_gamma=True

        relu70_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_18_")(batchnorm70_1_18_)

        conv71_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_18_")(relu70_1_18_)
        # conv71_1_18_, output shape: {[14,14,1024]}

        batchnorm71_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_18_",)(conv71_1_18_) #TODO: fix_gamma=True

        conv69_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_19_")(relu67_)
        # conv69_1_19_, output shape: {[14,14,16]}

        batchnorm69_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_19_",)(conv69_1_19_) #TODO: fix_gamma=True

        relu69_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_19_")(batchnorm69_1_19_)

        conv70_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_19_")(relu69_1_19_)
        # conv70_1_19_, output shape: {[14,14,16]}

        batchnorm70_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_19_",)(conv70_1_19_) #TODO: fix_gamma=True

        relu70_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_19_")(batchnorm70_1_19_)

        conv71_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_19_")(relu70_1_19_)
        # conv71_1_19_, output shape: {[14,14,1024]}

        batchnorm71_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_19_",)(conv71_1_19_) #TODO: fix_gamma=True

        conv69_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_20_")(relu67_)
        # conv69_1_20_, output shape: {[14,14,16]}

        batchnorm69_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_20_",)(conv69_1_20_) #TODO: fix_gamma=True

        relu69_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_20_")(batchnorm69_1_20_)

        conv70_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_20_")(relu69_1_20_)
        # conv70_1_20_, output shape: {[14,14,16]}

        batchnorm70_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_20_",)(conv70_1_20_) #TODO: fix_gamma=True

        relu70_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_20_")(batchnorm70_1_20_)

        conv71_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_20_")(relu70_1_20_)
        # conv71_1_20_, output shape: {[14,14,1024]}

        batchnorm71_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_20_",)(conv71_1_20_) #TODO: fix_gamma=True

        conv69_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_21_")(relu67_)
        # conv69_1_21_, output shape: {[14,14,16]}

        batchnorm69_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_21_",)(conv69_1_21_) #TODO: fix_gamma=True

        relu69_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_21_")(batchnorm69_1_21_)

        conv70_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_21_")(relu69_1_21_)
        # conv70_1_21_, output shape: {[14,14,16]}

        batchnorm70_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_21_",)(conv70_1_21_) #TODO: fix_gamma=True

        relu70_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_21_")(batchnorm70_1_21_)

        conv71_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_21_")(relu70_1_21_)
        # conv71_1_21_, output shape: {[14,14,1024]}

        batchnorm71_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_21_",)(conv71_1_21_) #TODO: fix_gamma=True

        conv69_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_22_")(relu67_)
        # conv69_1_22_, output shape: {[14,14,16]}

        batchnorm69_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_22_",)(conv69_1_22_) #TODO: fix_gamma=True

        relu69_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_22_")(batchnorm69_1_22_)

        conv70_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_22_")(relu69_1_22_)
        # conv70_1_22_, output shape: {[14,14,16]}

        batchnorm70_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_22_",)(conv70_1_22_) #TODO: fix_gamma=True

        relu70_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_22_")(batchnorm70_1_22_)

        conv71_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_22_")(relu70_1_22_)
        # conv71_1_22_, output shape: {[14,14,1024]}

        batchnorm71_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_22_",)(conv71_1_22_) #TODO: fix_gamma=True

        conv69_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_23_")(relu67_)
        # conv69_1_23_, output shape: {[14,14,16]}

        batchnorm69_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_23_",)(conv69_1_23_) #TODO: fix_gamma=True

        relu69_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_23_")(batchnorm69_1_23_)

        conv70_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_23_")(relu69_1_23_)
        # conv70_1_23_, output shape: {[14,14,16]}

        batchnorm70_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_23_",)(conv70_1_23_) #TODO: fix_gamma=True

        relu70_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_23_")(batchnorm70_1_23_)

        conv71_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_23_")(relu70_1_23_)
        # conv71_1_23_, output shape: {[14,14,1024]}

        batchnorm71_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_23_",)(conv71_1_23_) #TODO: fix_gamma=True

        conv69_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_24_")(relu67_)
        # conv69_1_24_, output shape: {[14,14,16]}

        batchnorm69_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_24_",)(conv69_1_24_) #TODO: fix_gamma=True

        relu69_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_24_")(batchnorm69_1_24_)

        conv70_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_24_")(relu69_1_24_)
        # conv70_1_24_, output shape: {[14,14,16]}

        batchnorm70_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_24_",)(conv70_1_24_) #TODO: fix_gamma=True

        relu70_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_24_")(batchnorm70_1_24_)

        conv71_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_24_")(relu70_1_24_)
        # conv71_1_24_, output shape: {[14,14,1024]}

        batchnorm71_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_24_",)(conv71_1_24_) #TODO: fix_gamma=True

        conv69_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_25_")(relu67_)
        # conv69_1_25_, output shape: {[14,14,16]}

        batchnorm69_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_25_",)(conv69_1_25_) #TODO: fix_gamma=True

        relu69_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_25_")(batchnorm69_1_25_)

        conv70_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_25_")(relu69_1_25_)
        # conv70_1_25_, output shape: {[14,14,16]}

        batchnorm70_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_25_",)(conv70_1_25_) #TODO: fix_gamma=True

        relu70_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_25_")(batchnorm70_1_25_)

        conv71_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_25_")(relu70_1_25_)
        # conv71_1_25_, output shape: {[14,14,1024]}

        batchnorm71_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_25_",)(conv71_1_25_) #TODO: fix_gamma=True

        conv69_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_26_")(relu67_)
        # conv69_1_26_, output shape: {[14,14,16]}

        batchnorm69_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_26_",)(conv69_1_26_) #TODO: fix_gamma=True

        relu69_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_26_")(batchnorm69_1_26_)

        conv70_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_26_")(relu69_1_26_)
        # conv70_1_26_, output shape: {[14,14,16]}

        batchnorm70_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_26_",)(conv70_1_26_) #TODO: fix_gamma=True

        relu70_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_26_")(batchnorm70_1_26_)

        conv71_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_26_")(relu70_1_26_)
        # conv71_1_26_, output shape: {[14,14,1024]}

        batchnorm71_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_26_",)(conv71_1_26_) #TODO: fix_gamma=True

        conv69_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_27_")(relu67_)
        # conv69_1_27_, output shape: {[14,14,16]}

        batchnorm69_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_27_",)(conv69_1_27_) #TODO: fix_gamma=True

        relu69_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_27_")(batchnorm69_1_27_)

        conv70_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_27_")(relu69_1_27_)
        # conv70_1_27_, output shape: {[14,14,16]}

        batchnorm70_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_27_",)(conv70_1_27_) #TODO: fix_gamma=True

        relu70_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_27_")(batchnorm70_1_27_)

        conv71_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_27_")(relu70_1_27_)
        # conv71_1_27_, output shape: {[14,14,1024]}

        batchnorm71_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_27_",)(conv71_1_27_) #TODO: fix_gamma=True

        conv69_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_28_")(relu67_)
        # conv69_1_28_, output shape: {[14,14,16]}

        batchnorm69_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_28_",)(conv69_1_28_) #TODO: fix_gamma=True

        relu69_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_28_")(batchnorm69_1_28_)

        conv70_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_28_")(relu69_1_28_)
        # conv70_1_28_, output shape: {[14,14,16]}

        batchnorm70_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_28_",)(conv70_1_28_) #TODO: fix_gamma=True

        relu70_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_28_")(batchnorm70_1_28_)

        conv71_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_28_")(relu70_1_28_)
        # conv71_1_28_, output shape: {[14,14,1024]}

        batchnorm71_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_28_",)(conv71_1_28_) #TODO: fix_gamma=True

        conv69_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_29_")(relu67_)
        # conv69_1_29_, output shape: {[14,14,16]}

        batchnorm69_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_29_",)(conv69_1_29_) #TODO: fix_gamma=True

        relu69_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_29_")(batchnorm69_1_29_)

        conv70_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_29_")(relu69_1_29_)
        # conv70_1_29_, output shape: {[14,14,16]}

        batchnorm70_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_29_",)(conv70_1_29_) #TODO: fix_gamma=True

        relu70_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_29_")(batchnorm70_1_29_)

        conv71_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_29_")(relu70_1_29_)
        # conv71_1_29_, output shape: {[14,14,1024]}

        batchnorm71_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_29_",)(conv71_1_29_) #TODO: fix_gamma=True

        conv69_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_30_")(relu67_)
        # conv69_1_30_, output shape: {[14,14,16]}

        batchnorm69_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_30_",)(conv69_1_30_) #TODO: fix_gamma=True

        relu69_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_30_")(batchnorm69_1_30_)

        conv70_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_30_")(relu69_1_30_)
        # conv70_1_30_, output shape: {[14,14,16]}

        batchnorm70_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_30_",)(conv70_1_30_) #TODO: fix_gamma=True

        relu70_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_30_")(batchnorm70_1_30_)

        conv71_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_30_")(relu70_1_30_)
        # conv71_1_30_, output shape: {[14,14,1024]}

        batchnorm71_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_30_",)(conv71_1_30_) #TODO: fix_gamma=True

        conv69_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_31_")(relu67_)
        # conv69_1_31_, output shape: {[14,14,16]}

        batchnorm69_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_31_",)(conv69_1_31_) #TODO: fix_gamma=True

        relu69_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_31_")(batchnorm69_1_31_)

        conv70_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_31_")(relu69_1_31_)
        # conv70_1_31_, output shape: {[14,14,16]}

        batchnorm70_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_31_",)(conv70_1_31_) #TODO: fix_gamma=True

        relu70_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_31_")(batchnorm70_1_31_)

        conv71_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_31_")(relu70_1_31_)
        # conv71_1_31_, output shape: {[14,14,1024]}

        batchnorm71_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_31_",)(conv71_1_31_) #TODO: fix_gamma=True

        conv69_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv69_1_32_")(relu67_)
        # conv69_1_32_, output shape: {[14,14,16]}

        batchnorm69_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm69_1_32_",)(conv69_1_32_) #TODO: fix_gamma=True

        relu69_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu69_1_32_")(batchnorm69_1_32_)

        conv70_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv70_1_32_")(relu69_1_32_)
        # conv70_1_32_, output shape: {[14,14,16]}

        batchnorm70_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm70_1_32_",)(conv70_1_32_) #TODO: fix_gamma=True

        relu70_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu70_1_32_")(batchnorm70_1_32_)

        conv71_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv71_1_32_")(relu70_1_32_)
        # conv71_1_32_, output shape: {[14,14,1024]}

        batchnorm71_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm71_1_32_",)(conv71_1_32_) #TODO: fix_gamma=True

        add72_1_ = tf.keras.layers.Add()([batchnorm71_1_1_,  batchnorm71_1_2_,  batchnorm71_1_3_,  batchnorm71_1_4_,  batchnorm71_1_5_,  batchnorm71_1_6_,  batchnorm71_1_7_,  batchnorm71_1_8_,  batchnorm71_1_9_,  batchnorm71_1_10_,  batchnorm71_1_11_,  batchnorm71_1_12_,  batchnorm71_1_13_,  batchnorm71_1_14_,  batchnorm71_1_15_,  batchnorm71_1_16_,  batchnorm71_1_17_,  batchnorm71_1_18_,  batchnorm71_1_19_,  batchnorm71_1_20_,  batchnorm71_1_21_,  batchnorm71_1_22_,  batchnorm71_1_23_,  batchnorm71_1_24_,  batchnorm71_1_25_,  batchnorm71_1_26_,  batchnorm71_1_27_,  batchnorm71_1_28_,  batchnorm71_1_29_,  batchnorm71_1_30_,  batchnorm71_1_31_,  batchnorm71_1_32_])
        # add72_1_, output shape: {[14,14,1024]}

        add73_ = tf.keras.layers.Add()([add72_1_,  relu67_])
        # add73_, output shape: {[14,14,1024]}

        relu73_ = tf.keras.layers.Activation(activation = "relu", name="relu73_")(add73_)

        conv75_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_1_")(relu73_)
        # conv75_1_1_, output shape: {[14,14,16]}

        batchnorm75_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_1_",)(conv75_1_1_) #TODO: fix_gamma=True

        relu75_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_1_")(batchnorm75_1_1_)

        conv76_1_1_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_1_")(relu75_1_1_)
        # conv76_1_1_, output shape: {[14,14,16]}

        batchnorm76_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_1_",)(conv76_1_1_) #TODO: fix_gamma=True

        relu76_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_1_")(batchnorm76_1_1_)

        conv77_1_1_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_1_")(relu76_1_1_)
        # conv77_1_1_, output shape: {[14,14,1024]}

        batchnorm77_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_1_",)(conv77_1_1_) #TODO: fix_gamma=True

        conv75_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_2_")(relu73_)
        # conv75_1_2_, output shape: {[14,14,16]}

        batchnorm75_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_2_",)(conv75_1_2_) #TODO: fix_gamma=True

        relu75_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_2_")(batchnorm75_1_2_)

        conv76_1_2_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_2_")(relu75_1_2_)
        # conv76_1_2_, output shape: {[14,14,16]}

        batchnorm76_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_2_",)(conv76_1_2_) #TODO: fix_gamma=True

        relu76_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_2_")(batchnorm76_1_2_)

        conv77_1_2_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_2_")(relu76_1_2_)
        # conv77_1_2_, output shape: {[14,14,1024]}

        batchnorm77_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_2_",)(conv77_1_2_) #TODO: fix_gamma=True

        conv75_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_3_")(relu73_)
        # conv75_1_3_, output shape: {[14,14,16]}

        batchnorm75_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_3_",)(conv75_1_3_) #TODO: fix_gamma=True

        relu75_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_3_")(batchnorm75_1_3_)

        conv76_1_3_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_3_")(relu75_1_3_)
        # conv76_1_3_, output shape: {[14,14,16]}

        batchnorm76_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_3_",)(conv76_1_3_) #TODO: fix_gamma=True

        relu76_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_3_")(batchnorm76_1_3_)

        conv77_1_3_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_3_")(relu76_1_3_)
        # conv77_1_3_, output shape: {[14,14,1024]}

        batchnorm77_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_3_",)(conv77_1_3_) #TODO: fix_gamma=True

        conv75_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_4_")(relu73_)
        # conv75_1_4_, output shape: {[14,14,16]}

        batchnorm75_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_4_",)(conv75_1_4_) #TODO: fix_gamma=True

        relu75_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_4_")(batchnorm75_1_4_)

        conv76_1_4_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_4_")(relu75_1_4_)
        # conv76_1_4_, output shape: {[14,14,16]}

        batchnorm76_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_4_",)(conv76_1_4_) #TODO: fix_gamma=True

        relu76_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_4_")(batchnorm76_1_4_)

        conv77_1_4_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_4_")(relu76_1_4_)
        # conv77_1_4_, output shape: {[14,14,1024]}

        batchnorm77_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_4_",)(conv77_1_4_) #TODO: fix_gamma=True

        conv75_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_5_")(relu73_)
        # conv75_1_5_, output shape: {[14,14,16]}

        batchnorm75_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_5_",)(conv75_1_5_) #TODO: fix_gamma=True

        relu75_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_5_")(batchnorm75_1_5_)

        conv76_1_5_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_5_")(relu75_1_5_)
        # conv76_1_5_, output shape: {[14,14,16]}

        batchnorm76_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_5_",)(conv76_1_5_) #TODO: fix_gamma=True

        relu76_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_5_")(batchnorm76_1_5_)

        conv77_1_5_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_5_")(relu76_1_5_)
        # conv77_1_5_, output shape: {[14,14,1024]}

        batchnorm77_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_5_",)(conv77_1_5_) #TODO: fix_gamma=True

        conv75_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_6_")(relu73_)
        # conv75_1_6_, output shape: {[14,14,16]}

        batchnorm75_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_6_",)(conv75_1_6_) #TODO: fix_gamma=True

        relu75_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_6_")(batchnorm75_1_6_)

        conv76_1_6_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_6_")(relu75_1_6_)
        # conv76_1_6_, output shape: {[14,14,16]}

        batchnorm76_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_6_",)(conv76_1_6_) #TODO: fix_gamma=True

        relu76_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_6_")(batchnorm76_1_6_)

        conv77_1_6_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_6_")(relu76_1_6_)
        # conv77_1_6_, output shape: {[14,14,1024]}

        batchnorm77_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_6_",)(conv77_1_6_) #TODO: fix_gamma=True

        conv75_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_7_")(relu73_)
        # conv75_1_7_, output shape: {[14,14,16]}

        batchnorm75_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_7_",)(conv75_1_7_) #TODO: fix_gamma=True

        relu75_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_7_")(batchnorm75_1_7_)

        conv76_1_7_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_7_")(relu75_1_7_)
        # conv76_1_7_, output shape: {[14,14,16]}

        batchnorm76_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_7_",)(conv76_1_7_) #TODO: fix_gamma=True

        relu76_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_7_")(batchnorm76_1_7_)

        conv77_1_7_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_7_")(relu76_1_7_)
        # conv77_1_7_, output shape: {[14,14,1024]}

        batchnorm77_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_7_",)(conv77_1_7_) #TODO: fix_gamma=True

        conv75_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_8_")(relu73_)
        # conv75_1_8_, output shape: {[14,14,16]}

        batchnorm75_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_8_",)(conv75_1_8_) #TODO: fix_gamma=True

        relu75_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_8_")(batchnorm75_1_8_)

        conv76_1_8_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_8_")(relu75_1_8_)
        # conv76_1_8_, output shape: {[14,14,16]}

        batchnorm76_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_8_",)(conv76_1_8_) #TODO: fix_gamma=True

        relu76_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_8_")(batchnorm76_1_8_)

        conv77_1_8_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_8_")(relu76_1_8_)
        # conv77_1_8_, output shape: {[14,14,1024]}

        batchnorm77_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_8_",)(conv77_1_8_) #TODO: fix_gamma=True

        conv75_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_9_")(relu73_)
        # conv75_1_9_, output shape: {[14,14,16]}

        batchnorm75_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_9_",)(conv75_1_9_) #TODO: fix_gamma=True

        relu75_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_9_")(batchnorm75_1_9_)

        conv76_1_9_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_9_")(relu75_1_9_)
        # conv76_1_9_, output shape: {[14,14,16]}

        batchnorm76_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_9_",)(conv76_1_9_) #TODO: fix_gamma=True

        relu76_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_9_")(batchnorm76_1_9_)

        conv77_1_9_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_9_")(relu76_1_9_)
        # conv77_1_9_, output shape: {[14,14,1024]}

        batchnorm77_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_9_",)(conv77_1_9_) #TODO: fix_gamma=True

        conv75_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_10_")(relu73_)
        # conv75_1_10_, output shape: {[14,14,16]}

        batchnorm75_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_10_",)(conv75_1_10_) #TODO: fix_gamma=True

        relu75_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_10_")(batchnorm75_1_10_)

        conv76_1_10_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_10_")(relu75_1_10_)
        # conv76_1_10_, output shape: {[14,14,16]}

        batchnorm76_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_10_",)(conv76_1_10_) #TODO: fix_gamma=True

        relu76_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_10_")(batchnorm76_1_10_)

        conv77_1_10_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_10_")(relu76_1_10_)
        # conv77_1_10_, output shape: {[14,14,1024]}

        batchnorm77_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_10_",)(conv77_1_10_) #TODO: fix_gamma=True

        conv75_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_11_")(relu73_)
        # conv75_1_11_, output shape: {[14,14,16]}

        batchnorm75_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_11_",)(conv75_1_11_) #TODO: fix_gamma=True

        relu75_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_11_")(batchnorm75_1_11_)

        conv76_1_11_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_11_")(relu75_1_11_)
        # conv76_1_11_, output shape: {[14,14,16]}

        batchnorm76_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_11_",)(conv76_1_11_) #TODO: fix_gamma=True

        relu76_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_11_")(batchnorm76_1_11_)

        conv77_1_11_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_11_")(relu76_1_11_)
        # conv77_1_11_, output shape: {[14,14,1024]}

        batchnorm77_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_11_",)(conv77_1_11_) #TODO: fix_gamma=True

        conv75_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_12_")(relu73_)
        # conv75_1_12_, output shape: {[14,14,16]}

        batchnorm75_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_12_",)(conv75_1_12_) #TODO: fix_gamma=True

        relu75_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_12_")(batchnorm75_1_12_)

        conv76_1_12_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_12_")(relu75_1_12_)
        # conv76_1_12_, output shape: {[14,14,16]}

        batchnorm76_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_12_",)(conv76_1_12_) #TODO: fix_gamma=True

        relu76_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_12_")(batchnorm76_1_12_)

        conv77_1_12_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_12_")(relu76_1_12_)
        # conv77_1_12_, output shape: {[14,14,1024]}

        batchnorm77_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_12_",)(conv77_1_12_) #TODO: fix_gamma=True

        conv75_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_13_")(relu73_)
        # conv75_1_13_, output shape: {[14,14,16]}

        batchnorm75_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_13_",)(conv75_1_13_) #TODO: fix_gamma=True

        relu75_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_13_")(batchnorm75_1_13_)

        conv76_1_13_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_13_")(relu75_1_13_)
        # conv76_1_13_, output shape: {[14,14,16]}

        batchnorm76_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_13_",)(conv76_1_13_) #TODO: fix_gamma=True

        relu76_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_13_")(batchnorm76_1_13_)

        conv77_1_13_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_13_")(relu76_1_13_)
        # conv77_1_13_, output shape: {[14,14,1024]}

        batchnorm77_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_13_",)(conv77_1_13_) #TODO: fix_gamma=True

        conv75_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_14_")(relu73_)
        # conv75_1_14_, output shape: {[14,14,16]}

        batchnorm75_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_14_",)(conv75_1_14_) #TODO: fix_gamma=True

        relu75_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_14_")(batchnorm75_1_14_)

        conv76_1_14_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_14_")(relu75_1_14_)
        # conv76_1_14_, output shape: {[14,14,16]}

        batchnorm76_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_14_",)(conv76_1_14_) #TODO: fix_gamma=True

        relu76_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_14_")(batchnorm76_1_14_)

        conv77_1_14_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_14_")(relu76_1_14_)
        # conv77_1_14_, output shape: {[14,14,1024]}

        batchnorm77_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_14_",)(conv77_1_14_) #TODO: fix_gamma=True

        conv75_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_15_")(relu73_)
        # conv75_1_15_, output shape: {[14,14,16]}

        batchnorm75_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_15_",)(conv75_1_15_) #TODO: fix_gamma=True

        relu75_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_15_")(batchnorm75_1_15_)

        conv76_1_15_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_15_")(relu75_1_15_)
        # conv76_1_15_, output shape: {[14,14,16]}

        batchnorm76_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_15_",)(conv76_1_15_) #TODO: fix_gamma=True

        relu76_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_15_")(batchnorm76_1_15_)

        conv77_1_15_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_15_")(relu76_1_15_)
        # conv77_1_15_, output shape: {[14,14,1024]}

        batchnorm77_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_15_",)(conv77_1_15_) #TODO: fix_gamma=True

        conv75_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_16_")(relu73_)
        # conv75_1_16_, output shape: {[14,14,16]}

        batchnorm75_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_16_",)(conv75_1_16_) #TODO: fix_gamma=True

        relu75_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_16_")(batchnorm75_1_16_)

        conv76_1_16_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_16_")(relu75_1_16_)
        # conv76_1_16_, output shape: {[14,14,16]}

        batchnorm76_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_16_",)(conv76_1_16_) #TODO: fix_gamma=True

        relu76_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_16_")(batchnorm76_1_16_)

        conv77_1_16_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_16_")(relu76_1_16_)
        # conv77_1_16_, output shape: {[14,14,1024]}

        batchnorm77_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_16_",)(conv77_1_16_) #TODO: fix_gamma=True

        conv75_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_17_")(relu73_)
        # conv75_1_17_, output shape: {[14,14,16]}

        batchnorm75_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_17_",)(conv75_1_17_) #TODO: fix_gamma=True

        relu75_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_17_")(batchnorm75_1_17_)

        conv76_1_17_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_17_")(relu75_1_17_)
        # conv76_1_17_, output shape: {[14,14,16]}

        batchnorm76_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_17_",)(conv76_1_17_) #TODO: fix_gamma=True

        relu76_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_17_")(batchnorm76_1_17_)

        conv77_1_17_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_17_")(relu76_1_17_)
        # conv77_1_17_, output shape: {[14,14,1024]}

        batchnorm77_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_17_",)(conv77_1_17_) #TODO: fix_gamma=True

        conv75_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_18_")(relu73_)
        # conv75_1_18_, output shape: {[14,14,16]}

        batchnorm75_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_18_",)(conv75_1_18_) #TODO: fix_gamma=True

        relu75_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_18_")(batchnorm75_1_18_)

        conv76_1_18_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_18_")(relu75_1_18_)
        # conv76_1_18_, output shape: {[14,14,16]}

        batchnorm76_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_18_",)(conv76_1_18_) #TODO: fix_gamma=True

        relu76_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_18_")(batchnorm76_1_18_)

        conv77_1_18_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_18_")(relu76_1_18_)
        # conv77_1_18_, output shape: {[14,14,1024]}

        batchnorm77_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_18_",)(conv77_1_18_) #TODO: fix_gamma=True

        conv75_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_19_")(relu73_)
        # conv75_1_19_, output shape: {[14,14,16]}

        batchnorm75_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_19_",)(conv75_1_19_) #TODO: fix_gamma=True

        relu75_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_19_")(batchnorm75_1_19_)

        conv76_1_19_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_19_")(relu75_1_19_)
        # conv76_1_19_, output shape: {[14,14,16]}

        batchnorm76_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_19_",)(conv76_1_19_) #TODO: fix_gamma=True

        relu76_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_19_")(batchnorm76_1_19_)

        conv77_1_19_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_19_")(relu76_1_19_)
        # conv77_1_19_, output shape: {[14,14,1024]}

        batchnorm77_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_19_",)(conv77_1_19_) #TODO: fix_gamma=True

        conv75_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_20_")(relu73_)
        # conv75_1_20_, output shape: {[14,14,16]}

        batchnorm75_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_20_",)(conv75_1_20_) #TODO: fix_gamma=True

        relu75_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_20_")(batchnorm75_1_20_)

        conv76_1_20_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_20_")(relu75_1_20_)
        # conv76_1_20_, output shape: {[14,14,16]}

        batchnorm76_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_20_",)(conv76_1_20_) #TODO: fix_gamma=True

        relu76_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_20_")(batchnorm76_1_20_)

        conv77_1_20_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_20_")(relu76_1_20_)
        # conv77_1_20_, output shape: {[14,14,1024]}

        batchnorm77_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_20_",)(conv77_1_20_) #TODO: fix_gamma=True

        conv75_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_21_")(relu73_)
        # conv75_1_21_, output shape: {[14,14,16]}

        batchnorm75_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_21_",)(conv75_1_21_) #TODO: fix_gamma=True

        relu75_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_21_")(batchnorm75_1_21_)

        conv76_1_21_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_21_")(relu75_1_21_)
        # conv76_1_21_, output shape: {[14,14,16]}

        batchnorm76_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_21_",)(conv76_1_21_) #TODO: fix_gamma=True

        relu76_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_21_")(batchnorm76_1_21_)

        conv77_1_21_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_21_")(relu76_1_21_)
        # conv77_1_21_, output shape: {[14,14,1024]}

        batchnorm77_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_21_",)(conv77_1_21_) #TODO: fix_gamma=True

        conv75_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_22_")(relu73_)
        # conv75_1_22_, output shape: {[14,14,16]}

        batchnorm75_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_22_",)(conv75_1_22_) #TODO: fix_gamma=True

        relu75_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_22_")(batchnorm75_1_22_)

        conv76_1_22_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_22_")(relu75_1_22_)
        # conv76_1_22_, output shape: {[14,14,16]}

        batchnorm76_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_22_",)(conv76_1_22_) #TODO: fix_gamma=True

        relu76_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_22_")(batchnorm76_1_22_)

        conv77_1_22_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_22_")(relu76_1_22_)
        # conv77_1_22_, output shape: {[14,14,1024]}

        batchnorm77_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_22_",)(conv77_1_22_) #TODO: fix_gamma=True

        conv75_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_23_")(relu73_)
        # conv75_1_23_, output shape: {[14,14,16]}

        batchnorm75_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_23_",)(conv75_1_23_) #TODO: fix_gamma=True

        relu75_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_23_")(batchnorm75_1_23_)

        conv76_1_23_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_23_")(relu75_1_23_)
        # conv76_1_23_, output shape: {[14,14,16]}

        batchnorm76_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_23_",)(conv76_1_23_) #TODO: fix_gamma=True

        relu76_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_23_")(batchnorm76_1_23_)

        conv77_1_23_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_23_")(relu76_1_23_)
        # conv77_1_23_, output shape: {[14,14,1024]}

        batchnorm77_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_23_",)(conv77_1_23_) #TODO: fix_gamma=True

        conv75_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_24_")(relu73_)
        # conv75_1_24_, output shape: {[14,14,16]}

        batchnorm75_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_24_",)(conv75_1_24_) #TODO: fix_gamma=True

        relu75_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_24_")(batchnorm75_1_24_)

        conv76_1_24_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_24_")(relu75_1_24_)
        # conv76_1_24_, output shape: {[14,14,16]}

        batchnorm76_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_24_",)(conv76_1_24_) #TODO: fix_gamma=True

        relu76_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_24_")(batchnorm76_1_24_)

        conv77_1_24_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_24_")(relu76_1_24_)
        # conv77_1_24_, output shape: {[14,14,1024]}

        batchnorm77_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_24_",)(conv77_1_24_) #TODO: fix_gamma=True

        conv75_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_25_")(relu73_)
        # conv75_1_25_, output shape: {[14,14,16]}

        batchnorm75_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_25_",)(conv75_1_25_) #TODO: fix_gamma=True

        relu75_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_25_")(batchnorm75_1_25_)

        conv76_1_25_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_25_")(relu75_1_25_)
        # conv76_1_25_, output shape: {[14,14,16]}

        batchnorm76_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_25_",)(conv76_1_25_) #TODO: fix_gamma=True

        relu76_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_25_")(batchnorm76_1_25_)

        conv77_1_25_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_25_")(relu76_1_25_)
        # conv77_1_25_, output shape: {[14,14,1024]}

        batchnorm77_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_25_",)(conv77_1_25_) #TODO: fix_gamma=True

        conv75_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_26_")(relu73_)
        # conv75_1_26_, output shape: {[14,14,16]}

        batchnorm75_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_26_",)(conv75_1_26_) #TODO: fix_gamma=True

        relu75_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_26_")(batchnorm75_1_26_)

        conv76_1_26_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_26_")(relu75_1_26_)
        # conv76_1_26_, output shape: {[14,14,16]}

        batchnorm76_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_26_",)(conv76_1_26_) #TODO: fix_gamma=True

        relu76_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_26_")(batchnorm76_1_26_)

        conv77_1_26_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_26_")(relu76_1_26_)
        # conv77_1_26_, output shape: {[14,14,1024]}

        batchnorm77_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_26_",)(conv77_1_26_) #TODO: fix_gamma=True

        conv75_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_27_")(relu73_)
        # conv75_1_27_, output shape: {[14,14,16]}

        batchnorm75_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_27_",)(conv75_1_27_) #TODO: fix_gamma=True

        relu75_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_27_")(batchnorm75_1_27_)

        conv76_1_27_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_27_")(relu75_1_27_)
        # conv76_1_27_, output shape: {[14,14,16]}

        batchnorm76_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_27_",)(conv76_1_27_) #TODO: fix_gamma=True

        relu76_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_27_")(batchnorm76_1_27_)

        conv77_1_27_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_27_")(relu76_1_27_)
        # conv77_1_27_, output shape: {[14,14,1024]}

        batchnorm77_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_27_",)(conv77_1_27_) #TODO: fix_gamma=True

        conv75_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_28_")(relu73_)
        # conv75_1_28_, output shape: {[14,14,16]}

        batchnorm75_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_28_",)(conv75_1_28_) #TODO: fix_gamma=True

        relu75_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_28_")(batchnorm75_1_28_)

        conv76_1_28_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_28_")(relu75_1_28_)
        # conv76_1_28_, output shape: {[14,14,16]}

        batchnorm76_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_28_",)(conv76_1_28_) #TODO: fix_gamma=True

        relu76_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_28_")(batchnorm76_1_28_)

        conv77_1_28_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_28_")(relu76_1_28_)
        # conv77_1_28_, output shape: {[14,14,1024]}

        batchnorm77_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_28_",)(conv77_1_28_) #TODO: fix_gamma=True

        conv75_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_29_")(relu73_)
        # conv75_1_29_, output shape: {[14,14,16]}

        batchnorm75_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_29_",)(conv75_1_29_) #TODO: fix_gamma=True

        relu75_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_29_")(batchnorm75_1_29_)

        conv76_1_29_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_29_")(relu75_1_29_)
        # conv76_1_29_, output shape: {[14,14,16]}

        batchnorm76_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_29_",)(conv76_1_29_) #TODO: fix_gamma=True

        relu76_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_29_")(batchnorm76_1_29_)

        conv77_1_29_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_29_")(relu76_1_29_)
        # conv77_1_29_, output shape: {[14,14,1024]}

        batchnorm77_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_29_",)(conv77_1_29_) #TODO: fix_gamma=True

        conv75_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_30_")(relu73_)
        # conv75_1_30_, output shape: {[14,14,16]}

        batchnorm75_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_30_",)(conv75_1_30_) #TODO: fix_gamma=True

        relu75_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_30_")(batchnorm75_1_30_)

        conv76_1_30_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_30_")(relu75_1_30_)
        # conv76_1_30_, output shape: {[14,14,16]}

        batchnorm76_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_30_",)(conv76_1_30_) #TODO: fix_gamma=True

        relu76_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_30_")(batchnorm76_1_30_)

        conv77_1_30_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_30_")(relu76_1_30_)
        # conv77_1_30_, output shape: {[14,14,1024]}

        batchnorm77_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_30_",)(conv77_1_30_) #TODO: fix_gamma=True

        conv75_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_31_")(relu73_)
        # conv75_1_31_, output shape: {[14,14,16]}

        batchnorm75_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_31_",)(conv75_1_31_) #TODO: fix_gamma=True

        relu75_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_31_")(batchnorm75_1_31_)

        conv76_1_31_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_31_")(relu75_1_31_)
        # conv76_1_31_, output shape: {[14,14,16]}

        batchnorm76_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_31_",)(conv76_1_31_) #TODO: fix_gamma=True

        relu76_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_31_")(batchnorm76_1_31_)

        conv77_1_31_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_31_")(relu76_1_31_)
        # conv77_1_31_, output shape: {[14,14,1024]}

        batchnorm77_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_31_",)(conv77_1_31_) #TODO: fix_gamma=True

        conv75_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv75_1_32_")(relu73_)
        # conv75_1_32_, output shape: {[14,14,16]}

        batchnorm75_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm75_1_32_",)(conv75_1_32_) #TODO: fix_gamma=True

        relu75_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu75_1_32_")(batchnorm75_1_32_)

        conv76_1_32_ = tf.keras.layers.Conv2D(16, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv76_1_32_")(relu75_1_32_)
        # conv76_1_32_, output shape: {[14,14,16]}

        batchnorm76_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm76_1_32_",)(conv76_1_32_) #TODO: fix_gamma=True

        relu76_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu76_1_32_")(batchnorm76_1_32_)

        conv77_1_32_ = tf.keras.layers.Conv2D(1024, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv77_1_32_")(relu76_1_32_)
        # conv77_1_32_, output shape: {[14,14,1024]}

        batchnorm77_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm77_1_32_",)(conv77_1_32_) #TODO: fix_gamma=True

        add78_1_ = tf.keras.layers.Add()([batchnorm77_1_1_,  batchnorm77_1_2_,  batchnorm77_1_3_,  batchnorm77_1_4_,  batchnorm77_1_5_,  batchnorm77_1_6_,  batchnorm77_1_7_,  batchnorm77_1_8_,  batchnorm77_1_9_,  batchnorm77_1_10_,  batchnorm77_1_11_,  batchnorm77_1_12_,  batchnorm77_1_13_,  batchnorm77_1_14_,  batchnorm77_1_15_,  batchnorm77_1_16_,  batchnorm77_1_17_,  batchnorm77_1_18_,  batchnorm77_1_19_,  batchnorm77_1_20_,  batchnorm77_1_21_,  batchnorm77_1_22_,  batchnorm77_1_23_,  batchnorm77_1_24_,  batchnorm77_1_25_,  batchnorm77_1_26_,  batchnorm77_1_27_,  batchnorm77_1_28_,  batchnorm77_1_29_,  batchnorm77_1_30_,  batchnorm77_1_31_,  batchnorm77_1_32_])
        # add78_1_, output shape: {[14,14,1024]}

        add79_ = tf.keras.layers.Add()([add78_1_,  relu73_])
        # add79_, output shape: {[14,14,1024]}

        relu79_ = tf.keras.layers.Activation(activation = "relu", name="relu79_")(add79_)

        conv81_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_1_")(relu79_)
        # conv81_1_1_, output shape: {[14,14,32]}

        batchnorm81_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_1_",)(conv81_1_1_) #TODO: fix_gamma=True

        relu81_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_1_")(batchnorm81_1_1_)

        conv82_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_1_")(relu81_1_1_)
        # conv82_1_1_, output shape: {[7,7,32]}

        batchnorm82_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_1_",)(conv82_1_1_) #TODO: fix_gamma=True

        relu82_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_1_")(batchnorm82_1_1_)

        conv83_1_1_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_1_")(relu82_1_1_)
        # conv83_1_1_, output shape: {[7,7,2048]}

        batchnorm83_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_1_",)(conv83_1_1_) #TODO: fix_gamma=True

        conv81_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_2_")(relu79_)
        # conv81_1_2_, output shape: {[14,14,32]}

        batchnorm81_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_2_",)(conv81_1_2_) #TODO: fix_gamma=True

        relu81_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_2_")(batchnorm81_1_2_)

        conv82_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_2_")(relu81_1_2_)
        # conv82_1_2_, output shape: {[7,7,32]}

        batchnorm82_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_2_",)(conv82_1_2_) #TODO: fix_gamma=True

        relu82_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_2_")(batchnorm82_1_2_)

        conv83_1_2_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_2_")(relu82_1_2_)
        # conv83_1_2_, output shape: {[7,7,2048]}

        batchnorm83_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_2_",)(conv83_1_2_) #TODO: fix_gamma=True

        conv81_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_3_")(relu79_)
        # conv81_1_3_, output shape: {[14,14,32]}

        batchnorm81_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_3_",)(conv81_1_3_) #TODO: fix_gamma=True

        relu81_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_3_")(batchnorm81_1_3_)

        conv82_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_3_")(relu81_1_3_)
        # conv82_1_3_, output shape: {[7,7,32]}

        batchnorm82_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_3_",)(conv82_1_3_) #TODO: fix_gamma=True

        relu82_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_3_")(batchnorm82_1_3_)

        conv83_1_3_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_3_")(relu82_1_3_)
        # conv83_1_3_, output shape: {[7,7,2048]}

        batchnorm83_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_3_",)(conv83_1_3_) #TODO: fix_gamma=True

        conv81_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_4_")(relu79_)
        # conv81_1_4_, output shape: {[14,14,32]}

        batchnorm81_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_4_",)(conv81_1_4_) #TODO: fix_gamma=True

        relu81_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_4_")(batchnorm81_1_4_)

        conv82_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_4_")(relu81_1_4_)
        # conv82_1_4_, output shape: {[7,7,32]}

        batchnorm82_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_4_",)(conv82_1_4_) #TODO: fix_gamma=True

        relu82_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_4_")(batchnorm82_1_4_)

        conv83_1_4_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_4_")(relu82_1_4_)
        # conv83_1_4_, output shape: {[7,7,2048]}

        batchnorm83_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_4_",)(conv83_1_4_) #TODO: fix_gamma=True

        conv81_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_5_")(relu79_)
        # conv81_1_5_, output shape: {[14,14,32]}

        batchnorm81_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_5_",)(conv81_1_5_) #TODO: fix_gamma=True

        relu81_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_5_")(batchnorm81_1_5_)

        conv82_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_5_")(relu81_1_5_)
        # conv82_1_5_, output shape: {[7,7,32]}

        batchnorm82_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_5_",)(conv82_1_5_) #TODO: fix_gamma=True

        relu82_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_5_")(batchnorm82_1_5_)

        conv83_1_5_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_5_")(relu82_1_5_)
        # conv83_1_5_, output shape: {[7,7,2048]}

        batchnorm83_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_5_",)(conv83_1_5_) #TODO: fix_gamma=True

        conv81_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_6_")(relu79_)
        # conv81_1_6_, output shape: {[14,14,32]}

        batchnorm81_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_6_",)(conv81_1_6_) #TODO: fix_gamma=True

        relu81_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_6_")(batchnorm81_1_6_)

        conv82_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_6_")(relu81_1_6_)
        # conv82_1_6_, output shape: {[7,7,32]}

        batchnorm82_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_6_",)(conv82_1_6_) #TODO: fix_gamma=True

        relu82_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_6_")(batchnorm82_1_6_)

        conv83_1_6_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_6_")(relu82_1_6_)
        # conv83_1_6_, output shape: {[7,7,2048]}

        batchnorm83_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_6_",)(conv83_1_6_) #TODO: fix_gamma=True

        conv81_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_7_")(relu79_)
        # conv81_1_7_, output shape: {[14,14,32]}

        batchnorm81_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_7_",)(conv81_1_7_) #TODO: fix_gamma=True

        relu81_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_7_")(batchnorm81_1_7_)

        conv82_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_7_")(relu81_1_7_)
        # conv82_1_7_, output shape: {[7,7,32]}

        batchnorm82_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_7_",)(conv82_1_7_) #TODO: fix_gamma=True

        relu82_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_7_")(batchnorm82_1_7_)

        conv83_1_7_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_7_")(relu82_1_7_)
        # conv83_1_7_, output shape: {[7,7,2048]}

        batchnorm83_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_7_",)(conv83_1_7_) #TODO: fix_gamma=True

        conv81_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_8_")(relu79_)
        # conv81_1_8_, output shape: {[14,14,32]}

        batchnorm81_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_8_",)(conv81_1_8_) #TODO: fix_gamma=True

        relu81_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_8_")(batchnorm81_1_8_)

        conv82_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_8_")(relu81_1_8_)
        # conv82_1_8_, output shape: {[7,7,32]}

        batchnorm82_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_8_",)(conv82_1_8_) #TODO: fix_gamma=True

        relu82_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_8_")(batchnorm82_1_8_)

        conv83_1_8_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_8_")(relu82_1_8_)
        # conv83_1_8_, output shape: {[7,7,2048]}

        batchnorm83_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_8_",)(conv83_1_8_) #TODO: fix_gamma=True

        conv81_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_9_")(relu79_)
        # conv81_1_9_, output shape: {[14,14,32]}

        batchnorm81_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_9_",)(conv81_1_9_) #TODO: fix_gamma=True

        relu81_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_9_")(batchnorm81_1_9_)

        conv82_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_9_")(relu81_1_9_)
        # conv82_1_9_, output shape: {[7,7,32]}

        batchnorm82_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_9_",)(conv82_1_9_) #TODO: fix_gamma=True

        relu82_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_9_")(batchnorm82_1_9_)

        conv83_1_9_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_9_")(relu82_1_9_)
        # conv83_1_9_, output shape: {[7,7,2048]}

        batchnorm83_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_9_",)(conv83_1_9_) #TODO: fix_gamma=True

        conv81_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_10_")(relu79_)
        # conv81_1_10_, output shape: {[14,14,32]}

        batchnorm81_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_10_",)(conv81_1_10_) #TODO: fix_gamma=True

        relu81_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_10_")(batchnorm81_1_10_)

        conv82_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_10_")(relu81_1_10_)
        # conv82_1_10_, output shape: {[7,7,32]}

        batchnorm82_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_10_",)(conv82_1_10_) #TODO: fix_gamma=True

        relu82_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_10_")(batchnorm82_1_10_)

        conv83_1_10_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_10_")(relu82_1_10_)
        # conv83_1_10_, output shape: {[7,7,2048]}

        batchnorm83_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_10_",)(conv83_1_10_) #TODO: fix_gamma=True

        conv81_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_11_")(relu79_)
        # conv81_1_11_, output shape: {[14,14,32]}

        batchnorm81_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_11_",)(conv81_1_11_) #TODO: fix_gamma=True

        relu81_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_11_")(batchnorm81_1_11_)

        conv82_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_11_")(relu81_1_11_)
        # conv82_1_11_, output shape: {[7,7,32]}

        batchnorm82_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_11_",)(conv82_1_11_) #TODO: fix_gamma=True

        relu82_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_11_")(batchnorm82_1_11_)

        conv83_1_11_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_11_")(relu82_1_11_)
        # conv83_1_11_, output shape: {[7,7,2048]}

        batchnorm83_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_11_",)(conv83_1_11_) #TODO: fix_gamma=True

        conv81_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_12_")(relu79_)
        # conv81_1_12_, output shape: {[14,14,32]}

        batchnorm81_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_12_",)(conv81_1_12_) #TODO: fix_gamma=True

        relu81_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_12_")(batchnorm81_1_12_)

        conv82_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_12_")(relu81_1_12_)
        # conv82_1_12_, output shape: {[7,7,32]}

        batchnorm82_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_12_",)(conv82_1_12_) #TODO: fix_gamma=True

        relu82_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_12_")(batchnorm82_1_12_)

        conv83_1_12_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_12_")(relu82_1_12_)
        # conv83_1_12_, output shape: {[7,7,2048]}

        batchnorm83_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_12_",)(conv83_1_12_) #TODO: fix_gamma=True

        conv81_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_13_")(relu79_)
        # conv81_1_13_, output shape: {[14,14,32]}

        batchnorm81_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_13_",)(conv81_1_13_) #TODO: fix_gamma=True

        relu81_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_13_")(batchnorm81_1_13_)

        conv82_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_13_")(relu81_1_13_)
        # conv82_1_13_, output shape: {[7,7,32]}

        batchnorm82_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_13_",)(conv82_1_13_) #TODO: fix_gamma=True

        relu82_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_13_")(batchnorm82_1_13_)

        conv83_1_13_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_13_")(relu82_1_13_)
        # conv83_1_13_, output shape: {[7,7,2048]}

        batchnorm83_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_13_",)(conv83_1_13_) #TODO: fix_gamma=True

        conv81_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_14_")(relu79_)
        # conv81_1_14_, output shape: {[14,14,32]}

        batchnorm81_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_14_",)(conv81_1_14_) #TODO: fix_gamma=True

        relu81_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_14_")(batchnorm81_1_14_)

        conv82_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_14_")(relu81_1_14_)
        # conv82_1_14_, output shape: {[7,7,32]}

        batchnorm82_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_14_",)(conv82_1_14_) #TODO: fix_gamma=True

        relu82_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_14_")(batchnorm82_1_14_)

        conv83_1_14_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_14_")(relu82_1_14_)
        # conv83_1_14_, output shape: {[7,7,2048]}

        batchnorm83_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_14_",)(conv83_1_14_) #TODO: fix_gamma=True

        conv81_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_15_")(relu79_)
        # conv81_1_15_, output shape: {[14,14,32]}

        batchnorm81_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_15_",)(conv81_1_15_) #TODO: fix_gamma=True

        relu81_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_15_")(batchnorm81_1_15_)

        conv82_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_15_")(relu81_1_15_)
        # conv82_1_15_, output shape: {[7,7,32]}

        batchnorm82_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_15_",)(conv82_1_15_) #TODO: fix_gamma=True

        relu82_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_15_")(batchnorm82_1_15_)

        conv83_1_15_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_15_")(relu82_1_15_)
        # conv83_1_15_, output shape: {[7,7,2048]}

        batchnorm83_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_15_",)(conv83_1_15_) #TODO: fix_gamma=True

        conv81_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_16_")(relu79_)
        # conv81_1_16_, output shape: {[14,14,32]}

        batchnorm81_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_16_",)(conv81_1_16_) #TODO: fix_gamma=True

        relu81_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_16_")(batchnorm81_1_16_)

        conv82_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_16_")(relu81_1_16_)
        # conv82_1_16_, output shape: {[7,7,32]}

        batchnorm82_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_16_",)(conv82_1_16_) #TODO: fix_gamma=True

        relu82_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_16_")(batchnorm82_1_16_)

        conv83_1_16_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_16_")(relu82_1_16_)
        # conv83_1_16_, output shape: {[7,7,2048]}

        batchnorm83_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_16_",)(conv83_1_16_) #TODO: fix_gamma=True

        conv81_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_17_")(relu79_)
        # conv81_1_17_, output shape: {[14,14,32]}

        batchnorm81_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_17_",)(conv81_1_17_) #TODO: fix_gamma=True

        relu81_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_17_")(batchnorm81_1_17_)

        conv82_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_17_")(relu81_1_17_)
        # conv82_1_17_, output shape: {[7,7,32]}

        batchnorm82_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_17_",)(conv82_1_17_) #TODO: fix_gamma=True

        relu82_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_17_")(batchnorm82_1_17_)

        conv83_1_17_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_17_")(relu82_1_17_)
        # conv83_1_17_, output shape: {[7,7,2048]}

        batchnorm83_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_17_",)(conv83_1_17_) #TODO: fix_gamma=True

        conv81_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_18_")(relu79_)
        # conv81_1_18_, output shape: {[14,14,32]}

        batchnorm81_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_18_",)(conv81_1_18_) #TODO: fix_gamma=True

        relu81_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_18_")(batchnorm81_1_18_)

        conv82_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_18_")(relu81_1_18_)
        # conv82_1_18_, output shape: {[7,7,32]}

        batchnorm82_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_18_",)(conv82_1_18_) #TODO: fix_gamma=True

        relu82_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_18_")(batchnorm82_1_18_)

        conv83_1_18_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_18_")(relu82_1_18_)
        # conv83_1_18_, output shape: {[7,7,2048]}

        batchnorm83_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_18_",)(conv83_1_18_) #TODO: fix_gamma=True

        conv81_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_19_")(relu79_)
        # conv81_1_19_, output shape: {[14,14,32]}

        batchnorm81_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_19_",)(conv81_1_19_) #TODO: fix_gamma=True

        relu81_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_19_")(batchnorm81_1_19_)

        conv82_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_19_")(relu81_1_19_)
        # conv82_1_19_, output shape: {[7,7,32]}

        batchnorm82_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_19_",)(conv82_1_19_) #TODO: fix_gamma=True

        relu82_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_19_")(batchnorm82_1_19_)

        conv83_1_19_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_19_")(relu82_1_19_)
        # conv83_1_19_, output shape: {[7,7,2048]}

        batchnorm83_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_19_",)(conv83_1_19_) #TODO: fix_gamma=True

        conv81_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_20_")(relu79_)
        # conv81_1_20_, output shape: {[14,14,32]}

        batchnorm81_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_20_",)(conv81_1_20_) #TODO: fix_gamma=True

        relu81_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_20_")(batchnorm81_1_20_)

        conv82_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_20_")(relu81_1_20_)
        # conv82_1_20_, output shape: {[7,7,32]}

        batchnorm82_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_20_",)(conv82_1_20_) #TODO: fix_gamma=True

        relu82_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_20_")(batchnorm82_1_20_)

        conv83_1_20_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_20_")(relu82_1_20_)
        # conv83_1_20_, output shape: {[7,7,2048]}

        batchnorm83_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_20_",)(conv83_1_20_) #TODO: fix_gamma=True

        conv81_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_21_")(relu79_)
        # conv81_1_21_, output shape: {[14,14,32]}

        batchnorm81_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_21_",)(conv81_1_21_) #TODO: fix_gamma=True

        relu81_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_21_")(batchnorm81_1_21_)

        conv82_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_21_")(relu81_1_21_)
        # conv82_1_21_, output shape: {[7,7,32]}

        batchnorm82_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_21_",)(conv82_1_21_) #TODO: fix_gamma=True

        relu82_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_21_")(batchnorm82_1_21_)

        conv83_1_21_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_21_")(relu82_1_21_)
        # conv83_1_21_, output shape: {[7,7,2048]}

        batchnorm83_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_21_",)(conv83_1_21_) #TODO: fix_gamma=True

        conv81_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_22_")(relu79_)
        # conv81_1_22_, output shape: {[14,14,32]}

        batchnorm81_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_22_",)(conv81_1_22_) #TODO: fix_gamma=True

        relu81_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_22_")(batchnorm81_1_22_)

        conv82_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_22_")(relu81_1_22_)
        # conv82_1_22_, output shape: {[7,7,32]}

        batchnorm82_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_22_",)(conv82_1_22_) #TODO: fix_gamma=True

        relu82_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_22_")(batchnorm82_1_22_)

        conv83_1_22_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_22_")(relu82_1_22_)
        # conv83_1_22_, output shape: {[7,7,2048]}

        batchnorm83_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_22_",)(conv83_1_22_) #TODO: fix_gamma=True

        conv81_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_23_")(relu79_)
        # conv81_1_23_, output shape: {[14,14,32]}

        batchnorm81_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_23_",)(conv81_1_23_) #TODO: fix_gamma=True

        relu81_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_23_")(batchnorm81_1_23_)

        conv82_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_23_")(relu81_1_23_)
        # conv82_1_23_, output shape: {[7,7,32]}

        batchnorm82_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_23_",)(conv82_1_23_) #TODO: fix_gamma=True

        relu82_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_23_")(batchnorm82_1_23_)

        conv83_1_23_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_23_")(relu82_1_23_)
        # conv83_1_23_, output shape: {[7,7,2048]}

        batchnorm83_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_23_",)(conv83_1_23_) #TODO: fix_gamma=True

        conv81_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_24_")(relu79_)
        # conv81_1_24_, output shape: {[14,14,32]}

        batchnorm81_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_24_",)(conv81_1_24_) #TODO: fix_gamma=True

        relu81_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_24_")(batchnorm81_1_24_)

        conv82_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_24_")(relu81_1_24_)
        # conv82_1_24_, output shape: {[7,7,32]}

        batchnorm82_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_24_",)(conv82_1_24_) #TODO: fix_gamma=True

        relu82_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_24_")(batchnorm82_1_24_)

        conv83_1_24_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_24_")(relu82_1_24_)
        # conv83_1_24_, output shape: {[7,7,2048]}

        batchnorm83_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_24_",)(conv83_1_24_) #TODO: fix_gamma=True

        conv81_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_25_")(relu79_)
        # conv81_1_25_, output shape: {[14,14,32]}

        batchnorm81_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_25_",)(conv81_1_25_) #TODO: fix_gamma=True

        relu81_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_25_")(batchnorm81_1_25_)

        conv82_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_25_")(relu81_1_25_)
        # conv82_1_25_, output shape: {[7,7,32]}

        batchnorm82_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_25_",)(conv82_1_25_) #TODO: fix_gamma=True

        relu82_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_25_")(batchnorm82_1_25_)

        conv83_1_25_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_25_")(relu82_1_25_)
        # conv83_1_25_, output shape: {[7,7,2048]}

        batchnorm83_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_25_",)(conv83_1_25_) #TODO: fix_gamma=True

        conv81_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_26_")(relu79_)
        # conv81_1_26_, output shape: {[14,14,32]}

        batchnorm81_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_26_",)(conv81_1_26_) #TODO: fix_gamma=True

        relu81_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_26_")(batchnorm81_1_26_)

        conv82_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_26_")(relu81_1_26_)
        # conv82_1_26_, output shape: {[7,7,32]}

        batchnorm82_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_26_",)(conv82_1_26_) #TODO: fix_gamma=True

        relu82_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_26_")(batchnorm82_1_26_)

        conv83_1_26_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_26_")(relu82_1_26_)
        # conv83_1_26_, output shape: {[7,7,2048]}

        batchnorm83_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_26_",)(conv83_1_26_) #TODO: fix_gamma=True

        conv81_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_27_")(relu79_)
        # conv81_1_27_, output shape: {[14,14,32]}

        batchnorm81_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_27_",)(conv81_1_27_) #TODO: fix_gamma=True

        relu81_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_27_")(batchnorm81_1_27_)

        conv82_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_27_")(relu81_1_27_)
        # conv82_1_27_, output shape: {[7,7,32]}

        batchnorm82_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_27_",)(conv82_1_27_) #TODO: fix_gamma=True

        relu82_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_27_")(batchnorm82_1_27_)

        conv83_1_27_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_27_")(relu82_1_27_)
        # conv83_1_27_, output shape: {[7,7,2048]}

        batchnorm83_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_27_",)(conv83_1_27_) #TODO: fix_gamma=True

        conv81_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_28_")(relu79_)
        # conv81_1_28_, output shape: {[14,14,32]}

        batchnorm81_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_28_",)(conv81_1_28_) #TODO: fix_gamma=True

        relu81_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_28_")(batchnorm81_1_28_)

        conv82_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_28_")(relu81_1_28_)
        # conv82_1_28_, output shape: {[7,7,32]}

        batchnorm82_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_28_",)(conv82_1_28_) #TODO: fix_gamma=True

        relu82_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_28_")(batchnorm82_1_28_)

        conv83_1_28_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_28_")(relu82_1_28_)
        # conv83_1_28_, output shape: {[7,7,2048]}

        batchnorm83_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_28_",)(conv83_1_28_) #TODO: fix_gamma=True

        conv81_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_29_")(relu79_)
        # conv81_1_29_, output shape: {[14,14,32]}

        batchnorm81_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_29_",)(conv81_1_29_) #TODO: fix_gamma=True

        relu81_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_29_")(batchnorm81_1_29_)

        conv82_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_29_")(relu81_1_29_)
        # conv82_1_29_, output shape: {[7,7,32]}

        batchnorm82_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_29_",)(conv82_1_29_) #TODO: fix_gamma=True

        relu82_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_29_")(batchnorm82_1_29_)

        conv83_1_29_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_29_")(relu82_1_29_)
        # conv83_1_29_, output shape: {[7,7,2048]}

        batchnorm83_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_29_",)(conv83_1_29_) #TODO: fix_gamma=True

        conv81_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_30_")(relu79_)
        # conv81_1_30_, output shape: {[14,14,32]}

        batchnorm81_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_30_",)(conv81_1_30_) #TODO: fix_gamma=True

        relu81_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_30_")(batchnorm81_1_30_)

        conv82_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_30_")(relu81_1_30_)
        # conv82_1_30_, output shape: {[7,7,32]}

        batchnorm82_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_30_",)(conv82_1_30_) #TODO: fix_gamma=True

        relu82_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_30_")(batchnorm82_1_30_)

        conv83_1_30_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_30_")(relu82_1_30_)
        # conv83_1_30_, output shape: {[7,7,2048]}

        batchnorm83_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_30_",)(conv83_1_30_) #TODO: fix_gamma=True

        conv81_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_31_")(relu79_)
        # conv81_1_31_, output shape: {[14,14,32]}

        batchnorm81_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_31_",)(conv81_1_31_) #TODO: fix_gamma=True

        relu81_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_31_")(batchnorm81_1_31_)

        conv82_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_31_")(relu81_1_31_)
        # conv82_1_31_, output shape: {[7,7,32]}

        batchnorm82_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_31_",)(conv82_1_31_) #TODO: fix_gamma=True

        relu82_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_31_")(batchnorm82_1_31_)

        conv83_1_31_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_31_")(relu82_1_31_)
        # conv83_1_31_, output shape: {[7,7,2048]}

        batchnorm83_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_31_",)(conv83_1_31_) #TODO: fix_gamma=True

        conv81_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv81_1_32_")(relu79_)
        # conv81_1_32_, output shape: {[14,14,32]}

        batchnorm81_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm81_1_32_",)(conv81_1_32_) #TODO: fix_gamma=True

        relu81_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu81_1_32_")(batchnorm81_1_32_)

        conv82_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv82_1_32_")(relu81_1_32_)
        # conv82_1_32_, output shape: {[7,7,32]}

        batchnorm82_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm82_1_32_",)(conv82_1_32_) #TODO: fix_gamma=True

        relu82_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu82_1_32_")(batchnorm82_1_32_)

        conv83_1_32_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv83_1_32_")(relu82_1_32_)
        # conv83_1_32_, output shape: {[7,7,2048]}

        batchnorm83_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm83_1_32_",)(conv83_1_32_) #TODO: fix_gamma=True

        add84_1_ = tf.keras.layers.Add()([batchnorm83_1_1_,  batchnorm83_1_2_,  batchnorm83_1_3_,  batchnorm83_1_4_,  batchnorm83_1_5_,  batchnorm83_1_6_,  batchnorm83_1_7_,  batchnorm83_1_8_,  batchnorm83_1_9_,  batchnorm83_1_10_,  batchnorm83_1_11_,  batchnorm83_1_12_,  batchnorm83_1_13_,  batchnorm83_1_14_,  batchnorm83_1_15_,  batchnorm83_1_16_,  batchnorm83_1_17_,  batchnorm83_1_18_,  batchnorm83_1_19_,  batchnorm83_1_20_,  batchnorm83_1_21_,  batchnorm83_1_22_,  batchnorm83_1_23_,  batchnorm83_1_24_,  batchnorm83_1_25_,  batchnorm83_1_26_,  batchnorm83_1_27_,  batchnorm83_1_28_,  batchnorm83_1_29_,  batchnorm83_1_30_,  batchnorm83_1_31_,  batchnorm83_1_32_])
        # add84_1_, output shape: {[7,7,2048]}

        conv80_2_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(2,2), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv80_2_")(relu79_)
        # conv80_2_, output shape: {[7,7,2048]}

        batchnorm80_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm80_2_",)(conv80_2_) #TODO: fix_gamma=True

        add85_ = tf.keras.layers.Add()([add84_1_,  batchnorm80_2_])
        # add85_, output shape: {[7,7,2048]}

        relu85_ = tf.keras.layers.Activation(activation = "relu", name="relu85_")(add85_)

        conv87_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_1_")(relu85_)
        # conv87_1_1_, output shape: {[7,7,32]}

        batchnorm87_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_1_",)(conv87_1_1_) #TODO: fix_gamma=True

        relu87_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_1_")(batchnorm87_1_1_)

        conv88_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_1_")(relu87_1_1_)
        # conv88_1_1_, output shape: {[7,7,32]}

        batchnorm88_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_1_",)(conv88_1_1_) #TODO: fix_gamma=True

        relu88_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_1_")(batchnorm88_1_1_)

        conv89_1_1_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_1_")(relu88_1_1_)
        # conv89_1_1_, output shape: {[7,7,2048]}

        batchnorm89_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_1_",)(conv89_1_1_) #TODO: fix_gamma=True

        conv87_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_2_")(relu85_)
        # conv87_1_2_, output shape: {[7,7,32]}

        batchnorm87_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_2_",)(conv87_1_2_) #TODO: fix_gamma=True

        relu87_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_2_")(batchnorm87_1_2_)

        conv88_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_2_")(relu87_1_2_)
        # conv88_1_2_, output shape: {[7,7,32]}

        batchnorm88_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_2_",)(conv88_1_2_) #TODO: fix_gamma=True

        relu88_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_2_")(batchnorm88_1_2_)

        conv89_1_2_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_2_")(relu88_1_2_)
        # conv89_1_2_, output shape: {[7,7,2048]}

        batchnorm89_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_2_",)(conv89_1_2_) #TODO: fix_gamma=True

        conv87_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_3_")(relu85_)
        # conv87_1_3_, output shape: {[7,7,32]}

        batchnorm87_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_3_",)(conv87_1_3_) #TODO: fix_gamma=True

        relu87_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_3_")(batchnorm87_1_3_)

        conv88_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_3_")(relu87_1_3_)
        # conv88_1_3_, output shape: {[7,7,32]}

        batchnorm88_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_3_",)(conv88_1_3_) #TODO: fix_gamma=True

        relu88_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_3_")(batchnorm88_1_3_)

        conv89_1_3_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_3_")(relu88_1_3_)
        # conv89_1_3_, output shape: {[7,7,2048]}

        batchnorm89_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_3_",)(conv89_1_3_) #TODO: fix_gamma=True

        conv87_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_4_")(relu85_)
        # conv87_1_4_, output shape: {[7,7,32]}

        batchnorm87_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_4_",)(conv87_1_4_) #TODO: fix_gamma=True

        relu87_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_4_")(batchnorm87_1_4_)

        conv88_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_4_")(relu87_1_4_)
        # conv88_1_4_, output shape: {[7,7,32]}

        batchnorm88_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_4_",)(conv88_1_4_) #TODO: fix_gamma=True

        relu88_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_4_")(batchnorm88_1_4_)

        conv89_1_4_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_4_")(relu88_1_4_)
        # conv89_1_4_, output shape: {[7,7,2048]}

        batchnorm89_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_4_",)(conv89_1_4_) #TODO: fix_gamma=True

        conv87_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_5_")(relu85_)
        # conv87_1_5_, output shape: {[7,7,32]}

        batchnorm87_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_5_",)(conv87_1_5_) #TODO: fix_gamma=True

        relu87_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_5_")(batchnorm87_1_5_)

        conv88_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_5_")(relu87_1_5_)
        # conv88_1_5_, output shape: {[7,7,32]}

        batchnorm88_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_5_",)(conv88_1_5_) #TODO: fix_gamma=True

        relu88_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_5_")(batchnorm88_1_5_)

        conv89_1_5_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_5_")(relu88_1_5_)
        # conv89_1_5_, output shape: {[7,7,2048]}

        batchnorm89_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_5_",)(conv89_1_5_) #TODO: fix_gamma=True

        conv87_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_6_")(relu85_)
        # conv87_1_6_, output shape: {[7,7,32]}

        batchnorm87_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_6_",)(conv87_1_6_) #TODO: fix_gamma=True

        relu87_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_6_")(batchnorm87_1_6_)

        conv88_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_6_")(relu87_1_6_)
        # conv88_1_6_, output shape: {[7,7,32]}

        batchnorm88_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_6_",)(conv88_1_6_) #TODO: fix_gamma=True

        relu88_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_6_")(batchnorm88_1_6_)

        conv89_1_6_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_6_")(relu88_1_6_)
        # conv89_1_6_, output shape: {[7,7,2048]}

        batchnorm89_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_6_",)(conv89_1_6_) #TODO: fix_gamma=True

        conv87_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_7_")(relu85_)
        # conv87_1_7_, output shape: {[7,7,32]}

        batchnorm87_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_7_",)(conv87_1_7_) #TODO: fix_gamma=True

        relu87_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_7_")(batchnorm87_1_7_)

        conv88_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_7_")(relu87_1_7_)
        # conv88_1_7_, output shape: {[7,7,32]}

        batchnorm88_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_7_",)(conv88_1_7_) #TODO: fix_gamma=True

        relu88_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_7_")(batchnorm88_1_7_)

        conv89_1_7_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_7_")(relu88_1_7_)
        # conv89_1_7_, output shape: {[7,7,2048]}

        batchnorm89_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_7_",)(conv89_1_7_) #TODO: fix_gamma=True

        conv87_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_8_")(relu85_)
        # conv87_1_8_, output shape: {[7,7,32]}

        batchnorm87_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_8_",)(conv87_1_8_) #TODO: fix_gamma=True

        relu87_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_8_")(batchnorm87_1_8_)

        conv88_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_8_")(relu87_1_8_)
        # conv88_1_8_, output shape: {[7,7,32]}

        batchnorm88_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_8_",)(conv88_1_8_) #TODO: fix_gamma=True

        relu88_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_8_")(batchnorm88_1_8_)

        conv89_1_8_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_8_")(relu88_1_8_)
        # conv89_1_8_, output shape: {[7,7,2048]}

        batchnorm89_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_8_",)(conv89_1_8_) #TODO: fix_gamma=True

        conv87_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_9_")(relu85_)
        # conv87_1_9_, output shape: {[7,7,32]}

        batchnorm87_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_9_",)(conv87_1_9_) #TODO: fix_gamma=True

        relu87_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_9_")(batchnorm87_1_9_)

        conv88_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_9_")(relu87_1_9_)
        # conv88_1_9_, output shape: {[7,7,32]}

        batchnorm88_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_9_",)(conv88_1_9_) #TODO: fix_gamma=True

        relu88_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_9_")(batchnorm88_1_9_)

        conv89_1_9_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_9_")(relu88_1_9_)
        # conv89_1_9_, output shape: {[7,7,2048]}

        batchnorm89_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_9_",)(conv89_1_9_) #TODO: fix_gamma=True

        conv87_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_10_")(relu85_)
        # conv87_1_10_, output shape: {[7,7,32]}

        batchnorm87_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_10_",)(conv87_1_10_) #TODO: fix_gamma=True

        relu87_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_10_")(batchnorm87_1_10_)

        conv88_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_10_")(relu87_1_10_)
        # conv88_1_10_, output shape: {[7,7,32]}

        batchnorm88_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_10_",)(conv88_1_10_) #TODO: fix_gamma=True

        relu88_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_10_")(batchnorm88_1_10_)

        conv89_1_10_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_10_")(relu88_1_10_)
        # conv89_1_10_, output shape: {[7,7,2048]}

        batchnorm89_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_10_",)(conv89_1_10_) #TODO: fix_gamma=True

        conv87_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_11_")(relu85_)
        # conv87_1_11_, output shape: {[7,7,32]}

        batchnorm87_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_11_",)(conv87_1_11_) #TODO: fix_gamma=True

        relu87_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_11_")(batchnorm87_1_11_)

        conv88_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_11_")(relu87_1_11_)
        # conv88_1_11_, output shape: {[7,7,32]}

        batchnorm88_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_11_",)(conv88_1_11_) #TODO: fix_gamma=True

        relu88_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_11_")(batchnorm88_1_11_)

        conv89_1_11_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_11_")(relu88_1_11_)
        # conv89_1_11_, output shape: {[7,7,2048]}

        batchnorm89_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_11_",)(conv89_1_11_) #TODO: fix_gamma=True

        conv87_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_12_")(relu85_)
        # conv87_1_12_, output shape: {[7,7,32]}

        batchnorm87_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_12_",)(conv87_1_12_) #TODO: fix_gamma=True

        relu87_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_12_")(batchnorm87_1_12_)

        conv88_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_12_")(relu87_1_12_)
        # conv88_1_12_, output shape: {[7,7,32]}

        batchnorm88_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_12_",)(conv88_1_12_) #TODO: fix_gamma=True

        relu88_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_12_")(batchnorm88_1_12_)

        conv89_1_12_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_12_")(relu88_1_12_)
        # conv89_1_12_, output shape: {[7,7,2048]}

        batchnorm89_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_12_",)(conv89_1_12_) #TODO: fix_gamma=True

        conv87_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_13_")(relu85_)
        # conv87_1_13_, output shape: {[7,7,32]}

        batchnorm87_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_13_",)(conv87_1_13_) #TODO: fix_gamma=True

        relu87_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_13_")(batchnorm87_1_13_)

        conv88_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_13_")(relu87_1_13_)
        # conv88_1_13_, output shape: {[7,7,32]}

        batchnorm88_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_13_",)(conv88_1_13_) #TODO: fix_gamma=True

        relu88_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_13_")(batchnorm88_1_13_)

        conv89_1_13_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_13_")(relu88_1_13_)
        # conv89_1_13_, output shape: {[7,7,2048]}

        batchnorm89_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_13_",)(conv89_1_13_) #TODO: fix_gamma=True

        conv87_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_14_")(relu85_)
        # conv87_1_14_, output shape: {[7,7,32]}

        batchnorm87_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_14_",)(conv87_1_14_) #TODO: fix_gamma=True

        relu87_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_14_")(batchnorm87_1_14_)

        conv88_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_14_")(relu87_1_14_)
        # conv88_1_14_, output shape: {[7,7,32]}

        batchnorm88_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_14_",)(conv88_1_14_) #TODO: fix_gamma=True

        relu88_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_14_")(batchnorm88_1_14_)

        conv89_1_14_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_14_")(relu88_1_14_)
        # conv89_1_14_, output shape: {[7,7,2048]}

        batchnorm89_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_14_",)(conv89_1_14_) #TODO: fix_gamma=True

        conv87_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_15_")(relu85_)
        # conv87_1_15_, output shape: {[7,7,32]}

        batchnorm87_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_15_",)(conv87_1_15_) #TODO: fix_gamma=True

        relu87_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_15_")(batchnorm87_1_15_)

        conv88_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_15_")(relu87_1_15_)
        # conv88_1_15_, output shape: {[7,7,32]}

        batchnorm88_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_15_",)(conv88_1_15_) #TODO: fix_gamma=True

        relu88_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_15_")(batchnorm88_1_15_)

        conv89_1_15_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_15_")(relu88_1_15_)
        # conv89_1_15_, output shape: {[7,7,2048]}

        batchnorm89_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_15_",)(conv89_1_15_) #TODO: fix_gamma=True

        conv87_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_16_")(relu85_)
        # conv87_1_16_, output shape: {[7,7,32]}

        batchnorm87_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_16_",)(conv87_1_16_) #TODO: fix_gamma=True

        relu87_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_16_")(batchnorm87_1_16_)

        conv88_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_16_")(relu87_1_16_)
        # conv88_1_16_, output shape: {[7,7,32]}

        batchnorm88_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_16_",)(conv88_1_16_) #TODO: fix_gamma=True

        relu88_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_16_")(batchnorm88_1_16_)

        conv89_1_16_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_16_")(relu88_1_16_)
        # conv89_1_16_, output shape: {[7,7,2048]}

        batchnorm89_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_16_",)(conv89_1_16_) #TODO: fix_gamma=True

        conv87_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_17_")(relu85_)
        # conv87_1_17_, output shape: {[7,7,32]}

        batchnorm87_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_17_",)(conv87_1_17_) #TODO: fix_gamma=True

        relu87_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_17_")(batchnorm87_1_17_)

        conv88_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_17_")(relu87_1_17_)
        # conv88_1_17_, output shape: {[7,7,32]}

        batchnorm88_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_17_",)(conv88_1_17_) #TODO: fix_gamma=True

        relu88_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_17_")(batchnorm88_1_17_)

        conv89_1_17_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_17_")(relu88_1_17_)
        # conv89_1_17_, output shape: {[7,7,2048]}

        batchnorm89_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_17_",)(conv89_1_17_) #TODO: fix_gamma=True

        conv87_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_18_")(relu85_)
        # conv87_1_18_, output shape: {[7,7,32]}

        batchnorm87_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_18_",)(conv87_1_18_) #TODO: fix_gamma=True

        relu87_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_18_")(batchnorm87_1_18_)

        conv88_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_18_")(relu87_1_18_)
        # conv88_1_18_, output shape: {[7,7,32]}

        batchnorm88_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_18_",)(conv88_1_18_) #TODO: fix_gamma=True

        relu88_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_18_")(batchnorm88_1_18_)

        conv89_1_18_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_18_")(relu88_1_18_)
        # conv89_1_18_, output shape: {[7,7,2048]}

        batchnorm89_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_18_",)(conv89_1_18_) #TODO: fix_gamma=True

        conv87_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_19_")(relu85_)
        # conv87_1_19_, output shape: {[7,7,32]}

        batchnorm87_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_19_",)(conv87_1_19_) #TODO: fix_gamma=True

        relu87_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_19_")(batchnorm87_1_19_)

        conv88_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_19_")(relu87_1_19_)
        # conv88_1_19_, output shape: {[7,7,32]}

        batchnorm88_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_19_",)(conv88_1_19_) #TODO: fix_gamma=True

        relu88_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_19_")(batchnorm88_1_19_)

        conv89_1_19_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_19_")(relu88_1_19_)
        # conv89_1_19_, output shape: {[7,7,2048]}

        batchnorm89_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_19_",)(conv89_1_19_) #TODO: fix_gamma=True

        conv87_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_20_")(relu85_)
        # conv87_1_20_, output shape: {[7,7,32]}

        batchnorm87_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_20_",)(conv87_1_20_) #TODO: fix_gamma=True

        relu87_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_20_")(batchnorm87_1_20_)

        conv88_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_20_")(relu87_1_20_)
        # conv88_1_20_, output shape: {[7,7,32]}

        batchnorm88_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_20_",)(conv88_1_20_) #TODO: fix_gamma=True

        relu88_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_20_")(batchnorm88_1_20_)

        conv89_1_20_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_20_")(relu88_1_20_)
        # conv89_1_20_, output shape: {[7,7,2048]}

        batchnorm89_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_20_",)(conv89_1_20_) #TODO: fix_gamma=True

        conv87_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_21_")(relu85_)
        # conv87_1_21_, output shape: {[7,7,32]}

        batchnorm87_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_21_",)(conv87_1_21_) #TODO: fix_gamma=True

        relu87_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_21_")(batchnorm87_1_21_)

        conv88_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_21_")(relu87_1_21_)
        # conv88_1_21_, output shape: {[7,7,32]}

        batchnorm88_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_21_",)(conv88_1_21_) #TODO: fix_gamma=True

        relu88_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_21_")(batchnorm88_1_21_)

        conv89_1_21_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_21_")(relu88_1_21_)
        # conv89_1_21_, output shape: {[7,7,2048]}

        batchnorm89_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_21_",)(conv89_1_21_) #TODO: fix_gamma=True

        conv87_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_22_")(relu85_)
        # conv87_1_22_, output shape: {[7,7,32]}

        batchnorm87_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_22_",)(conv87_1_22_) #TODO: fix_gamma=True

        relu87_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_22_")(batchnorm87_1_22_)

        conv88_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_22_")(relu87_1_22_)
        # conv88_1_22_, output shape: {[7,7,32]}

        batchnorm88_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_22_",)(conv88_1_22_) #TODO: fix_gamma=True

        relu88_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_22_")(batchnorm88_1_22_)

        conv89_1_22_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_22_")(relu88_1_22_)
        # conv89_1_22_, output shape: {[7,7,2048]}

        batchnorm89_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_22_",)(conv89_1_22_) #TODO: fix_gamma=True

        conv87_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_23_")(relu85_)
        # conv87_1_23_, output shape: {[7,7,32]}

        batchnorm87_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_23_",)(conv87_1_23_) #TODO: fix_gamma=True

        relu87_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_23_")(batchnorm87_1_23_)

        conv88_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_23_")(relu87_1_23_)
        # conv88_1_23_, output shape: {[7,7,32]}

        batchnorm88_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_23_",)(conv88_1_23_) #TODO: fix_gamma=True

        relu88_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_23_")(batchnorm88_1_23_)

        conv89_1_23_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_23_")(relu88_1_23_)
        # conv89_1_23_, output shape: {[7,7,2048]}

        batchnorm89_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_23_",)(conv89_1_23_) #TODO: fix_gamma=True

        conv87_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_24_")(relu85_)
        # conv87_1_24_, output shape: {[7,7,32]}

        batchnorm87_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_24_",)(conv87_1_24_) #TODO: fix_gamma=True

        relu87_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_24_")(batchnorm87_1_24_)

        conv88_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_24_")(relu87_1_24_)
        # conv88_1_24_, output shape: {[7,7,32]}

        batchnorm88_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_24_",)(conv88_1_24_) #TODO: fix_gamma=True

        relu88_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_24_")(batchnorm88_1_24_)

        conv89_1_24_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_24_")(relu88_1_24_)
        # conv89_1_24_, output shape: {[7,7,2048]}

        batchnorm89_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_24_",)(conv89_1_24_) #TODO: fix_gamma=True

        conv87_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_25_")(relu85_)
        # conv87_1_25_, output shape: {[7,7,32]}

        batchnorm87_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_25_",)(conv87_1_25_) #TODO: fix_gamma=True

        relu87_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_25_")(batchnorm87_1_25_)

        conv88_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_25_")(relu87_1_25_)
        # conv88_1_25_, output shape: {[7,7,32]}

        batchnorm88_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_25_",)(conv88_1_25_) #TODO: fix_gamma=True

        relu88_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_25_")(batchnorm88_1_25_)

        conv89_1_25_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_25_")(relu88_1_25_)
        # conv89_1_25_, output shape: {[7,7,2048]}

        batchnorm89_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_25_",)(conv89_1_25_) #TODO: fix_gamma=True

        conv87_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_26_")(relu85_)
        # conv87_1_26_, output shape: {[7,7,32]}

        batchnorm87_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_26_",)(conv87_1_26_) #TODO: fix_gamma=True

        relu87_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_26_")(batchnorm87_1_26_)

        conv88_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_26_")(relu87_1_26_)
        # conv88_1_26_, output shape: {[7,7,32]}

        batchnorm88_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_26_",)(conv88_1_26_) #TODO: fix_gamma=True

        relu88_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_26_")(batchnorm88_1_26_)

        conv89_1_26_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_26_")(relu88_1_26_)
        # conv89_1_26_, output shape: {[7,7,2048]}

        batchnorm89_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_26_",)(conv89_1_26_) #TODO: fix_gamma=True

        conv87_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_27_")(relu85_)
        # conv87_1_27_, output shape: {[7,7,32]}

        batchnorm87_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_27_",)(conv87_1_27_) #TODO: fix_gamma=True

        relu87_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_27_")(batchnorm87_1_27_)

        conv88_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_27_")(relu87_1_27_)
        # conv88_1_27_, output shape: {[7,7,32]}

        batchnorm88_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_27_",)(conv88_1_27_) #TODO: fix_gamma=True

        relu88_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_27_")(batchnorm88_1_27_)

        conv89_1_27_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_27_")(relu88_1_27_)
        # conv89_1_27_, output shape: {[7,7,2048]}

        batchnorm89_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_27_",)(conv89_1_27_) #TODO: fix_gamma=True

        conv87_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_28_")(relu85_)
        # conv87_1_28_, output shape: {[7,7,32]}

        batchnorm87_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_28_",)(conv87_1_28_) #TODO: fix_gamma=True

        relu87_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_28_")(batchnorm87_1_28_)

        conv88_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_28_")(relu87_1_28_)
        # conv88_1_28_, output shape: {[7,7,32]}

        batchnorm88_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_28_",)(conv88_1_28_) #TODO: fix_gamma=True

        relu88_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_28_")(batchnorm88_1_28_)

        conv89_1_28_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_28_")(relu88_1_28_)
        # conv89_1_28_, output shape: {[7,7,2048]}

        batchnorm89_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_28_",)(conv89_1_28_) #TODO: fix_gamma=True

        conv87_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_29_")(relu85_)
        # conv87_1_29_, output shape: {[7,7,32]}

        batchnorm87_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_29_",)(conv87_1_29_) #TODO: fix_gamma=True

        relu87_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_29_")(batchnorm87_1_29_)

        conv88_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_29_")(relu87_1_29_)
        # conv88_1_29_, output shape: {[7,7,32]}

        batchnorm88_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_29_",)(conv88_1_29_) #TODO: fix_gamma=True

        relu88_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_29_")(batchnorm88_1_29_)

        conv89_1_29_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_29_")(relu88_1_29_)
        # conv89_1_29_, output shape: {[7,7,2048]}

        batchnorm89_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_29_",)(conv89_1_29_) #TODO: fix_gamma=True

        conv87_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_30_")(relu85_)
        # conv87_1_30_, output shape: {[7,7,32]}

        batchnorm87_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_30_",)(conv87_1_30_) #TODO: fix_gamma=True

        relu87_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_30_")(batchnorm87_1_30_)

        conv88_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_30_")(relu87_1_30_)
        # conv88_1_30_, output shape: {[7,7,32]}

        batchnorm88_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_30_",)(conv88_1_30_) #TODO: fix_gamma=True

        relu88_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_30_")(batchnorm88_1_30_)

        conv89_1_30_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_30_")(relu88_1_30_)
        # conv89_1_30_, output shape: {[7,7,2048]}

        batchnorm89_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_30_",)(conv89_1_30_) #TODO: fix_gamma=True

        conv87_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_31_")(relu85_)
        # conv87_1_31_, output shape: {[7,7,32]}

        batchnorm87_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_31_",)(conv87_1_31_) #TODO: fix_gamma=True

        relu87_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_31_")(batchnorm87_1_31_)

        conv88_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_31_")(relu87_1_31_)
        # conv88_1_31_, output shape: {[7,7,32]}

        batchnorm88_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_31_",)(conv88_1_31_) #TODO: fix_gamma=True

        relu88_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_31_")(batchnorm88_1_31_)

        conv89_1_31_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_31_")(relu88_1_31_)
        # conv89_1_31_, output shape: {[7,7,2048]}

        batchnorm89_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_31_",)(conv89_1_31_) #TODO: fix_gamma=True

        conv87_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv87_1_32_")(relu85_)
        # conv87_1_32_, output shape: {[7,7,32]}

        batchnorm87_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm87_1_32_",)(conv87_1_32_) #TODO: fix_gamma=True

        relu87_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu87_1_32_")(batchnorm87_1_32_)

        conv88_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv88_1_32_")(relu87_1_32_)
        # conv88_1_32_, output shape: {[7,7,32]}

        batchnorm88_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm88_1_32_",)(conv88_1_32_) #TODO: fix_gamma=True

        relu88_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu88_1_32_")(batchnorm88_1_32_)

        conv89_1_32_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv89_1_32_")(relu88_1_32_)
        # conv89_1_32_, output shape: {[7,7,2048]}

        batchnorm89_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm89_1_32_",)(conv89_1_32_) #TODO: fix_gamma=True

        add90_1_ = tf.keras.layers.Add()([batchnorm89_1_1_,  batchnorm89_1_2_,  batchnorm89_1_3_,  batchnorm89_1_4_,  batchnorm89_1_5_,  batchnorm89_1_6_,  batchnorm89_1_7_,  batchnorm89_1_8_,  batchnorm89_1_9_,  batchnorm89_1_10_,  batchnorm89_1_11_,  batchnorm89_1_12_,  batchnorm89_1_13_,  batchnorm89_1_14_,  batchnorm89_1_15_,  batchnorm89_1_16_,  batchnorm89_1_17_,  batchnorm89_1_18_,  batchnorm89_1_19_,  batchnorm89_1_20_,  batchnorm89_1_21_,  batchnorm89_1_22_,  batchnorm89_1_23_,  batchnorm89_1_24_,  batchnorm89_1_25_,  batchnorm89_1_26_,  batchnorm89_1_27_,  batchnorm89_1_28_,  batchnorm89_1_29_,  batchnorm89_1_30_,  batchnorm89_1_31_,  batchnorm89_1_32_])
        # add90_1_, output shape: {[7,7,2048]}

        add91_ = tf.keras.layers.Add()([add90_1_,  relu85_])
        # add91_, output shape: {[7,7,2048]}

        relu91_ = tf.keras.layers.Activation(activation = "relu", name="relu91_")(add91_)

        conv93_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_1_")(relu91_)
        # conv93_1_1_, output shape: {[7,7,32]}

        batchnorm93_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_1_",)(conv93_1_1_) #TODO: fix_gamma=True

        relu93_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_1_")(batchnorm93_1_1_)

        conv94_1_1_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_1_")(relu93_1_1_)
        # conv94_1_1_, output shape: {[7,7,32]}

        batchnorm94_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_1_",)(conv94_1_1_) #TODO: fix_gamma=True

        relu94_1_1_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_1_")(batchnorm94_1_1_)

        conv95_1_1_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_1_")(relu94_1_1_)
        # conv95_1_1_, output shape: {[7,7,2048]}

        batchnorm95_1_1_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_1_",)(conv95_1_1_) #TODO: fix_gamma=True

        conv93_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_2_")(relu91_)
        # conv93_1_2_, output shape: {[7,7,32]}

        batchnorm93_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_2_",)(conv93_1_2_) #TODO: fix_gamma=True

        relu93_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_2_")(batchnorm93_1_2_)

        conv94_1_2_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_2_")(relu93_1_2_)
        # conv94_1_2_, output shape: {[7,7,32]}

        batchnorm94_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_2_",)(conv94_1_2_) #TODO: fix_gamma=True

        relu94_1_2_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_2_")(batchnorm94_1_2_)

        conv95_1_2_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_2_")(relu94_1_2_)
        # conv95_1_2_, output shape: {[7,7,2048]}

        batchnorm95_1_2_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_2_",)(conv95_1_2_) #TODO: fix_gamma=True

        conv93_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_3_")(relu91_)
        # conv93_1_3_, output shape: {[7,7,32]}

        batchnorm93_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_3_",)(conv93_1_3_) #TODO: fix_gamma=True

        relu93_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_3_")(batchnorm93_1_3_)

        conv94_1_3_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_3_")(relu93_1_3_)
        # conv94_1_3_, output shape: {[7,7,32]}

        batchnorm94_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_3_",)(conv94_1_3_) #TODO: fix_gamma=True

        relu94_1_3_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_3_")(batchnorm94_1_3_)

        conv95_1_3_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_3_")(relu94_1_3_)
        # conv95_1_3_, output shape: {[7,7,2048]}

        batchnorm95_1_3_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_3_",)(conv95_1_3_) #TODO: fix_gamma=True

        conv93_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_4_")(relu91_)
        # conv93_1_4_, output shape: {[7,7,32]}

        batchnorm93_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_4_",)(conv93_1_4_) #TODO: fix_gamma=True

        relu93_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_4_")(batchnorm93_1_4_)

        conv94_1_4_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_4_")(relu93_1_4_)
        # conv94_1_4_, output shape: {[7,7,32]}

        batchnorm94_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_4_",)(conv94_1_4_) #TODO: fix_gamma=True

        relu94_1_4_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_4_")(batchnorm94_1_4_)

        conv95_1_4_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_4_")(relu94_1_4_)
        # conv95_1_4_, output shape: {[7,7,2048]}

        batchnorm95_1_4_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_4_",)(conv95_1_4_) #TODO: fix_gamma=True

        conv93_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_5_")(relu91_)
        # conv93_1_5_, output shape: {[7,7,32]}

        batchnorm93_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_5_",)(conv93_1_5_) #TODO: fix_gamma=True

        relu93_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_5_")(batchnorm93_1_5_)

        conv94_1_5_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_5_")(relu93_1_5_)
        # conv94_1_5_, output shape: {[7,7,32]}

        batchnorm94_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_5_",)(conv94_1_5_) #TODO: fix_gamma=True

        relu94_1_5_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_5_")(batchnorm94_1_5_)

        conv95_1_5_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_5_")(relu94_1_5_)
        # conv95_1_5_, output shape: {[7,7,2048]}

        batchnorm95_1_5_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_5_",)(conv95_1_5_) #TODO: fix_gamma=True

        conv93_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_6_")(relu91_)
        # conv93_1_6_, output shape: {[7,7,32]}

        batchnorm93_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_6_",)(conv93_1_6_) #TODO: fix_gamma=True

        relu93_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_6_")(batchnorm93_1_6_)

        conv94_1_6_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_6_")(relu93_1_6_)
        # conv94_1_6_, output shape: {[7,7,32]}

        batchnorm94_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_6_",)(conv94_1_6_) #TODO: fix_gamma=True

        relu94_1_6_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_6_")(batchnorm94_1_6_)

        conv95_1_6_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_6_")(relu94_1_6_)
        # conv95_1_6_, output shape: {[7,7,2048]}

        batchnorm95_1_6_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_6_",)(conv95_1_6_) #TODO: fix_gamma=True

        conv93_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_7_")(relu91_)
        # conv93_1_7_, output shape: {[7,7,32]}

        batchnorm93_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_7_",)(conv93_1_7_) #TODO: fix_gamma=True

        relu93_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_7_")(batchnorm93_1_7_)

        conv94_1_7_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_7_")(relu93_1_7_)
        # conv94_1_7_, output shape: {[7,7,32]}

        batchnorm94_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_7_",)(conv94_1_7_) #TODO: fix_gamma=True

        relu94_1_7_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_7_")(batchnorm94_1_7_)

        conv95_1_7_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_7_")(relu94_1_7_)
        # conv95_1_7_, output shape: {[7,7,2048]}

        batchnorm95_1_7_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_7_",)(conv95_1_7_) #TODO: fix_gamma=True

        conv93_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_8_")(relu91_)
        # conv93_1_8_, output shape: {[7,7,32]}

        batchnorm93_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_8_",)(conv93_1_8_) #TODO: fix_gamma=True

        relu93_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_8_")(batchnorm93_1_8_)

        conv94_1_8_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_8_")(relu93_1_8_)
        # conv94_1_8_, output shape: {[7,7,32]}

        batchnorm94_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_8_",)(conv94_1_8_) #TODO: fix_gamma=True

        relu94_1_8_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_8_")(batchnorm94_1_8_)

        conv95_1_8_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_8_")(relu94_1_8_)
        # conv95_1_8_, output shape: {[7,7,2048]}

        batchnorm95_1_8_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_8_",)(conv95_1_8_) #TODO: fix_gamma=True

        conv93_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_9_")(relu91_)
        # conv93_1_9_, output shape: {[7,7,32]}

        batchnorm93_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_9_",)(conv93_1_9_) #TODO: fix_gamma=True

        relu93_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_9_")(batchnorm93_1_9_)

        conv94_1_9_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_9_")(relu93_1_9_)
        # conv94_1_9_, output shape: {[7,7,32]}

        batchnorm94_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_9_",)(conv94_1_9_) #TODO: fix_gamma=True

        relu94_1_9_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_9_")(batchnorm94_1_9_)

        conv95_1_9_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_9_")(relu94_1_9_)
        # conv95_1_9_, output shape: {[7,7,2048]}

        batchnorm95_1_9_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_9_",)(conv95_1_9_) #TODO: fix_gamma=True

        conv93_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_10_")(relu91_)
        # conv93_1_10_, output shape: {[7,7,32]}

        batchnorm93_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_10_",)(conv93_1_10_) #TODO: fix_gamma=True

        relu93_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_10_")(batchnorm93_1_10_)

        conv94_1_10_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_10_")(relu93_1_10_)
        # conv94_1_10_, output shape: {[7,7,32]}

        batchnorm94_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_10_",)(conv94_1_10_) #TODO: fix_gamma=True

        relu94_1_10_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_10_")(batchnorm94_1_10_)

        conv95_1_10_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_10_")(relu94_1_10_)
        # conv95_1_10_, output shape: {[7,7,2048]}

        batchnorm95_1_10_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_10_",)(conv95_1_10_) #TODO: fix_gamma=True

        conv93_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_11_")(relu91_)
        # conv93_1_11_, output shape: {[7,7,32]}

        batchnorm93_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_11_",)(conv93_1_11_) #TODO: fix_gamma=True

        relu93_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_11_")(batchnorm93_1_11_)

        conv94_1_11_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_11_")(relu93_1_11_)
        # conv94_1_11_, output shape: {[7,7,32]}

        batchnorm94_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_11_",)(conv94_1_11_) #TODO: fix_gamma=True

        relu94_1_11_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_11_")(batchnorm94_1_11_)

        conv95_1_11_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_11_")(relu94_1_11_)
        # conv95_1_11_, output shape: {[7,7,2048]}

        batchnorm95_1_11_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_11_",)(conv95_1_11_) #TODO: fix_gamma=True

        conv93_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_12_")(relu91_)
        # conv93_1_12_, output shape: {[7,7,32]}

        batchnorm93_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_12_",)(conv93_1_12_) #TODO: fix_gamma=True

        relu93_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_12_")(batchnorm93_1_12_)

        conv94_1_12_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_12_")(relu93_1_12_)
        # conv94_1_12_, output shape: {[7,7,32]}

        batchnorm94_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_12_",)(conv94_1_12_) #TODO: fix_gamma=True

        relu94_1_12_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_12_")(batchnorm94_1_12_)

        conv95_1_12_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_12_")(relu94_1_12_)
        # conv95_1_12_, output shape: {[7,7,2048]}

        batchnorm95_1_12_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_12_",)(conv95_1_12_) #TODO: fix_gamma=True

        conv93_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_13_")(relu91_)
        # conv93_1_13_, output shape: {[7,7,32]}

        batchnorm93_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_13_",)(conv93_1_13_) #TODO: fix_gamma=True

        relu93_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_13_")(batchnorm93_1_13_)

        conv94_1_13_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_13_")(relu93_1_13_)
        # conv94_1_13_, output shape: {[7,7,32]}

        batchnorm94_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_13_",)(conv94_1_13_) #TODO: fix_gamma=True

        relu94_1_13_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_13_")(batchnorm94_1_13_)

        conv95_1_13_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_13_")(relu94_1_13_)
        # conv95_1_13_, output shape: {[7,7,2048]}

        batchnorm95_1_13_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_13_",)(conv95_1_13_) #TODO: fix_gamma=True

        conv93_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_14_")(relu91_)
        # conv93_1_14_, output shape: {[7,7,32]}

        batchnorm93_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_14_",)(conv93_1_14_) #TODO: fix_gamma=True

        relu93_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_14_")(batchnorm93_1_14_)

        conv94_1_14_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_14_")(relu93_1_14_)
        # conv94_1_14_, output shape: {[7,7,32]}

        batchnorm94_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_14_",)(conv94_1_14_) #TODO: fix_gamma=True

        relu94_1_14_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_14_")(batchnorm94_1_14_)

        conv95_1_14_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_14_")(relu94_1_14_)
        # conv95_1_14_, output shape: {[7,7,2048]}

        batchnorm95_1_14_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_14_",)(conv95_1_14_) #TODO: fix_gamma=True

        conv93_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_15_")(relu91_)
        # conv93_1_15_, output shape: {[7,7,32]}

        batchnorm93_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_15_",)(conv93_1_15_) #TODO: fix_gamma=True

        relu93_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_15_")(batchnorm93_1_15_)

        conv94_1_15_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_15_")(relu93_1_15_)
        # conv94_1_15_, output shape: {[7,7,32]}

        batchnorm94_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_15_",)(conv94_1_15_) #TODO: fix_gamma=True

        relu94_1_15_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_15_")(batchnorm94_1_15_)

        conv95_1_15_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_15_")(relu94_1_15_)
        # conv95_1_15_, output shape: {[7,7,2048]}

        batchnorm95_1_15_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_15_",)(conv95_1_15_) #TODO: fix_gamma=True

        conv93_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_16_")(relu91_)
        # conv93_1_16_, output shape: {[7,7,32]}

        batchnorm93_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_16_",)(conv93_1_16_) #TODO: fix_gamma=True

        relu93_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_16_")(batchnorm93_1_16_)

        conv94_1_16_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_16_")(relu93_1_16_)
        # conv94_1_16_, output shape: {[7,7,32]}

        batchnorm94_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_16_",)(conv94_1_16_) #TODO: fix_gamma=True

        relu94_1_16_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_16_")(batchnorm94_1_16_)

        conv95_1_16_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_16_")(relu94_1_16_)
        # conv95_1_16_, output shape: {[7,7,2048]}

        batchnorm95_1_16_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_16_",)(conv95_1_16_) #TODO: fix_gamma=True

        conv93_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_17_")(relu91_)
        # conv93_1_17_, output shape: {[7,7,32]}

        batchnorm93_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_17_",)(conv93_1_17_) #TODO: fix_gamma=True

        relu93_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_17_")(batchnorm93_1_17_)

        conv94_1_17_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_17_")(relu93_1_17_)
        # conv94_1_17_, output shape: {[7,7,32]}

        batchnorm94_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_17_",)(conv94_1_17_) #TODO: fix_gamma=True

        relu94_1_17_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_17_")(batchnorm94_1_17_)

        conv95_1_17_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_17_")(relu94_1_17_)
        # conv95_1_17_, output shape: {[7,7,2048]}

        batchnorm95_1_17_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_17_",)(conv95_1_17_) #TODO: fix_gamma=True

        conv93_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_18_")(relu91_)
        # conv93_1_18_, output shape: {[7,7,32]}

        batchnorm93_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_18_",)(conv93_1_18_) #TODO: fix_gamma=True

        relu93_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_18_")(batchnorm93_1_18_)

        conv94_1_18_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_18_")(relu93_1_18_)
        # conv94_1_18_, output shape: {[7,7,32]}

        batchnorm94_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_18_",)(conv94_1_18_) #TODO: fix_gamma=True

        relu94_1_18_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_18_")(batchnorm94_1_18_)

        conv95_1_18_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_18_")(relu94_1_18_)
        # conv95_1_18_, output shape: {[7,7,2048]}

        batchnorm95_1_18_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_18_",)(conv95_1_18_) #TODO: fix_gamma=True

        conv93_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_19_")(relu91_)
        # conv93_1_19_, output shape: {[7,7,32]}

        batchnorm93_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_19_",)(conv93_1_19_) #TODO: fix_gamma=True

        relu93_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_19_")(batchnorm93_1_19_)

        conv94_1_19_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_19_")(relu93_1_19_)
        # conv94_1_19_, output shape: {[7,7,32]}

        batchnorm94_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_19_",)(conv94_1_19_) #TODO: fix_gamma=True

        relu94_1_19_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_19_")(batchnorm94_1_19_)

        conv95_1_19_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_19_")(relu94_1_19_)
        # conv95_1_19_, output shape: {[7,7,2048]}

        batchnorm95_1_19_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_19_",)(conv95_1_19_) #TODO: fix_gamma=True

        conv93_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_20_")(relu91_)
        # conv93_1_20_, output shape: {[7,7,32]}

        batchnorm93_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_20_",)(conv93_1_20_) #TODO: fix_gamma=True

        relu93_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_20_")(batchnorm93_1_20_)

        conv94_1_20_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_20_")(relu93_1_20_)
        # conv94_1_20_, output shape: {[7,7,32]}

        batchnorm94_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_20_",)(conv94_1_20_) #TODO: fix_gamma=True

        relu94_1_20_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_20_")(batchnorm94_1_20_)

        conv95_1_20_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_20_")(relu94_1_20_)
        # conv95_1_20_, output shape: {[7,7,2048]}

        batchnorm95_1_20_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_20_",)(conv95_1_20_) #TODO: fix_gamma=True

        conv93_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_21_")(relu91_)
        # conv93_1_21_, output shape: {[7,7,32]}

        batchnorm93_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_21_",)(conv93_1_21_) #TODO: fix_gamma=True

        relu93_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_21_")(batchnorm93_1_21_)

        conv94_1_21_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_21_")(relu93_1_21_)
        # conv94_1_21_, output shape: {[7,7,32]}

        batchnorm94_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_21_",)(conv94_1_21_) #TODO: fix_gamma=True

        relu94_1_21_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_21_")(batchnorm94_1_21_)

        conv95_1_21_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_21_")(relu94_1_21_)
        # conv95_1_21_, output shape: {[7,7,2048]}

        batchnorm95_1_21_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_21_",)(conv95_1_21_) #TODO: fix_gamma=True

        conv93_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_22_")(relu91_)
        # conv93_1_22_, output shape: {[7,7,32]}

        batchnorm93_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_22_",)(conv93_1_22_) #TODO: fix_gamma=True

        relu93_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_22_")(batchnorm93_1_22_)

        conv94_1_22_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_22_")(relu93_1_22_)
        # conv94_1_22_, output shape: {[7,7,32]}

        batchnorm94_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_22_",)(conv94_1_22_) #TODO: fix_gamma=True

        relu94_1_22_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_22_")(batchnorm94_1_22_)

        conv95_1_22_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_22_")(relu94_1_22_)
        # conv95_1_22_, output shape: {[7,7,2048]}

        batchnorm95_1_22_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_22_",)(conv95_1_22_) #TODO: fix_gamma=True

        conv93_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_23_")(relu91_)
        # conv93_1_23_, output shape: {[7,7,32]}

        batchnorm93_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_23_",)(conv93_1_23_) #TODO: fix_gamma=True

        relu93_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_23_")(batchnorm93_1_23_)

        conv94_1_23_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_23_")(relu93_1_23_)
        # conv94_1_23_, output shape: {[7,7,32]}

        batchnorm94_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_23_",)(conv94_1_23_) #TODO: fix_gamma=True

        relu94_1_23_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_23_")(batchnorm94_1_23_)

        conv95_1_23_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_23_")(relu94_1_23_)
        # conv95_1_23_, output shape: {[7,7,2048]}

        batchnorm95_1_23_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_23_",)(conv95_1_23_) #TODO: fix_gamma=True

        conv93_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_24_")(relu91_)
        # conv93_1_24_, output shape: {[7,7,32]}

        batchnorm93_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_24_",)(conv93_1_24_) #TODO: fix_gamma=True

        relu93_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_24_")(batchnorm93_1_24_)

        conv94_1_24_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_24_")(relu93_1_24_)
        # conv94_1_24_, output shape: {[7,7,32]}

        batchnorm94_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_24_",)(conv94_1_24_) #TODO: fix_gamma=True

        relu94_1_24_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_24_")(batchnorm94_1_24_)

        conv95_1_24_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_24_")(relu94_1_24_)
        # conv95_1_24_, output shape: {[7,7,2048]}

        batchnorm95_1_24_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_24_",)(conv95_1_24_) #TODO: fix_gamma=True

        conv93_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_25_")(relu91_)
        # conv93_1_25_, output shape: {[7,7,32]}

        batchnorm93_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_25_",)(conv93_1_25_) #TODO: fix_gamma=True

        relu93_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_25_")(batchnorm93_1_25_)

        conv94_1_25_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_25_")(relu93_1_25_)
        # conv94_1_25_, output shape: {[7,7,32]}

        batchnorm94_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_25_",)(conv94_1_25_) #TODO: fix_gamma=True

        relu94_1_25_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_25_")(batchnorm94_1_25_)

        conv95_1_25_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_25_")(relu94_1_25_)
        # conv95_1_25_, output shape: {[7,7,2048]}

        batchnorm95_1_25_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_25_",)(conv95_1_25_) #TODO: fix_gamma=True

        conv93_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_26_")(relu91_)
        # conv93_1_26_, output shape: {[7,7,32]}

        batchnorm93_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_26_",)(conv93_1_26_) #TODO: fix_gamma=True

        relu93_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_26_")(batchnorm93_1_26_)

        conv94_1_26_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_26_")(relu93_1_26_)
        # conv94_1_26_, output shape: {[7,7,32]}

        batchnorm94_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_26_",)(conv94_1_26_) #TODO: fix_gamma=True

        relu94_1_26_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_26_")(batchnorm94_1_26_)

        conv95_1_26_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_26_")(relu94_1_26_)
        # conv95_1_26_, output shape: {[7,7,2048]}

        batchnorm95_1_26_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_26_",)(conv95_1_26_) #TODO: fix_gamma=True

        conv93_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_27_")(relu91_)
        # conv93_1_27_, output shape: {[7,7,32]}

        batchnorm93_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_27_",)(conv93_1_27_) #TODO: fix_gamma=True

        relu93_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_27_")(batchnorm93_1_27_)

        conv94_1_27_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_27_")(relu93_1_27_)
        # conv94_1_27_, output shape: {[7,7,32]}

        batchnorm94_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_27_",)(conv94_1_27_) #TODO: fix_gamma=True

        relu94_1_27_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_27_")(batchnorm94_1_27_)

        conv95_1_27_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_27_")(relu94_1_27_)
        # conv95_1_27_, output shape: {[7,7,2048]}

        batchnorm95_1_27_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_27_",)(conv95_1_27_) #TODO: fix_gamma=True

        conv93_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_28_")(relu91_)
        # conv93_1_28_, output shape: {[7,7,32]}

        batchnorm93_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_28_",)(conv93_1_28_) #TODO: fix_gamma=True

        relu93_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_28_")(batchnorm93_1_28_)

        conv94_1_28_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_28_")(relu93_1_28_)
        # conv94_1_28_, output shape: {[7,7,32]}

        batchnorm94_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_28_",)(conv94_1_28_) #TODO: fix_gamma=True

        relu94_1_28_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_28_")(batchnorm94_1_28_)

        conv95_1_28_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_28_")(relu94_1_28_)
        # conv95_1_28_, output shape: {[7,7,2048]}

        batchnorm95_1_28_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_28_",)(conv95_1_28_) #TODO: fix_gamma=True

        conv93_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_29_")(relu91_)
        # conv93_1_29_, output shape: {[7,7,32]}

        batchnorm93_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_29_",)(conv93_1_29_) #TODO: fix_gamma=True

        relu93_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_29_")(batchnorm93_1_29_)

        conv94_1_29_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_29_")(relu93_1_29_)
        # conv94_1_29_, output shape: {[7,7,32]}

        batchnorm94_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_29_",)(conv94_1_29_) #TODO: fix_gamma=True

        relu94_1_29_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_29_")(batchnorm94_1_29_)

        conv95_1_29_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_29_")(relu94_1_29_)
        # conv95_1_29_, output shape: {[7,7,2048]}

        batchnorm95_1_29_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_29_",)(conv95_1_29_) #TODO: fix_gamma=True

        conv93_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_30_")(relu91_)
        # conv93_1_30_, output shape: {[7,7,32]}

        batchnorm93_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_30_",)(conv93_1_30_) #TODO: fix_gamma=True

        relu93_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_30_")(batchnorm93_1_30_)

        conv94_1_30_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_30_")(relu93_1_30_)
        # conv94_1_30_, output shape: {[7,7,32]}

        batchnorm94_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_30_",)(conv94_1_30_) #TODO: fix_gamma=True

        relu94_1_30_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_30_")(batchnorm94_1_30_)

        conv95_1_30_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_30_")(relu94_1_30_)
        # conv95_1_30_, output shape: {[7,7,2048]}

        batchnorm95_1_30_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_30_",)(conv95_1_30_) #TODO: fix_gamma=True

        conv93_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_31_")(relu91_)
        # conv93_1_31_, output shape: {[7,7,32]}

        batchnorm93_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_31_",)(conv93_1_31_) #TODO: fix_gamma=True

        relu93_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_31_")(batchnorm93_1_31_)

        conv94_1_31_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_31_")(relu93_1_31_)
        # conv94_1_31_, output shape: {[7,7,32]}

        batchnorm94_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_31_",)(conv94_1_31_) #TODO: fix_gamma=True

        relu94_1_31_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_31_")(batchnorm94_1_31_)

        conv95_1_31_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_31_")(relu94_1_31_)
        # conv95_1_31_, output shape: {[7,7,2048]}

        batchnorm95_1_31_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_31_",)(conv95_1_31_) #TODO: fix_gamma=True

        conv93_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv93_1_32_")(relu91_)
        # conv93_1_32_, output shape: {[7,7,32]}

        batchnorm93_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm93_1_32_",)(conv93_1_32_) #TODO: fix_gamma=True

        relu93_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu93_1_32_")(batchnorm93_1_32_)

        conv94_1_32_ = tf.keras.layers.Conv2D(32, 
                                                 kernel_size=(3,3), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv94_1_32_")(relu93_1_32_)
        # conv94_1_32_, output shape: {[7,7,32]}

        batchnorm94_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm94_1_32_",)(conv94_1_32_) #TODO: fix_gamma=True

        relu94_1_32_ = tf.keras.layers.Activation(activation = "relu", name="relu94_1_32_")(batchnorm94_1_32_)

        conv95_1_32_ = tf.keras.layers.Conv2D(2048, 
                                                 kernel_size=(1,1), 
                                                 strides=(1,1), 
                                                 use_bias=True,
                                                 padding="same",
                                                 kernel_regularizer=self._regularizer_, 
                                                 kernel_constraint=self._weight_constraint_, 
                                                 name="conv95_1_32_")(relu94_1_32_)
        # conv95_1_32_, output shape: {[7,7,2048]}

        batchnorm95_1_32_ = tf.keras.layers.BatchNormalization(beta_regularizer=self._regularizer_,
                                                             gamma_regularizer=self._regularizer_,
                                                             beta_constraint=self._weight_constraint_,
                                                             gamma_constraint=self._weight_constraint_,
                                                             name="batchnorm95_1_32_",)(conv95_1_32_) #TODO: fix_gamma=True

        add96_1_ = tf.keras.layers.Add()([batchnorm95_1_1_,  batchnorm95_1_2_,  batchnorm95_1_3_,  batchnorm95_1_4_,  batchnorm95_1_5_,  batchnorm95_1_6_,  batchnorm95_1_7_,  batchnorm95_1_8_,  batchnorm95_1_9_,  batchnorm95_1_10_,  batchnorm95_1_11_,  batchnorm95_1_12_,  batchnorm95_1_13_,  batchnorm95_1_14_,  batchnorm95_1_15_,  batchnorm95_1_16_,  batchnorm95_1_17_,  batchnorm95_1_18_,  batchnorm95_1_19_,  batchnorm95_1_20_,  batchnorm95_1_21_,  batchnorm95_1_22_,  batchnorm95_1_23_,  batchnorm95_1_24_,  batchnorm95_1_25_,  batchnorm95_1_26_,  batchnorm95_1_27_,  batchnorm95_1_28_,  batchnorm95_1_29_,  batchnorm95_1_30_,  batchnorm95_1_31_,  batchnorm95_1_32_])
        # add96_1_, output shape: {[7,7,2048]}

        add97_ = tf.keras.layers.Add()([add96_1_,  relu91_])
        # add97_, output shape: {[7,7,2048]}

        relu97_ = tf.keras.layers.Activation(activation = "relu", name="relu97_")(add97_)

        #Why is this intendation neccessary? In the generated file this will have the same intendation as the 
        #avg version above (and as the other layers, and how it has to be)
        globalpooling97_ = tf.keras.layers.GlobalMaxPool2D(data_format = "channels_last", name="globalpooling97_")(relu97_)
        # globalpooling97_, output shape: {[1,1,2048]}


        fc97_ = tf.keras.layers.Dense(1000,
                                                use_bias=False,
                                                kernel_regularizer=self._regularizer_, 
                                                kernel_constraint=self._weight_constraint_, 
                                                name="fc97_")(globalpooling97_)

        #TODO check for axis=1 and maybe change to axis=3 cause of reverse tf 
        softmax97_ = tf.keras.layers.Softmax(name="softmax97_")(fc97_)

        #Just an "Identity" layer with the appropriate output name, as if softmax is required it is generated from the Softmax.ftl template        
        predictions_ = tf.keras.layers.Lambda(lambda x: x, name="predictions_")(softmax97_)
        output_names.append("predictions_")
        


#************* End Stream 0*******************************        
                 
        self.model = tf.keras.models.Model(inputs=input_tensors, outputs=[predictions_])
             
        for i, node in enumerate(self.model.outputs):
            tf.identity(node, name="output_" + str(i))
