 configuration TopoQNet {
     agent_name : "TopoAgent"

     context : cpu

     learning_method : reinforcement

     rl_algorithm : ddpg

     self_play : no

     critic : topology.agent.network.topoCritic

     environment : ros_interface {
         state: "/topo/state"
         terminal: "/topo/terminal"
         action: "/topo/step"
         reset: "/topo/reset"
         reward: "/topo/reward"
     }

     num_episodes : 20
     discount_factor : 0.995
     num_max_steps : 1
     training_interval : 1
     start_training_at : 0

     evaluation_samples : 3

     use_double_dqn : false

     snapshot_interval : 100

     replay_memory : buffer{
         memory_size : 1000000
         sample_size : 64
     }

     strategy : ornstein_uhlenbeck{
         epsilon : 1.0
         min_epsilon : 0.01
         epsilon_decay_method: linear
         epsilon_decay : 0.005
         epsilon_decay_start: 10
         epsilon_decay_per_step: true
         theta: (0.1,0.1,0.1,0.1,0.1,0.1,0.1)
         mu: (0.04,0.0,0.0,0.0,0.0,0.0,0.0)
         sigma: (0.2,0.2,0.2,0.2,0.2,0.2,0.2)
     }

     actor_optimizer: adam {
         learning_rate: 0.0001
     }

     critic_optimizer: adam {
         learning_rate: 0.001
     }
 }
