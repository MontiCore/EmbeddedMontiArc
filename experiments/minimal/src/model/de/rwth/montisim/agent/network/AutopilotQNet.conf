configuration AutopilotQNet {
	context: gpu

	learning_method: reinforcement

	agent_name: "AutopilotAgent"

	rl_algorithm: td3

	policy_delay: 5

	self_play: yes

	critic: de.rwth.montisim.agent.network.autopilotCritic

	environment: ros_interface {
		state: "/sim/state"
		terminal: "/sim/terminal"
		action: "/sim/step"
		reset: "/sim/reset"
		reward: "/sim/reward"
	}

	num_episodes: 250
	discount_factor: 0.99
	num_max_steps: 5000
	training_interval: 1
	start_training_at: 100

	evaluation_samples: 3
	soft_target_update_rate: 0.004

	snapshot_interval: 10

	replay_memory: buffer {
		memory_size: 100000
		sample_size: 400
	}

	strategy: ornstein_uhlenbeck {
		/* from epsgreedy */
		epsilon: 1.0
		min_epsilon: 0.0
		epsilon_decay_method: linear
		epsilon_decay: 0.0005
		epsilon_decay_start: 0
		epsilon_decay_per_step: false

		/* from gaussian */
         theta: (0.15, 0.15, 0.15)
         mu: (0.0, 0.0, 0.0)
         sigma: (0.2, 0.2, 0.2)

	}

	actor_optimizer: adam {
		learning_rate: 0.0001
	}

	critic_optimizer: adam {
		learning_rate: 0.001
	}
}
