/* (c) https://github.com/MontiCore/monticore */
configuration LanderActor {
    agent_name: "LanderActor"
    
    context: gpu

    learning_method: reinforcement
    rl_algorithm: ddpg-algorithm
    critic: lander.agent.landerCritic

    environment: gym { name: "LunarLanderContinuous-v2" }

    discount_factor: 0.99

    num_episodes: 4000
    start_training_at: 1

    num_max_steps: 10000
    training_interval: 1

    snapshot_interval: 250
    evaluation_samples: 100

    soft_target_update_rate: 0.001

    replay_memory: buffer{
        memory_size : 1000000
        sample_size : 64
    }

    strategy : ornstein_uhlenbeck {
        epsilon : 1.0
        min_epsilon : 0.025
        epsilon_decay_method: linear
        epsilon_decay_start: 200
        epsilon_decay : 0.0005
        mu: (0, 0)
        theta: (0.15, 0.15)
        sigma: (0.2, 0.2)
    }

    actor_optimizer : adam {
        learning_rate : 0.0001
    }

    critic_optimizer : adam {
        learning_rate : 0.001
    }
}
