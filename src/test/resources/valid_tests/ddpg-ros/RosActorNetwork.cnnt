/* (c) https://github.com/MontiCore/monticore */
configuration RosActorNetwork {
    learning_method : reinforcement

    rl_algorithm : ddpg-algorithm

    critic : comp.rosCriticNetwork

    environment : ros_interface {
        state_topic : "/environment/state"
        action_topic : "/environment/action"
        reset_topic : "/environment/reset"
        reward_topic: "/environment/reward"
    }

    agent_name : "ddpg-agent"

    num_episodes : 2500
    discount_factor : 0.9998
    num_max_steps : 2000
    training_interval : 1

    start_training_at: 50

    soft_target_update_rate: 0.001

    snapshot_interval: 500
    evaluation_samples: 1000

    replay_memory : buffer{
        memory_size : 1500000
        sample_size : 128
    }

    strategy : ornstein_uhlenbeck{
        epsilon: 1.0
        min_epsilon: 0.001
        epsilon_decay_method: linear
        epsilon_decay : 0.0001
        epsilon_decay_start: 50
        mu: (0.0, 0.1, 0.3)
        theta: (0.5, 0.0, 0.8)
        sigma: (0.3, 0.6, -0.9)
    }

    actor_optimizer : adam{
        learning_rate : 0.001
        learning_rate_minimum : 0.00001
    }

    critic_optimizer : rmsprop{
        learning_rate : 0.001
        learning_rate_minimum : 0.00001
        weight_decay : 0.01
        learning_rate_decay : 0.9
        learning_rate_policy : step
        step_size : 1000
        rescale_grad : 1.1
        clip_gradient : 10
        gamma1 : 0.9
        gamma2 : 0.9
        epsilon : 0.000001
        centered : true
        clip_weights : 10
    }
}
