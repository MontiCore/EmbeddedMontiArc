/* (c) https://github.com/MontiCore/monticore */
configuration RosActorNetwork {
    learning_method: reinforcement

    rl_algorithm: ddpg

    critic: comp.rosCriticNetwork

    environment: ros_interface {
        state: "/environment/state"
        action: "/environment/action"
        reset: "/environment/reset"
        reward: "/environment/reward"
    }

    agent_name: "ddpg-agent"

    num_episodes: 2500
    discount_factor: 0.9998
    num_max_steps: 2000
    training_interval: 1

    start_training_at: 50

    soft_target_update_rate: 0.001

    snapshot_interval: 500
    evaluation_samples: 1000

    replay_memory: buffer{
        memory_size: 1500000
        sample_size: 128
    }

    strategy: ornstein_uhlenbeck{
        epsilon: 1.0
        min_epsilon: 0.001
        epsilon_decay_method: linear
        epsilon_decay: 0.0001
        epsilon_decay_start: 50
        mu: (0.0, 0.1, 0.3)
        theta: (0.5, 0.0, 0.8)
        sigma: (0.3, 0.6, -0.9)
    }

    actor_optimizer: adam{
        learning_rate: 0.001
        learning_rate_minimum: 0.00001
    }

    critic_optimizer: rmsprop{
        learning_rate: 0.001
        learning_rate_minimum: 0.00001
        weight_decay: 0.01
        learning_rate_decay: 0.9
        learning_rate_policy: step
        step_size: 1000
        rescale_grad: 1.1
        clip_gradient: 10.0
        gamma1: 0.9
        gamma2: 0.9
        epsilon: 0.000001
        centered: true
        clip_weights: 10.0
    }
}