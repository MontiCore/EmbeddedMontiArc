/* (c) https://github.com/MontiCore/monticore */
configuration TorcsActor {
    context : gpu

    learning_method : reinforcement

    agent_name: "TorcsAgent"

    rl_algorithm: td3

    policy_noise: 0.2
    noise_clip: 0.5
    policy_delay: 2

    critic: torcs.agent.network.torcsCritic

    environment : ros_interface {
        state: "/torcs/state"
        terminal: "/torcs/terminal"
        action: "/torcs/step"
        reset: "/torcs/reset"
    }

    reward: torcs.agent.network.reward

    num_episodes : 3500
    discount_factor : 0.99
    num_max_steps : 900000
    training_interval : 1
    start_training_at: 0

    evaluation_samples: 1
    soft_target_update_rate: 0.005

    snapshot_interval : 150

    replay_memory : buffer{
        memory_size : 120000
        sample_size : 100
    }

    strategy : ornstein_uhlenbeck{
        epsilon : 1.0
        min_epsilon : 0.0001
        epsilon_decay_method: linear
        epsilon_decay : 0.000008
        epsilon_decay_start: 10
        epsilon_decay_per_step: true
        theta: (0.6, 1.0, 1.0)
        mu: (0.0, 0.0, -1.2)
        sigma: (0.3, 0.2, 0.05)
    }

    actor_optimizer: adam {
        learning_rate: 0.001
    }

    critic_optimizer: adam {
        learning_rate: 0.001
    }
}