architecture ShowAttendTell(max_length=25, img_height=224, img_width=224, img_channels=3){
    def input Z(0:255)^{img_channels, img_height, img_width} images
    def output Z(0:37758)^{1} target[25]

    layer LSTM(units=512) decoder;

    layer FullyConnected(units = 256, flatten=false) features;
    layer FullyConnected(units = 1, flatten=false) attention;

    0 -> target[0];

    images ->
    Convolution(kernel=(7,7), channels=128, stride=(7,7), padding="valid") ->
    Convolution(kernel=(4,4), channels=128, stride=(4,4), padding="valid") ->
    Reshape(shape=(64, 128)) ->
    features;

    timed <t> GreedySearch(max_length=max_length){
    (
       (
           (
             features.output ->
             FullyConnected(units=512, flatten=false)
           |
             decoder.state[0] ->
             FullyConnected(units=512, flatten=false)
           ) ->
           BroadcastAdd() ->
           Tanh() ->
           FullyConnected(units=1, flatten=false) ->
           Softmax(axis=0) ->
           attention
       |
           features.output
       )->
       BroadcastMultiply() ->
       ReduceSum(axis=0) ->
       ExpandDims(axis=0)
       |
       target[t-1] ->
       Embedding(output_dim=256)
    ) ->
    Concatenate(axis=1) ->
    decoder ->
    FullyConnected(units=37758) ->
    Tanh() ->
    Dropout(p=0.25) ->
    Softmax() ->
    ArgMax() ->
    target[t]
    };

 }

