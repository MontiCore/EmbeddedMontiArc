configuration TurtlebotQNet {
    agent_name: "TurtlebotAgent"

    context: cpu

    learning_method: reinforcement
    rl_algorithm: dqn
    self_play: yes

    environment: ros_interface {
        state: "/preprocessor/state"
        terminal: "/preprocessor/terminal"
        reward:"/preprocessor/reward"
        action: "/gazebo/step"
        reset: "/gazebo/reset"
    }

    discount_factor: 0.99

    num_episodes: 200
    start_training_at: 1
    
    num_max_steps: 6000
    training_interval: 1 //training start bei online turtlebot is 64

    use_fix_target_network: true
    target_network_update_interval: 2000
    use_double_dqn: true

    snapshot_interval: 1000
    evaluation_samples: 50

    loss: l1

    replay_memory: buffer {
        memory_size : 1000000
        sample_size : 64
    }

    strategy : epsgreedy {
        epsilon : 0.99
        min_epsilon : 0.05
        epsilon_decay_method: linear
        epsilon_decay : 0.0001
    }

    optimizer : rmsprop {
        learning_rate : 0.001 // online is 0.00025
        epsilon : 0.000001
    }

}