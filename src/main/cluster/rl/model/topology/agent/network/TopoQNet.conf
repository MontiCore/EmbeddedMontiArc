 configuration TopoQNet {
     agent_name : "TopoAgent"

     context: cpu

     learning_method : reinforcement

     self_play : no

     environment : ros_interface {
         state: "/topo/state"
         terminal: "/topo/terminal"
         action: "/topo/step"
         reset: "/topo/reset"
         reward: "/topo/reward"
     }

     num_episodes : 2
     discount_factor : 0.995
     num_max_steps : 900
     training_interval : 1
     start_training_at : 1

     evaluation_samples: 10

     use_fix_target_network: true
     target_network_update_interval: 50
     use_double_dqn: false

     loss: huber

     snapshot_interval : 100

     replay_memory : buffer{
         memory_size : 1000000
         sample_size : 64
     }

     strategy : epsgreedy{
         epsilon : 1.0
         min_epsilon : 0.01
         epsilon_decay_method: linear
         epsilon_decay : 0.005
         epsilon_decay_start: 10
         epsilon_decay_per_step: true
     }

     optimizer: rmsprop {
         learning_rate: 0.00025
     }
 }
