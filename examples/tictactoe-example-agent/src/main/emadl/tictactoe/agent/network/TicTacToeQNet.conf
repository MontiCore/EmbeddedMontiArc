configuration TicTacToeQNet {
    agent_name: "TicTacToeAgent"
    
    context: cpu

    learning_method: reinforcement
    rl_algorithm: dqn
    self_play: yes

    environment: ros_interface { 
        state: "/gdl/tictactoe/trainingAgent/state"
        terminal: "/gdl/tictactoe/trainingAgent/terminal"
        reward: "/gdl/tictactoe/trainingAgent/reward"
        action: "/gdl/tictactoe/trainingAgent/action"
        reset: "/gdl/tictactoe/trainingAgent/reset"
    }
    
    discount_factor: 0.99

    num_episodes: 100000
    start_training_at: 1
    
    num_max_steps: 100
    training_interval: 1

    use_fix_target_network: true
    target_network_update_interval: 100
    use_double_dqn: true

    snapshot_interval: 1000
    evaluation_samples: 50

    loss: l1

    replay_memory: buffer {
        memory_size : 100000
        sample_size : 150
    }

    strategy : epsgreedy {
        epsilon : 0.8
        min_epsilon : 0.1
        epsilon_decay_method: linear
        epsilon_decay : 0.00000875
    }

    optimizer : adam {
        learning_rate : 0.001
    }
    
}
