configuration TicTacToeQNet {
    agent_name: "TicTacToeAgent"
    
    context: cpu

    learning_method: reinforcement
    rl_algorithm: dqn
    self_play: yes

    environment: ros_interface { 
        state: "/gdl/tictactoe/trainingAgent/state"
        terminal: "/gdl/tictactoe/trainingAgent/terminal"
        reward: "/gdl/tictactoe/trainingAgent/reward"
        action: "/gdl/tictactoe/trainingAgent/action"
        reset: "/gdl/tictactoe/trainingAgent/reset"
    }
    
    discount_factor: 0.99

    num_episodes: 20000
    start_training_at: 1
    
    num_max_steps: 100
    training_interval: 10

    use_fix_target_network: true
    target_network_update_interval: 100
    use_double_dqn: true

    snapshot_interval: 200
    evaluation_samples: 20

    loss: l1

    replay_memory: buffer {
        memory_size : 50000
        sample_size : 1
    }

    strategy : epsgreedy {
        epsilon : 0.99
        min_epsilon : 0.01
        epsilon_decay_method: linear
        epsilon_decay : 0.0001
    }

    optimizer : rmsprop {
        learning_rate : 0.001
        epsilon : 0.000001
    }
    
}
