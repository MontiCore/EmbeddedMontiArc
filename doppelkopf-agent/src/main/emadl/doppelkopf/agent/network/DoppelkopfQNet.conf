configuration DoppelkopfQNet {
    agent_name: "DoppelkopfAgent"
    
    context: cpu

    learning_method: reinforcement
    rl_algorithm: dqn
    self_play: yes

    environment: ros_interface { 
        state: "/gdl/doppelkopf/trainingAgent/state"
        terminal: "/gdl/doppelkopf/trainingAgent/terminal"
        reward: "/gdl/doppelkopf/trainingAgent/reward"
        action: "/gdl/doppelkopf/trainingAgent/action"
        reset: "/gdl/doppelkopf/trainingAgent/reset"
    }
    
    discount_factor: 0.99

    num_episodes: 3000
    start_training_at: 1
    
    num_max_steps: 100
    training_interval: 1

    use_fix_target_network: true
    target_network_update_interval: 2000
    use_double_dqn: true

    snapshot_interval: 100
    evaluation_samples: 10

    loss: l1

    replay_memory: buffer {
        memory_size : 20000
        sample_size : 150
    }

    strategy : epsgreedy {
        epsilon : 0.8
        min_epsilon : 0.2
        epsilon_decay_method: linear
        epsilon_decay : 0.0002
    }

    optimizer : adam {
        learning_rate : 0.0015
    }
    
}
