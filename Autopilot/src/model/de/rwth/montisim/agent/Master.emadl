package de.rwth.montisim.agent;

import de.rwth.montisim.agent.network.AutopilotQNet;

component Master<N1 total_vehicles, N1 training_vehicles=5> {
    ports
        in Q^{total_vehicles, 25} state,
        in N self_index,
        out Q(-1:1)^{3} action,

	implementation Math {
		/* Remove self from state */
		Q^{(total_vehicles-1), 25} filtered_state = zeros((total_vehicles-1), 25); // does this work?
		for i=1:(self_index-1)
			filtered_state(i) = state(i);
		end
		for i=(self_index+1):total_vehicles
			filtered_state(i-1) = state(i);
		end

		/* Calculate positional distance of vehicles to self */
		Q^{(total_vehicles-1), 2} distances = zeros((total_vehicles-1), 2); // index : distance
		Q^{25} self_state = state(self_index);
		// See https://git.rwth-aachen.de/monticore/EmbeddedMontiArc/applications/reinforcement_learning/coopmontisimautopilot
		Q self_positionX = self_state(22);
		Q self_positionY = self_state(23);
		for i=1:(total_vehicles-1)
			Q^{25} other_state = filtered_state(i);
			Q other_positionX = other_state(22);
			Q other_positionY = other_state(23);
			Q distance = ((self_positionX - other_positionX) ^ 2 + (self_positionY - other_positionY) ^ 2) ^ 0.5;
			distances(i, 1) = i;
			distances(i, 2) = distance;
		end

		/* Sort the filtered_state by distances */
		Q^{(total_vehicles-1), 2} sorted = sortrows(distances, 2);

		/* Get training_vehicles nearest vehicles */
		static Q^{training_vehicles, 26} training_state = zeros(training_vehicles, 26);
		N upperbound = min(total_vehicles, training_vehicles);
		for i=1:upperbound
			// Copy state for vehicle
			for j=1:25
				training_state(i,j) = filtered_state(sorted(i), j);
			end
			training_state(i,26) = 1; // vehicle exists
		end
		if training_vehicles > total_vehicles
			for i=(total_vehicles+1):training_vehicles
				training_state(i,26) = 0; // vehicle does not exist
			end
		end
	}

	/* TODO */
    instance AutopilotQNet<training_vehicles> qnet;

    connect training_state -> qnet.state;
    connect qnet.action -> action;
}